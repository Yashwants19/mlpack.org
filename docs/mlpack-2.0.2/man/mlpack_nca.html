<html>
<head>
<!-- probably could use more keywords -->
<meta name="keywords" content="mlpack, libmlpack, c++, armadillo, machine learning, data mining, classification, regression, tree-based methods, dual-tree algorithm">
<meta name="description" content="mlpack: a scalable c++ machine learning library">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>mlpack: a scalable c++ machine learning library</title>
</head>
<link rel="stylesheet" href="../../../style.css">
<link rel="stylesheet" href="../../style-man.css">
<link href='http://fonts.googleapis.com/css?family=Maven+Pro:500'
rel='stylesheet' type='text/css'>
<body>
<!-- very simple table with as few words as possible -->
<br>
<div class="titlebar">
   <a href="http://www.mlpack.org"><img src="../../../mlpack.png"></a>
</div>
<center>
<div class="mlnavbar">
  <div class="navcontainer">
   <div class="mlnavitem" name="mlnavmain"><a href="../../../index.html">main</a></div>
   <div class="mlnavitem" name="mlnavabout"><a href="../../../about.html">about</a></div>
   <div class="mlnavitem" name="mlnavdoc"><a href="../../../docs.html">docs</a></div>
   <div class="mlnavitem" name="mlnavhelp"><a href="../../../help.html">get help</a></div>
   <div class="mlnavitem" name="mlnavbugs"><a
href="https://github.com/mlpack/mlpack">github</a></div>
  </div>
</div>
</center>
<div class="separator"></div>
<center>
<div class="mainsection smallertext manpage">


<h1 align="center">mlpack_nca</h1>



<h2>NAME
<a name="NAME"></a>
</h2>



<p class="closemargin first"><font class="code">mlpack_nca</font>
- neighborhood components analysis (nca)</p>

<h2>SYNOPSIS
<a name="SYNOPSIS"></a>
</h2>



<p class="closemargin first"><font class="code">mlpack_nca</font>
[<font class="code">-h</font>] [<font class="code">-v</font>] <font class="code">-i</font> <font class="code2">string</font> <font class="code">-o</font>
<font class="code2">string</font> [<font class="code">-A</font> <font class="code2">double</font>] [<font class="code">-b</font>
<font class="code2">int</font>] [<font class="code">-l</font> <font class="code2">string</font>] [<font class="code">-L</font>] [<font class="code">-n</font>
<font class="code2">int</font>] [<font class="code">-T</font> <font class="code2">int</font>] [<font class="code">-M</font> <font class="code2">double</font>]
[<font class="code">-m</font> <font class="code2">double</font>] [<font class="code">-N</font>] [<font class="code">-B</font> <font class="code2">int</font>]
[<font class="code">-O</font> <font class="code2">string</font>] [<font class="code">-s</font> <font class="code2">int</font>] [<font class="code">-a</font>
<font class="code2">double</font>] [<font class="code">-t</font> <font class="code2">double</font>] [<font class="code">-V</font>]
[<font class="code">-w</font> <font class="code2">double</font>]</p>

<h2>DESCRIPTION
<a name="DESCRIPTION"></a>
</h2>


<p class="closemargin first">This program
implements Neighborhood Components Analysis, both a linear
dimensionality reduction technique and a distance learning
technique. The method seeks to improve k-nearest-neighbor
classification on a dataset by scaling the dimensions. The
method is nonparametric, and does not require a value of k.
It works by using stochastic (&quot;soft&quot;) neighbor
assignments and using optimization techniques over the
gradient of the accuracy of the neighbor assignments.</p>

<p class="closemargin first">To work, this
algorithm needs labeled data. It can be given as the last
row of the input dataset (<font class="code">--input_file</font>), or
alternatively in a separate file (<font class="code">--labels_file</font>).</p>

<p class="closemargin first">This
implementation of NCA uses stochastic gradient descent,
mini-batch stochastic gradient descent, or the L_BFGS
optimizer. These optimizers do not guarantee global
convergence for a nonconvex objective function (NCA&rsquo;s
objective function is nonconvex), so the final results could
depend on the random seed or other optimizer parameters.</p>

<p class="closemargin first">Stochastic
gradient descent, specified by <font class="code">--optimizer</font>
&quot;sgd&quot;, depends primarily on two parameters: the
step size (<font class="code">--step_size</font>) and the maximum number of
iterations (<font class="code">--max_iterations</font>). In addition, a
normalized starting point can be used (<font class="code">--normalize</font>),
which is necessary if many warnings of the form
&rsquo;Denominator of p_i is 0!&rsquo; are given. Tuning the
step size can be a tedious affair. In general, the step size
is too large if the objective is not mostly uniformly
decreasing, or if zero-valued denominator warnings are being
issued. The step size is too small if the objective is
changing very slowly. Setting the termination condition can
be done easily once a good step size parameter is found;
either increase the maximum iterations to a large number and
allow SGD to find a minimum, or set the maximum iterations
to 0 (allowing infinite iterations) and set the tolerance
(<font class="code">--tolerance</font>) to define the maximum allowed
difference between objectives for SGD to terminate. Be
careful-<font class="code">--setting</font> the tolerance instead of the
maximum iterations can take a very long time and may
actually never converge due to the properties of the SGD
optimizer. Note that a single iteration of SGD refers to a
single point, so to take a single pass over the dataset, set
<font class="code">--max_iterations</font> equal to the number of points in the
dataset.</p>

<p class="closemargin first">The mini-batch
SGD optimizer, specified by <font class="code">--optimizer</font>
&quot;minibatch-sgd&quot;, has the same parameters as SGD,
but the batch size may also be specified with the
<font class="code">--batch_size</font> (<font class="code">-b</font>) option. Each iteration of
mini-batch SGD refers to a single mini-batch.</p>

<p class="closemargin first">The L-BFGS
optimizer, specified by <font class="code">--optimizer</font>
&quot;lbfgs&quot;, uses a back-tracking line search
algorithm to minimize a function. The following parameters
are used by L-BFGS: <font class="code">--num_basis</font> (specifies the number
of memory points used by L-BFGS), <font class="code">--max_iterations</font>,
<font class="code">--armijo_constant</font>, <font class="code">--wolfe</font>, <font class="code">--tolerance</font>
(the optimization is terminated when the gradient norm is
below this value), <font class="code">--max_line_search_trials</font>,
<font class="code">--min_step</font> and <font class="code">--max_step</font> (which both refer to
the line search routine). For more details on the L-BFGS
optimizer, consult either the mlpack L-BFGS documentation
(in lbfgs.hpp) or the vast set of published literature on
L-BFGS.</p>

<p class="closemargin first">By default, the
SGD optimizer is used.</p>

<h2>REQUIRED OPTIONS
<a name="REQUIRED OPTIONS"></a>
</h2>



<p class="closemargin first"><font class="code">--input_file
(-i) [</font><font class="code2">string</font><font class="code">]</font></p>

<p class="farmargin">Input dataset to run NCA
on.</p>

<p class="closemargin"><font class="code">--output_file (-o)
[</font><font class="code2">string</font><font class="code">]</font></p>

<p class="farmargin">Output file for learned
distance matrix.</p>

<h2>OPTIONS
<a name="OPTIONS"></a>
</h2>



<p class="closemargin first"><font class="code">--armijo_constant
(-A) [</font><font class="code2">double</font><font class="code">]</font></p>

<p class="farmargin">Armijo constant for L-BFGS.
Default value 0.0001.</p>

<p class="closemargin"><font class="code">--batch_size (-b)
[</font><font class="code2">int</font><font class="code">]</font></p>

<p class="farmargin">Batch size for mini-batch SGD.
Default value 50.</p>

<p class="closemargin"><font class="code">--help (-h)</font></p>

<p class="farmargin">Default help info.</p>

<p class="closemargin"><font class="code">--info
[</font><font class="code2">string</font><font class="code">]</font></p>

<p class="farmargin">Get help on a specific module
or option. Default value &rsquo;&rsquo;.</p>

<p class="closemargin"><font class="code">--labels_file (-l)
[</font><font class="code2">string</font><font class="code">]</font></p>

<p class="farmargin">File of labels for input
dataset. Default value &rsquo;&rsquo;.</p>

<p class="closemargin"><font class="code">--linear_scan (-L)</font></p>

<p class="farmargin">Don&rsquo;t shuffle the order
in which data points are visited for SGD or mini-batch
SGD.</p>

<p class="closemargin"><font class="code">--max_iterations (-n)
[</font><font class="code2">int</font><font class="code">]</font></p>

<p class="farmargin">Maximum number of iterations
for SGD or L-BFGS (0 indicates no limit). Default value
500000.</p>

<p class="closemargin"><font class="code">--max_line_search_trials
(-T) [</font><font class="code2">int</font><font class="code">]</font></p>

<p class="farmargin">Maximum number of line search
trials for L-BFGS. Default value 50.</p>

<p class="closemargin"><font class="code">--max_step (-M)
[</font><font class="code2">double</font><font class="code">]</font></p>

<p class="farmargin">Maximum step of line search for
L-BFGS. Default value 1e+20.</p>

<p class="closemargin"><font class="code">--min_step (-m)
[</font><font class="code2">double</font><font class="code">]</font></p>

<p class="farmargin">Minimum step of line search for
L-BFGS. Default value 1e-20.</p>

<p class="closemargin"><font class="code">--normalize (-N)</font></p>

<p class="farmargin">Use a normalized starting point
for optimization. This is useful for when points are far
apart, or when SGD is returning NaN.</p>

<p class="closemargin"><font class="code">--num_basis (-B)
[</font><font class="code2">int</font><font class="code">]</font></p>

<p class="farmargin">Number of memory points to be
stored for L-BFGS. Default value 5.</p>

<p class="closemargin"><font class="code">--optimizer (-O)
[</font><font class="code2">string</font><font class="code">]</font></p>

<p class="farmargin">Optimizer to use;
&rsquo;sgd&rsquo;, &rsquo;minibatch-sgd&rsquo;, or
&rsquo;lbfgs&rsquo;. Default value &rsquo;sgd&rsquo;.</p>

<p class="closemargin"><font class="code">--seed (-s)
[</font><font class="code2">int</font><font class="code">]</font></p>

<p class="farmargin">Random seed. If 0,
&rsquo;std::time(NULL)&rsquo; is used. Default value 0.</p>

<p class="closemargin"><font class="code">--step_size (-a)
[</font><font class="code2">double</font><font class="code">]</font></p>

<p class="farmargin">Step size for stochastic
gradient descent (alpha). Default value 0.01.</p>

<p class="closemargin"><font class="code">--tolerance (-t)
[</font><font class="code2">double</font><font class="code">]</font></p>

<p class="farmargin">Maximum tolerance for
termination of SGD or L-BFGS. Default value 1e-07.</p>

<p class="closemargin"><font class="code">--verbose (-v)</font></p>

<p class="farmargin">Display informational messages
and the full list of parameters and timers at the end of
execution.</p>

<p class="closemargin"><font class="code">--version (-V)</font></p>

<p class="farmargin">Display the version of
mlpack.</p>

<p class="closemargin"><font class="code">--wolfe (-w)
[</font><font class="code2">double</font><font class="code">]</font></p>

<p class="farmargin">Wolfe condition parameter for
L-BFGS. Default value 0.9.</p>

<h2>ADDITIONAL INFORMATION
<a name="ADDITIONAL INFORMATION"></a>
</h2>


<p class="closemargin first">For further
information, including relevant papers, citations, and
theory, consult the documentation found at
http://www.mlpack.org or included with your DISTRIBUTION OF
MLPACK.</p>
</div></body></html>
