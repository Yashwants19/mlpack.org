<html >
<head >

<meta name="keywords" content="mlpack, libmlpack, c++, armadillo, machine learning, data mining, classification, regression, tree-based methods, dual-tree algorithm">
<meta name="description" content="mlpack: a fast, flexible c++ machine learning library">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title >mlpack: a fast, flexible c++ machine learning library</title>
</head><link rel="stylesheet" href="../../../style.css" /></link><link rel="stylesheet" href="../../style-python.css" /></link><link href="http://fonts.googleapis.com/css?family=Maven+Pro:500" rel="stylesheet" type="text/css" /></link>



<body ><br /></br>


<div class="titlebar">
   <a href="http://www.mlpack.org"><img src="../../../mlpack.png"></a>
</div>
<center >
<div class="mlnavbar">
  <div class="navcontainer">
   <div class="mlnavitem" name="mlnavmain"><a href="../../../index.html">main</a></div>
   <div class="mlnavitem" name="mlnavabout"><a href="../../../about.html">about</a></div>
   <div class="mlnavitem" name="mlnavdoc"><a href="../../../docs.html">docs</a></div>
   <div class="mlnavitem" name="mlnavhelp"><a href="../../../help.html">get help</a></div>
   <div class="mlnavitem" name="mlnavbugs"><a href="https://github.com/mlpack/mlpack">github</a></div>
  </div>
</div>
</center>
<div class="separator"></div>
<center >
<div class="mainsection smallertext pythonpage">
<h1><center>mlpack.decision_tree</center></h1>
<font class="titlecode">decision_tree(...)</font><br />
<font class="titlebold">Decision tree</font>
<p class="importcode">&gt;&gt;&gt; from mlpack import decision_tree</p><p>Train and evaluate using a decision tree.  Given a dataset containing numeric or categorical features, and associated labels for each point in the dataset, this program can train a decision tree on that data.</p>
</p>
<p>The training file and associated labels are specified with the <font class="code">'training'</font> and <font class="code">'labels'</font> parameters, respectively.  The labels should be in the range [0, num_classes - 1]. Optionally, if <font class="code">'labels'</font> is not specified, the labels are assumed to be the last dimension of the training dataset.</p>
</p>
<p>When a model is trained, the <font class="code">'output_model'</font> output parameter may be used to save the trained model.  A model may be loaded for predictions with the <font class="code">'input_model'</font>'</font> parameter.  The <font class="code">'input_model'</font> parameter may not be specified when the <font class="code">'training'</font> parameter is specified.  The <font class="code">'minimum_leaf_size'</font> parameter specifies the minimum number of training points that must fall into each leaf for it to be split.  The <font class="code">'minimum_gain_split'</font> parameter specifies the minimum gain that is needed for the node to split. If <font class="code">'print_training_error'</font> is specified, the training error will be printed.</p>
</p>
<p>Test data may be specified with the <font class="code">'test'</font> parameter, and if performance numbers are desired for that test set, labels may be specified with the <font class="code">'test_labels'</font> parameter.  Predictions for each test point may be saved via the <font class="code">'predictions'</font> output parameter.  Class probabilities for each prediction may be saved with the <font class="code">'probabilities'</font> output parameter.</p>
</p>
<p>For example, to train a decision tree with a minimum leaf size of 20 on the dataset contained in <font class="code">'data'</font> with labels <font class="code">'labels'</font>, saving the output model to <font class="code">'tree'</font> and printing the training error, one could call</p>
</p>
<p class="codeblock"><font class="code">&gt;&gt;&gt; decision_tree(training=data, labels=labels, minimum_leaf_size=20,<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  minimum_gain_split=0.001, print_training_error=True)<br />
&gt;&gt;&gt; tree = output['output_model']</font></p>
</p>
<p>Then, to use that model to classify points in <font class="code">'test_set'</font> and print the test error given the labels <font class="code">'test_labels'</font> using that model, while saving the predictions for each point to <font class="code">'predictions'</font>, one could call </p>
</p>
<p class="codeblock"><font class="code">&gt;&gt;&gt; decision_tree(input_model=tree, test=test_set, test_labels=test_labels)<br />
&gt;&gt;&gt; predictions = output['predictions']</font></p><h2>input options</h2>
<ul>
<li><font class="code">copy_all_inputs</font> <font class="codetype">(bool)</font>: If specified, all input parameters will be deep copied before the method is run.  This is useful for debugging problems where the input parameters are being modified by the algorithm, but can slow down the code.</li><li><font class="code">input_model</font> <font class="codetype">(mlpack.DecisionTreeModelType)</font>: Pre-trained decision tree, to be used with test points.</li><li><font class="code">labels</font> <font class="codetype">(numpy vector or array, int/long dtype)</font>: Training labels.</li><li><font class="code">minimum_gain_split</font> <font class="codetype">(float)</font>: Minimum gain for node splitting.  Default value 1e-07.</li><li><font class="code">minimum_leaf_size</font> <font class="codetype">(int)</font>: Minimum number of points in a leaf.  Default value 20.</li><li><font class="code">print_training_error</font> <font class="codetype">(bool)</font>: Print the training error.</li><li><font class="code">test</font> <font class="codetype">(mlpack.TUPLE_TYPEType)</font>: Testing dataset (may be categorical).</li><li><font class="code">test_labels</font> <font class="codetype">(numpy matrix or arraylike, int/long dtype)</font>: Test point labels, if accuracy calculation is desired.</li><li><font class="code">training</font> <font class="codetype">(mlpack.TUPLE_TYPEType)</font>: Training dataset (may be categorical).</li><li><font class="code">verbose</font> <font class="codetype">(bool)</font>: Display informational messages and the full list of parameters and timers at the end of execution.</li><li><font class="code">weights</font> <font class="codetype">(numpy matrix or arraylike, float dtype)</font>: The weight of labels</li></ul>
<h2>output options</h2>
<p>The return value from the binding is a dict containing the following elements:</p>
<ul><li><font class="code">output_model</font> <font class="codetype">(mlpack.DecisionTreeModelType)</font>: Output for trained decision tree.</li><li><font class="code">predictions</font> <font class="codetype">(numpy vector, int dtype)</font>: Class predictions for each test point.</li><li><font class="code">probabilities</font> <font class="codetype">(numpy matrix, float dtype)</font>: Class probabilities for each test point.</li><ul>
</ul>
</div>
</body>
</html>
