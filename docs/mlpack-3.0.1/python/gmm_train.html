<html >
<head >

<meta name="keywords" content="mlpack, libmlpack, c++, armadillo, machine learning, data mining, classification, regression, tree-based methods, dual-tree algorithm">
<meta name="description" content="mlpack: a fast, flexible c++ machine learning library">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title >mlpack: a fast, flexible c++ machine learning library</title>
</head><link rel="stylesheet" href="../../../style.css" /></link><link rel="stylesheet" href="../../style-python.css" /></link><link href="http://fonts.googleapis.com/css?family=Maven+Pro:500" rel="stylesheet" type="text/css" /></link>



<body ><br /></br>


<div class="titlebar">
   <a href="http://www.mlpack.org"><img src="../../../mlpack.png"></a>
</div>
<center >
<div class="mlnavbar">
  <div class="navcontainer">
   <div class="mlnavitem" name="mlnavmain"><a href="../../../index.html">main</a></div>
   <div class="mlnavitem" name="mlnavabout"><a href="../../../about.html">about</a></div>
   <div class="mlnavitem" name="mlnavdoc"><a href="../../../docs.html">docs</a></div>
   <div class="mlnavitem" name="mlnavhelp"><a href="../../../help.html">get help</a></div>
   <div class="mlnavitem" name="mlnavbugs"><a href="https://github.com/mlpack/mlpack">github</a></div>
  </div>
</div>
</center>
<div class="separator"></div>
<center >
<div class="mainsection smallertext pythonpage">
<h1><center>mlpack.gmm_train</center></h1>
<font class="titlecode">gmm_train(...)</font><br />
<font class="titlebold">Gaussian Mixture Model (GMM) Training</font>
<p class="importcode">&gt;&gt;&gt; from mlpack import gmm_train</p><p>This program takes a parametric estimate of a Gaussian mixture model (GMM) using the EM algorithm to find the maximum likelihood estimate.  The model may be saved and reused by other mlpack GMM tools.</p>
</p>
<p>The input data to train on must be specified with the <font class="code">'input'</font> parameter, and the number of Gaussians in the model must be specified with the <font class="code">'gaussians'</font> parameter.  Optionally, many trials with different random initializations may be run, and the result with highest log-likelihood on the training data will be taken.  The number of trials to run is specified with the <font class="code">'trials'</font> parameter.  By default, only one trial is run.</p>
</p>
<p>The tolerance for convergence and maximum number of iterations of the EM algorithm are specified with the <font class="code">'tolerance'</font> and <font class="code">'max_iterations'</font> parameters, respectively.  The GMM may be initialized for training with another model, specified with the <font class="code">'input_model'</font> parameter. Otherwise, the model is initialized by running k-means on the data.  The k-means clustering initialization can be controlled with the <font class="code">'refined_start'</font>, <font class="code">'samplings'</font>, and <font class="code">'percentage'</font> parameters.  If <font class="code">'refined_start'</font> is specified, then the Bradley-Fayyad refined start initialization will be used.  This can often lead to better clustering results.</p>
</p>
<p>The <font class="code">'diagonal_covariance'</font> flag will cause the learned covariances to be diagonal matrices.  This significantly simplifies the model itself and causes training to be faster, but restricts the ability to fit more complex GMMs.</p>
</p>
<p>If GMM training fails with an error indicating that a covariance matrix could not be inverted, make sure that the <font class="code">'no_force_positive'</font> parameter is not specified.  Alternately, adding a small amount of Gaussian noise (using the <font class="code">'noise'</font> parameter) to the entire dataset may help prevent Gaussians with zero variance in a particular dimension, which is usually the cause of non-invertible covariance matrices.</p>
</p>
<p>The <font class="code">'no_force_positive'</font> parameter, if set, will avoid the checks after each iteration of the EM algorithm which ensure that the covariance matrices are positive definite.  Specifying the flag can cause faster runtime, but may also cause non-positive definite covariance matrices, which will cause the program to crash.</p>
</p>
<p>As an example, to train a 6-Gaussian GMM on the data in <font class="code">'data'</font> with a maximum of 100 iterations of EM and 3 trials, saving the trained GMM to <font class="code">'gmm'</font>, the following command can be used:</p>
</p>
<p class="codeblock"><font class="code">&gt;&gt;&gt; gmm_train(input=data, gaussians=6, trials=3)<br />
&gt;&gt;&gt; gmm = output['output_model']</font></p>
</p>
<p>To re-train that GMM on another set of data <font class="code">'data2'</font>, the following command may be used: </p>
</p>
<p class="codeblock"><font class="code">&gt;&gt;&gt; gmm_train(input_model=gmm, input=data2, gaussians=6)<br />
&gt;&gt;&gt; new_gmm = output['output_model']</font></p><h2>input options</h2>
<ul>
<li><font class="code">gaussians</font> <font class="codetype">(int)</font>: <font class="required">[required]</font> Number of Gaussians in the GMM.  Default value 0.</li><li><font class="code">input</font> <font class="codetype">(numpy matrix or arraylike, float dtype)</font>: <font class="required">[required]</font> The training data on which the model will be fit.</li><li><font class="code">copy_all_inputs</font> <font class="codetype">(bool)</font>: If specified, all input parameters will be deep copied before the method is run.  This is useful for debugging problems where the input parameters are being modified by the algorithm, but can slow down the code.</li><li><font class="code">diagonal_covariance</font> <font class="codetype">(bool)</font>: Force the covariance of the Gaussians to be diagonal.  This can accelerate training time significantly.</li><li><font class="code">input_model</font> <font class="codetype">(mlpack.GMMType)</font>: Initial input GMM model to start training with.</li><li><font class="code">max_iterations</font> <font class="codetype">(int)</font>: Maximum number of iterations of EM algorithm (passing 0 will run until convergence).  Default value 250.</li><li><font class="code">no_force_positive</font> <font class="codetype">(bool)</font>: Do not force the covariance matrices to be positive definite.</li><li><font class="code">noise</font> <font class="codetype">(float)</font>: Variance of zero-mean Gaussian noise to add to data.  Default value 0.</li><li><font class="code">percentage</font> <font class="codetype">(float)</font>: If using --refined_start, specify the percentage of the dataset used for each sampling (should be between 0.0 and 1.0).  Default value 0.02.</li><li><font class="code">refined_start</font> <font class="codetype">(bool)</font>: During the initialization, use refined initial positions for k-means clustering (Bradley and Fayyad, 1998).</li><li><font class="code">samplings</font> <font class="codetype">(int)</font>: If using --refined_start, specify the number of samplings used for initial points.  Default value 100.</li><li><font class="code">seed</font> <font class="codetype">(int)</font>: Random seed.  If 0, 'std::time(NULL)' is used.  Default value 0.</li><li><font class="code">tolerance</font> <font class="codetype">(float)</font>: Tolerance for convergence of EM.  Default value 1e-10.</li><li><font class="code">trials</font> <font class="codetype">(int)</font>: Number of trials to perform in training GMM.  Default value 1.</li><li><font class="code">verbose</font> <font class="codetype">(bool)</font>: Display informational messages and the full list of parameters and timers at the end of execution.</li></ul>
<h2>output options</h2>
<p>The return value from the binding is a dict containing the following elements:</p>
<ul><li><font class="code">output_model</font> <font class="codetype">(mlpack.GMMType)</font>: Output for trained GMM model.</li><ul>
</ul>
</div>
</body>
</html>
