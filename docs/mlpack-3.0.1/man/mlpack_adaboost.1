.\" Text automatically generated by txt2man
.TH mlpack_adaboost 1 "10 May 2018" "mlpack-git-e21aabc1c" "User Commands"
.SH NAME
\fBmlpack_adaboost \fP- adaboost
.SH SYNOPSIS
.nf
.fam C
 \fBmlpack_adaboost\fP [\fB-m\fP \fIunknown\fP] [\fB-i\fP \fIint\fP] [\fB-l\fP \fIstring\fP] [\fB-T\fP \fIstring\fP] [\fB-e\fP \fIdouble\fP] [\fB-t\fP \fIstring\fP] [\fB-V\fP \fIbool\fP] [\fB-w\fP \fIstring\fP] [\fB-o\fP \fIstring\fP] [\fB-M\fP \fIunknown\fP] [\fB-h\fP \fB-v\fP] 
.fam T
.fi
.fam T
.fi
.SH DESCRIPTION


This program implements the AdaBoost (or Adaptive Boosting) algorithm. The
variant of AdaBoost implemented here is AdaBoost.MH. It uses a weak learner,
either decision stumps or perceptrons, and over many iterations, creates a
strong learner that is a weighted ensemble of weak learners. It runs these
iterations until a tolerance value is crossed for change in the value of the
weighted training error.
.PP
For more information about the algorithm, see the paper "Improved Boosting
Algorithms Using Confidence-Rated Predictions", by R.E. Schapire and Y.
Singer.
.PP
This program allows training of an AdaBoost model, and then application of
that model to a test dataset. To train a model, a dataset must be passed with
the '\fB--training_file\fP (\fB-t\fP)' option. Labels can be given with the
\(cq\fB--labels_file\fP (\fB-l\fP)' option; if no labels are specified, the labels will be
assumed to be the last column of the input dataset. Alternately, an AdaBoost
model may be loaded with the '\fB--input_model_file\fP (\fB-m\fP)' option.
.PP
Once a model is trained or loaded, it may be used to provide class predictions
for a given test dataset. A test dataset may be specified with the
\(cq\fB--test_file\fP (\fB-T\fP)' parameter. The predicted classes for each point in the
test dataset are output to the '\fB--output_file\fP (\fB-o\fP)' output parameter. The
AdaBoost model itself is output to the '\fB--output_model_file\fP (\fB-M\fP)'output
parameter.
.PP
For example, to run AdaBoost on an input dataset 'data.csv' with perceptrons
as the weak learner type, storing the trained model in 'model.bin', one could
use the following command: 
.PP
$ adaboost \fB--training_file\fP data.csv \fB--output_file\fP model.csv \fB--weak_learner\fP
perceptron
.PP
Similarly, an already-trained model in 'model.bin' can be used to provide
class predictions from test data 'test_data.csv' and store the output in
\(cqpredictions.csv' with the following command: 
.PP
$ adaboost \fB--input_model_file\fP model.bin \fB--test_file\fP test_data.csv
\fB--output_file\fP predictions.csv
.RE
.PP

.SH OPTIONAL INPUT OPTIONS 

.TP
.B
\fB--help\fP (\fB-h\fP) [\fIbool\fP]
Default help info. 
.TP
.B
\fB--info\fP [\fIstring\fP]
Get help on a specific module or option.  Default value ''. 
.TP
.B
\fB--input_model_file\fP (\fB-m\fP) [\fIunknown\fP]
Input AdaBoost model. Default value ''. 
.TP
.B
\fB--iterations\fP (\fB-i\fP) [\fIint\fP]
The maximum number of boosting iterations to be run (0 will run until convergence.) Default value 1000. 
.TP
.B
\fB--labels_file\fP (\fB-l\fP) [\fIstring\fP]
Labels for the training set. Default value ''. 
.TP
.B
\fB--test_file\fP (\fB-T\fP) [\fIstring\fP]
Test dataset. Default value ''. 
.TP
.B
\fB--tolerance\fP (\fB-e\fP) [\fIdouble\fP]
The tolerance for change in values of the weighted error during training. Default value 1e-10. 
.TP
.B
\fB--training_file\fP (\fB-t\fP) [\fIstring\fP]
Dataset for training AdaBoost. Default value ''. 
.TP
.B
\fB--verbose\fP (\fB-v\fP) [\fIbool\fP]
Display informational messages and the full list of parameters and timers at the end of execution. 
.TP
.B
\fB--version\fP (\fB-V\fP) [\fIbool\fP]
Display the version of mlpack. 
.TP
.B
\fB--weak_learner\fP (\fB-w\fP) [\fIstring\fP] The type of weak learner to use: 'decision_stump', or 'perceptron'. Default value 'decision_stump'.
.SH OPTIONAL OUTPUT OPTIONS 

.TP
.B
\fB--output_file\fP (\fB-o\fP) [\fIstring\fP]
Predicted labels for the test set. Default value ''. 
.TP
.B
\fB--output_model_file\fP (\fB-M\fP) [\fIunknown\fP]
Output trained AdaBoost model. Default value ''.
.SH ADDITIONAL INFORMATION

For further information, including relevant papers, citations, and theory,
consult the documentation found at http://www.mlpack.org or included with your
distribution of mlpack.
