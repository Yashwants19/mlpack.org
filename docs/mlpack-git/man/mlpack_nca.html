<html >
<head >

<meta name="keywords" content="mlpack, libmlpack, c++, armadillo, machine learning, data mining, classification, regression, tree-based methods, dual-tree algorithm">
<meta name="description" content="mlpack: a scalable c++ machine learning library">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title >mlpack: a scalable c++ machine learning library</title>
</head><link rel="stylesheet" href="../../../style.css" /></link><link rel="stylesheet" href="../../style-man.css" /></link><link href="http://fonts.googleapis.com/css?family=Maven+Pro:500" rel="stylesheet" type="text/css" /></link>



<body ><br /></br>


<div class="titlebar">
   <a href="http://www.mlpack.org"><img src="../../../mlpack.png"></a>
</div>
<center >
<div class="mlnavbar">
  <div class="navcontainer">
   <div class="mlnavitem" name="mlnavmain"><a href="../../../index.html">main</a></div>
   <div class="mlnavitem" name="mlnavabout"><a href="../../../about.html">about</a></div>
   <div class="mlnavitem" name="mlnavdoc"><a href="../../../docs.html">docs</a></div>
   <div class="mlnavitem" name="mlnavhelp"><a href="../../../help.html">get help</a></div>
   <div class="mlnavitem" name="mlnavbugs"><a href="https://github.com/mlpack/mlpack">github</a></div>
  </div>
</div>
</center>
<div class="separator"></div>
<center >
<div class="mainsection smallertext manpage">

<h1 align="center">mlpack_nca</h1>



<h2>NAME
<a name="NAME"></a>
</h2>



<p class="closemargin first"><font class="code">mlpack_nca</font>
- neighborhood components analysis (nca)</p>

<h2>SYNOPSIS
<a name="SYNOPSIS"></a>
</h2>



<p class="closemargin first"><font class="code">mlpack_nca</font>
[<font class="code">-h</font>] [<font class="code">-v</font>]</p>

<h2>DESCRIPTION
<a name="DESCRIPTION"></a>
</h2>


<p class="closemargin first">This program
implements Neighborhood Components Analysis, both a linear
dimensionality reduction technique and a distance learning
technique. The method seeks to improve k-nearest-neighbor
classification on a dataset by scaling the dimensions. The
method is nonparametric, and does not require a value of k.
It works by using stochastic (&quot;soft&quot;) neighbor
assignments and using optimization techniques over the
gradient of the accuracy of the neighbor assignments.</p>

<p class="closemargin first">To work, this
algorithm needs labeled data. It can be given as the last
row of the input dataset (specified with
&rsquo;<font class="code">--input_file</font> (<font class="code">-i</font>)&rsquo;), or
alternatively as a separate matrix (specified with
&rsquo;<font class="code">--labels_file</font> (<font class="code">-l</font>)&rsquo;).</p>

<p class="closemargin first">This
implementation of NCA uses stochastic gradient descent,
mini-batch stochastic gradient descent, or the L_BFGS
optimizer. These optimizers do not guarantee global
convergence for a nonconvex objective function (NCA&rsquo;s
objective function is nonconvex), so the final results could
depend on the random seed or other optimizer parameters.</p>

<p class="closemargin first">Stochastic
gradient descent, specified by the value &rsquo;sgd&rsquo;
for the parameter &rsquo;<font class="code">--optimizer</font>
(<font class="code">-O</font>)&rsquo;, depends primarily on two parameters: the
step size (specified with &rsquo;<font class="code">--step_size</font>
(<font class="code">-a</font>)&rsquo;) and the maximum number of iterations
(specified with &rsquo;<font class="code">--max_iterations</font>
(<font class="code">-n</font>)&rsquo;). In addition, a normalized starting
point can be used by specifying the
&rsquo;<font class="code">--normalize</font> (<font class="code">-N</font>)&rsquo; parameter,
which is necessary if many warnings of the form
&rsquo;Denominator of p_i is 0!&rsquo; are given. Tuning the
step size can be a tedious affair. In general, the step size
is too large if the objective is not mostly uniformly
decreasing, or if zero-valued denominator warnings are being
issued. The step size is too small if the objective is
changing very slowly. Setting the termination condition can
be done easily once a good step size parameter is found;
either increase the maximum iterations to a large number and
allow SGD to find a minimum, or set the maximum iterations
to 0 (allowing infinite iterations) and set the tolerance
(specified by &rsquo;<font class="code">--tolerance</font> (<font class="code">-t</font>)&rsquo;)
to define the maximum allowed difference between objectives
for SGD to terminate. Be careful-<font class="code">--setting</font> the
tolerance instead of the maximum iterations can take a very
long time and may actually never converge due to the
properties of the SGD optimizer. Note that a single
iteration of SGD refers to a single point, so to take a
single pass over the dataset, set the value of the
&rsquo;<font class="code">--max_iterations</font> (<font class="code">-n</font>)&rsquo; parameter
equal to the number of points in the dataset.</p>

<p class="closemargin first">The mini-batch
SGD optimizer, specified by the value
&rsquo;minibatch-sgd&rsquo; for the parameter
&rsquo;<font class="code">--optimizer</font> (<font class="code">-O</font>)&rsquo;, has the same
parameters as SGD, but the batch size may also be specified
with the &rsquo;<font class="code">--batch_size</font> (<font class="code">-b</font>)&rsquo;
option. Each iteration of mini-batch SGD refers to a single
mini-batch.</p>

<p class="closemargin first">The L-BFGS
optimizer, specified by the value &rsquo;lbfgs&rsquo; for
the parameter &rsquo;<font class="code">--optimizer</font> (<font class="code">-O</font>)&rsquo;,
uses a back-tracking line search algorithm to minimize a
function. The following parameters are used by L-BFGS:
&rsquo;<font class="code">--num_basis</font> (<font class="code">-B</font>)&rsquo; (specifies the
number of memory points used by L-BFGS),
&rsquo;<font class="code">--max_iterations</font> (<font class="code">-n</font>)&rsquo;,
&rsquo;<font class="code">--armijo_constant</font> (<font class="code">-A</font>)&rsquo;,
&rsquo;<font class="code">--wolfe</font> (<font class="code">-w</font>)&rsquo;,
&rsquo;<font class="code">--tolerance</font> (<font class="code">-t</font>)&rsquo; (the
optimization is terminated when the gradient norm is below
this value), &rsquo;<font class="code">--max_line_search_trials</font>
(<font class="code">-T</font>)&rsquo;, &rsquo;<font class="code">--min_step</font>
(<font class="code">-m</font>)&rsquo;, and &rsquo;<font class="code">--max_step</font>
(<font class="code">-M</font>)&rsquo; (which both refer to the line search
routine). For more details on the L-BFGS optimizer, consult
either the mlpack L-BFGS documentation (in lbfgs.hpp) or the
vast set of published literature on L-BFGS.</p>

<p class="closemargin first">By default, the
SGD optimizer is used.</p>

<h2>REQUIRED INPUT OPTIONS
<a name="REQUIRED INPUT OPTIONS"></a>
</h2>



<p class="closemargin first"><font class="code">--input_file
(-i) [string]</font></p>

<p class="farmargin">Input dataset to run NCA
on.</p>

<h2>OPTIONAL INPUT OPTIONS
<a name="OPTIONAL INPUT OPTIONS"></a>
</h2>



<p class="closemargin first"><font class="code">--armijo_constant</font>
(<font class="code">-A</font>) [double] Armijo constant for L-BFGS. Default
value 0.0001. <font class="code"><br>
--batch_size (-b) [int]</font></p>

<p class="farmargin">Batch size for mini-batch SGD.
Default value</p>

<table width="100
" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="22
"></td>
<td width="4
">


<p>50.</p></td>
<td width="74
">
</td></tr>
</table>

<p class="closemargin"><font class="code">--help (-h) [bool]</font></p>

<p class="farmargin">Default help info.</p>

<p class="closemargin"><font class="code">--info [string]</font></p>

<p class="farmargin">Get help on a specific module
or option. Default value &rsquo;&rsquo;.</p>

<p class="closemargin"><font class="code">--labels_file (-l)
[string]</font></p>

<p class="farmargin">Labels for input dataset.
Default value &rsquo;&rsquo;.</p>

<p class="closemargin"><font class="code">--linear_scan (-L)
[bool]</font></p>

<p class="farmargin">Don&rsquo;t shuffle the order
in which data points are visited for SGD or mini-batch
SGD.</p>

<p class="closemargin"><font class="code">--max_iterations (-n)
[int]</font></p>

<p class="farmargin">Maximum number of iterations
for SGD or L-BFGS (0 indicates no limit). Default value
500000. <font class="code">--max_line_search_trials</font> (<font class="code">-T</font>) [int]
Maximum number of line search trials for L-BFGS. Default
value 50.</p>

<p class="closemargin"><font class="code">--max_step (-M)
[double]</font></p>

<p class="farmargin">Maximum step of line search for
L-BFGS. Default value 1e+20.</p>

<p class="closemargin"><font class="code">--min_step (-m)
[double]</font></p>

<p class="farmargin">Minimum step of line search for
L-BFGS. Default value 1e-20.</p>

<p class="closemargin"><font class="code">--normalize (-N)
[bool]</font></p>

<p class="farmargin">Use a normalized starting point
for optimization. This is useful for when points are far
apart, or when SGD is returning NaN.</p>

<p class="closemargin"><font class="code">--num_basis (-B)
[int]</font></p>

<p class="farmargin">Number of memory points to be
stored for L-BFGS. Default value 5.</p>

<p class="closemargin"><font class="code">--optimizer (-O)
[string]</font></p>

<p class="farmargin">Optimizer to use;
&rsquo;sgd&rsquo;, &rsquo;minibatch-sgd&rsquo;, or
&rsquo;lbfgs&rsquo;. Default value &rsquo;sgd&rsquo;.</p>

<p class="closemargin"><font class="code">--seed (-s) [int]</font></p>

<p class="farmargin">Random seed. If 0,
&rsquo;std::time(NULL)&rsquo; is used. Default value 0.</p>

<p class="closemargin"><font class="code">--step_size (-a)
[double]</font></p>

<p class="farmargin">Step size for stochastic
gradient descent (alpha). Default value 0.01.</p>

<p class="closemargin"><font class="code">--tolerance (-t)
[double]</font></p>

<p class="farmargin">Maximum tolerance for
termination of SGD or L-BFGS. Default value 1e-07.</p>

<p class="closemargin"><font class="code">--verbose (-v)
[bool]</font></p>

<p class="farmargin">Display informational messages
and the full list of parameters and timers at the end of
execution.</p>

<p class="closemargin"><font class="code">--version (-V)
[bool]</font></p>

<p class="farmargin">Display the version of
mlpack.</p>

<p class="closemargin"><font class="code">--wolfe (-w)
[double]</font></p>

<p class="farmargin">Wolfe condition parameter for
L-BFGS. Default value 0.9.</p>

<h2>OPTIONAL OUTPUT OPTIONS
<a name="OPTIONAL OUTPUT OPTIONS"></a>
</h2>



<p class="closemargin first"><font class="code">--output_file
(-o) [string]</font></p>

<p class="farmargin">Output matrix for learned
distance matrix. Default value &rsquo;&rsquo;.</p>

<h2>ADDITIONAL INFORMATION
<a name="ADDITIONAL INFORMATION"></a>
</h2>


<h2>ADDITIONAL INFORMATION
<a name="ADDITIONAL INFORMATION"></a>
</h2>


<p class="closemargin first">For further
information, including relevant papers, citations, and
theory, For further information, including relevant papers,
citations, and theory, consult the documentation found at
http://www.mlpack.org or included with your consult the
documentation found at http://www.mlpack.org or included
with your DISTRIBUTION OF MLPACK. DISTRIBUTION OF
MLPACK.</p>
</div></body></html>
