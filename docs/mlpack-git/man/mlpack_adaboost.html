<html >
<head >

<meta name="keywords" content="mlpack, libmlpack, c++, armadillo, machine learning, data mining, classification, regression, tree-based methods, dual-tree algorithm">
<meta name="description" content="mlpack: a scalable c++ machine learning library">
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title >mlpack: a scalable c++ machine learning library</title>
</head><link rel="stylesheet" href="../../../style.css" /></link><link rel="stylesheet" href="../../style-man.css" /></link><link href="http://fonts.googleapis.com/css?family=Maven+Pro:500" rel="stylesheet" type="text/css" /></link>



<body ><br /></br>


<div class="titlebar">
   <a href="http://www.mlpack.org"><img src="../../../mlpack.png"></a>
</div>
<center >
<div class="mlnavbar">
  <div class="navcontainer">
   <div class="mlnavitem" name="mlnavmain"><a href="../../../index.html">main</a></div>
   <div class="mlnavitem" name="mlnavabout"><a href="../../../about.html">about</a></div>
   <div class="mlnavitem" name="mlnavdoc"><a href="../../../docs.html">docs</a></div>
   <div class="mlnavitem" name="mlnavhelp"><a href="../../../help.html">get help</a></div>
   <div class="mlnavitem" name="mlnavbugs"><a href="https://github.com/mlpack/mlpack">github</a></div>
  </div>
</div>
</center>
<div class="separator"></div>
<center >
<div class="mainsection smallertext manpage">

<h1 align="center">mlpack_adaboost</h1>



<h2>NAME
<a name="NAME"></a>
</h2>



<p class="closemargin first"><font class="code">mlpack_adaboost</font>
- adaboost</p>

<h2>SYNOPSIS
<a name="SYNOPSIS"></a>
</h2>



<p class="closemargin first"><font class="code">mlpack_adaboost</font>
[<font class="code">-h</font>] [<font class="code">-v</font>]</p>

<h2>DESCRIPTION
<a name="DESCRIPTION"></a>
</h2>


<p class="closemargin first">This program
implements the AdaBoost (or Adaptive Boosting) algorithm.
The variant of AdaBoost implemented here is AdaBoost.MH. It
uses a weak learner, either decision stumps or perceptrons,
and over many iterations, creates a strong learner that is a
weighted ensemble of weak learners. It runs these iterations
until a tolerance value is crossed for change in the value
of the weighted training error.</p>

<p class="closemargin first">For more
information about the algorithm, see the paper
&quot;Improved Boosting Algorithms Using Confidence-Rated
Predictions&quot;, by R.E. Schapire and Y. Singer.</p>

<p class="closemargin first">This program
allows training of an AdaBoost model, and then application
of that model to a test dataset. To train a model, a dataset
must be passed with the &rsquo;<font class="code">--training_file</font>
(<font class="code">-t</font>)&rsquo; option. Labels can be given with the
&rsquo;<font class="code">--labels_file</font> (<font class="code">-l</font>)&rsquo; option; if no
labels are specified, the labels will be assumed to be the
last column of the input dataset. Alternately, an AdaBoost
model may be loaded with the
&rsquo;<font class="code">--input_model_file</font> (<font class="code">-m</font>)&rsquo;
option.</p>

<p class="closemargin first">Once a model is
trained or loaded, it may be used to provide class
predictions for a given test dataset. A test dataset may be
specified with the &rsquo;<font class="code">--test_file</font>
(<font class="code">-T</font>)&rsquo; parameter. The predicted classes for each
point in the test dataset are output to the
&rsquo;<font class="code">--output_file</font> (<font class="code">-o</font>)&rsquo; output
parameter. The AdaBoost model itself is output to the
&rsquo;<font class="code">--output_model_file</font> (<font class="code">-M</font>)&rsquo;output
parameter.</p>

<p class="closemargin first">For example, to
run AdaBoost on an input dataset &rsquo;data.csv&rsquo; with
perceptrons as the weak learner type, storing the trained
model in &rsquo;model.bin&rsquo;, one could use the
following command:</p>

<p class="closemargin first">$ adaboost
<font class="code">--training_file</font> data.csv <font class="code">--output_file</font>
model.csv <font class="code">--weak_learner</font> perceptron</p>

<p class="closemargin first">Similarly, an
already-trained model in &rsquo;model.bin&rsquo; can be used
to provide class predictions from test data
&rsquo;test_data.csv&rsquo; and store the output in
&rsquo;predictions.csv&rsquo; with the following
command:</p>

<p class="closemargin first">$ adaboost
<font class="code">--input_model_file</font> model.bin <font class="code">--test_file</font>
test_data.csv <font class="code">--output_file</font> predictions.csv</p>

<h2>OPTIONAL INPUT OPTIONS
<a name="OPTIONAL INPUT OPTIONS"></a>
</h2>


<p class="closemargin first"><font class="code">--help (-h)
[bool]</font></p>

<p class="farmargin">Default help info.</p>

<p class="closemargin"><font class="code">--info [string]</font></p>

<p class="farmargin">Get help on a specific module
or option. Default value &rsquo;&rsquo;.
<font class="code">--input_model_file</font> (<font class="code">-m</font>) [string] Input
AdaBoost model. Default value &rsquo;&rsquo;.</p>

<p class="closemargin"><font class="code">--iterations (-i)
[int]</font></p>

<p class="farmargin">The maximum number of boosting
iterations to be run (0 will run until convergence.) Default
value 1000.</p>

<p class="closemargin"><font class="code">--labels_file (-l)
[string]</font></p>

<p class="farmargin">Labels for the training set.
Default value &rsquo;&rsquo;.</p>

<p class="closemargin"><font class="code">--test_file (-T)
[string]</font></p>

<p class="farmargin">Test dataset. Default value
&rsquo;&rsquo;.</p>

<p class="closemargin"><font class="code">--tolerance (-e)
[double]</font></p>

<p class="farmargin">The tolerance for change in
values of the weighted error during training. Default value
1e-10. <font class="code">--training_file</font> (<font class="code">-t</font>) [string] Dataset
for training AdaBoost. Default value &rsquo;&rsquo;.</p>

<p class="closemargin"><font class="code">--verbose (-v)
[bool]</font></p>

<p class="farmargin">Display informational messages
and the full list of parameters and timers at the end of
execution.</p>

<p class="closemargin"><font class="code">--version (-V)
[bool]</font></p>

<p class="farmargin">Display the version of mlpack.
<font class="code">--weak_learner</font> (<font class="code">-w</font>) [string] The type of weak
learner to use: &rsquo;decision_stump&rsquo;, or
&rsquo;perceptron&rsquo;. Default value
&rsquo;decision_stump&rsquo;.</p>

<h2>OPTIONAL OUTPUT OPTIONS
<a name="OPTIONAL OUTPUT OPTIONS"></a>
</h2>



<p class="closemargin first"><font class="code">--output_file
(-o) [string]</font></p>

<p class="farmargin">Predicted labels for the test
set. Default value &rsquo;&rsquo;.
<font class="code">--output_model_file</font> (<font class="code">-M</font>) [string] Output
trained AdaBoost model. Default value &rsquo;&rsquo;.</p>

<h2>ADDITIONAL INFORMATION
<a name="ADDITIONAL INFORMATION"></a>
</h2>


<h2>ADDITIONAL INFORMATION
<a name="ADDITIONAL INFORMATION"></a>
</h2>


<p class="closemargin first">For further
information, including relevant papers, citations, and
theory, For further information, including relevant papers,
citations, and theory, consult the documentation found at
http://www.mlpack.org or included with your consult the
documentation found at http://www.mlpack.org or included
with your DISTRIBUTION OF MLPACK. DISTRIBUTION OF
MLPACK.</p>
</div></body></html>
