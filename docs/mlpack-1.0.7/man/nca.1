.\" Text automatically generated by txt2man
.TH nca  "1" "" ""
.SH NAME
\fBnca \fP- neighborhood components analysis (nca)
.SH SYNOPSIS
.nf
.fam C
 \fBnca\fP [\fB-h\fP] [\fB-v\fP] \fB-i\fP \fIstring\fP \fB-o\fP \fIstring\fP [\fB-A\fP \fIdouble\fP] [\fB-l\fP \fIstring\fP] [\fB-L\fP] [\fB-n\fP \fIint\fP] [\fB-T\fP \fIint\fP] [\fB-M\fP \fIdouble\fP] [\fB-m\fP \fIdouble\fP] [\fB-N\fP] [\fB-B\fP \fIint\fP] [\fB-O\fP \fIstring\fP] [\fB-s\fP \fIint\fP] [\fB-a\fP \fIdouble\fP] [\fB-t\fP \fIdouble\fP] [\fB-w\fP \fIdouble\fP] 
.fam T
.fi
.fam T
.fi
.SH DESCRIPTION


This program implements Neighborhood Components Analysis, both a linear
dimensionality reduction technique and a distance learning technique. The
method seeks to improve k-nearest-neighbor classification on a dataset by
scaling the dimensions. The method is nonparametric, and does not require a
value of k. It works by using stochastic ("soft") neighbor assignments and
using optimization techniques over the gradient of the accuracy of the
neighbor assignments.
.PP
To work, this algorithm needs labeled data. It can be given as the last row
of the input dataset (\fB--input_file\fP), or alternatively in a separate file
(\fB--labels_file\fP).
.PP
This implementation of NCA uses either stochastic gradient descent or the
L_BFGS optimizer. Both of these optimizers do not guarantee global
convergence for a nonconvex objective function (NCA's objective function is
nonconvex), so the final results could depend on the random seed or other
optimizer parameters.
.PP
Stochastic gradient descent, specified by \fB--optimizer\fP "sgd", depends primarily
on two parameters: the step size (\fB--step_size\fP) and the maximum number of
iterations (\fB--max_iterations\fP). In addition, a normalized starting point can
be used (\fB--normalize\fP), which is necessary if many warnings of the form
\(cqDenominator of p_i is 0!' are given. Tuning the step size can be a tedious
affair. In general, the step size is too large if the objective is not mostly
uniformly decreasing, or if zero-valued denominator warnings are being issued.
The step size is too small if the objective is changing very slowly. Setting
the termination condition can be done easily once a good step size parameter
is found; either increase the maximum iterations to a large number and allow
SGD to find a minimum, or set the maximum iterations to 0 (allowing infinite
iterations) and set the tolerance (\fB--tolerance\fP) to define the maximum allowed
difference between objectives for SGD to terminate. Be careful -- setting the
tolerance instead of the maximum iterations can take a very long time and may
actually never converge due to the properties of the SGD optimizer.
.PP
The L-BFGS optimizer, specified by \fB--optimizer\fP "lbfgs", uses a back-tracking
line search algorithm to minimize a function. The following parameters are
used by L-BFGS: \fB--num_basis\fP (specifies the number of memory points used by
L-BFGS), \fB--max_iterations\fP, \fB--armijo_constant\fP, \fB--wolfe\fP, \fB--tolerance\fP (the
optimization is terminated when the gradient norm is below this value),
\fB--max_line_search_trials\fP, \fB--min_step\fP and \fB--max_step\fP (which both refer to the
line search routine). For more details on the L-BFGS optimizer, consult
either the MLPACK L-BFGS documentation (in lbfgs.hpp) or the vast set of
published literature on L-BFGS.
.PP
By default, the SGD optimizer is used.
.SH REQUIRED OPTIONS 

.TP
.B
\fB--input_file\fP (\fB-i\fP) [\fIstring\fP]
Input dataset to run NCA on. 
.TP
.B
\fB--output_file\fP (\fB-o\fP) [\fIstring\fP]
Output file for learned distance matrix.  
.SH OPTIONS 

.TP
.B
\fB--armijo_constant\fP (\fB-A\fP) [\fIdouble\fP]
Armijo constant for L-BFGS. Default value 0.0001. 
.TP
.B
\fB--help\fP (\fB-h\fP)
Default help info. 
.TP
.B
\fB--info\fP [\fIstring\fP]
Get help on a specific module or option.  Default value ''. 
.TP
.B
\fB--labels_file\fP (\fB-l\fP) [\fIstring\fP]
File of labels for input dataset. Default value ''. 
.TP
.B
\fB--linear_scan\fP (\fB-L\fP)
Don't shuffle the order in which data points are visited for SGD. 
.TP
.B
\fB--max_iterations\fP (\fB-n\fP) [\fIint\fP]
Maximum number of iterations for SGD or L-BFGS (0 indicates no limit). Default value 500000. 
.TP
.B
\fB--max_line_search_trials\fP (\fB-T\fP) [\fIint\fP]
Maximum number of line search trials for L-BFGS. Default value 50. 
.TP
.B
\fB--max_step\fP (\fB-M\fP) [\fIdouble\fP]
Maximum step of line search for L-BFGS. Default value 1e+20. 
.TP
.B
\fB--min_step\fP (\fB-m\fP) [\fIdouble\fP]
Minimum step of line search for L-BFGS. Default value 1e-20. 
.TP
.B
\fB--normalize\fP (\fB-N\fP)
Use a normalized starting point for optimization. This is useful for when points are far apart, or when SGD is returning NaN. 
.TP
.B
\fB--num_basis\fP (\fB-B\fP) [\fIint\fP]
Number of memory points to be stored for L-BFGS. Default value 5. 
.TP
.B
\fB--optimizer\fP (\fB-O\fP) [\fIstring\fP]
Optimizer to use; "sgd" or "lbfgs". Default value 'sgd'. 
.TP
.B
\fB--seed\fP (\fB-s\fP) [\fIint\fP]
Random seed. If 0, 'std::time(NULL)' is used.  Default value 0. 
.TP
.B
\fB--step_size\fP (\fB-a\fP) [\fIdouble\fP]
Step size for stochastic gradient descent (alpha). Default value 0.01. 
.TP
.B
\fB--tolerance\fP (\fB-t\fP) [\fIdouble\fP]
Maximum tolerance for termination of SGD or L-BFGS. Default value 1e-07. 
.TP
.B
\fB--verbose\fP (\fB-v\fP)
Display informational messages and the full list of parameters and timers at the end of execution. 
.TP
.B
\fB--wolfe\fP (\fB-w\fP) [\fIdouble\fP]
Wolfe condition parameter for L-BFGS. Default value 0.9.
.SH ADDITIONAL INFORMATION

For further information, including relevant papers, citations, and theory,
consult the documentation found at http://www.mlpack.org or included with your
distribution of MLPACK.
