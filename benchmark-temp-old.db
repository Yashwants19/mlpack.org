-- MySQL dump 10.13  Distrib 5.6.30, for debian-linux-gnu (x86_64)
--
-- Host: localhost    Database: benchmark_temp
-- ------------------------------------------------------
-- Server version	5.6.30-1

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `bootstrap`
--

DROP TABLE IF EXISTS `bootstrap`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `bootstrap` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `build_id` int(11) NOT NULL,
  `libary_id` int(11) NOT NULL,
  `metric` text NOT NULL,
  `dataset_id` int(11) NOT NULL,
  `method_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `build_id` (`build_id`),
  KEY `libary_id` (`libary_id`),
  KEY `dataset_id` (`dataset_id`),
  KEY `method_id` (`method_id`),
  CONSTRAINT `bootstrap_ibfk_1` FOREIGN KEY (`build_id`) REFERENCES `builds` (`id`) ON DELETE CASCADE,
  CONSTRAINT `bootstrap_ibfk_2` FOREIGN KEY (`libary_id`) REFERENCES `libraries` (`id`) ON DELETE CASCADE,
  CONSTRAINT `bootstrap_ibfk_3` FOREIGN KEY (`dataset_id`) REFERENCES `datasets` (`id`) ON DELETE CASCADE,
  CONSTRAINT `bootstrap_ibfk_4` FOREIGN KEY (`method_id`) REFERENCES `methods` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `bootstrap`
--

LOCK TABLES `bootstrap` WRITE;
/*!40000 ALTER TABLE `bootstrap` DISABLE KEYS */;
/*!40000 ALTER TABLE `bootstrap` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `builds`
--

DROP TABLE IF EXISTS `builds`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `builds` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `build` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  `libary_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `libary_id` (`libary_id`),
  CONSTRAINT `builds_ibfk_1` FOREIGN KEY (`libary_id`) REFERENCES `libraries` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=18 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `builds`
--

LOCK TABLES `builds` WRITE;
/*!40000 ALTER TABLE `builds` DISABLE KEYS */;
INSERT INTO `builds` VALUES (1,'2017-05-26 01:12:18',1),(2,'2017-05-29 18:00:48',2),(3,'2017-05-29 18:01:58',1),(4,'2017-05-29 18:02:15',2),(5,'2017-05-29 18:02:41',3),(6,'2017-05-31 17:52:02',4),(7,'2017-06-01 19:13:01',5),(8,'2017-06-02 09:31:24',6),(9,'2017-06-02 09:32:46',6),(10,'2017-06-02 14:59:30',7),(11,'2017-06-02 14:59:37',6),(12,'2017-06-06 22:00:33',8),(13,'2017-06-08 20:30:32',6),(14,'2017-11-23 19:56:47',6),(15,'2017-11-23 20:17:14',6),(16,'2018-07-04 00:04:14',9),(17,'2018-07-04 01:55:30',9);
/*!40000 ALTER TABLE `builds` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `datasets`
--

DROP TABLE IF EXISTS `datasets`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `datasets` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` text NOT NULL,
  `size` int(11) NOT NULL,
  `attributes` int(11) NOT NULL,
  `instances` int(11) NOT NULL,
  `type` text NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=45 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `datasets`
--

LOCK TABLES `datasets` WRITE;
/*!40000 ALTER TABLE `datasets` DISABLE KEYS */;
INSERT INTO `datasets` VALUES (1,'wine',0,13,178,'real'),(2,'cloud',0,10,2048,'real'),(3,'isolet',30,618,7797,'real'),(4,'corel-histogram',21,32,68040,'real'),(5,'covtype',72,55,581012,'real'),(6,'1000000-10-randu',76,10,1000000,'real'),(7,'mnist',122,784,70000,'real'),(8,'Twitter',163,78,583250,'real'),(9,'tinyImages100k',323,384,100000,'real'),(10,'pendigits',10,4649,256,'real'),(11,'iris',0,5,150,'real'),(12,'transfusion',0,5,518,'real'),(13,'madelon',4,501,2000,'real'),(14,'artificial',0,2,465,'real'),(15,'diabetes',0,10,442,'real'),(16,'ionosphere',0,35,351,'real'),(17,'shuttle',1,10,43500,'real'),(18,'arcene',3,10000,100,'real'),(19,'yearpredictionmsd',469,90,515345,'real'),(20,'cosExp',1,800,200,'real'),(21,'TomsHardware',8,97,28179,'real'),(22,'waveform',1,41,5000,'real'),(23,'USCensus1990',1,68,4000,'real'),(24,'oilspill',0,50,625,'real'),(25,'scene',4,295,1605,'real'),(26,'webpage',13,301,23187,'real'),(27,'mammography',0,7,7455,'real'),(28,'reuters',170,17388,5116,'real'),(29,'abalone19',0,11,2785,'real'),(30,'sickEuthyroid',0,43,2109,'real'),(31,'abalone7',0,11,2785,'real'),(32,'satellite',0,37,4290,'real'),(33,'ecoli',0,8,224,'real'),(34,'circle',0,2,150,'real'),(35,'stock',0,10,950,'real'),(36,'abalone',0,8,4177,'real'),(37,'bank8FM',1,10,8193,'real'),(38,'piano',1,224,257,'real'),(39,'optdigits',1,65,5620,'real'),(40,'cities',0,9,329,'real'),(41,'faces',1,20,10304,'real'),(42,'dexter',2,304,2001,'real'),(43,'ticdata2000',1,86,5822,'real'),(44,'vehicle',0,19,846,'real');
/*!40000 ALTER TABLE `datasets` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `libraries`
--

DROP TABLE IF EXISTS `libraries`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `libraries` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` text NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `libraries`
--

LOCK TABLES `libraries` WRITE;
/*!40000 ALTER TABLE `libraries` DISABLE KEYS */;
INSERT INTO `libraries` VALUES (1,'ann'),(2,'flann'),(3,'mlpack'),(4,'scikit'),(5,'weka'),(6,'shogun'),(7,'matlab'),(8,'mrpt'),(9,'elki');
/*!40000 ALTER TABLE `libraries` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `memory`
--

DROP TABLE IF EXISTS `memory`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `memory` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `build_id` int(11) NOT NULL,
  `libary_id` int(11) NOT NULL,
  `method_id` int(11) NOT NULL,
  `dataset_id` int(11) NOT NULL,
  `memory_info` text NOT NULL,
  PRIMARY KEY (`id`),
  KEY `build_id` (`build_id`),
  KEY `libary_id` (`libary_id`),
  KEY `dataset_id` (`dataset_id`),
  KEY `method_id` (`method_id`),
  CONSTRAINT `memory_ibfk_1` FOREIGN KEY (`build_id`) REFERENCES `builds` (`id`) ON DELETE CASCADE,
  CONSTRAINT `memory_ibfk_2` FOREIGN KEY (`libary_id`) REFERENCES `libraries` (`id`) ON DELETE CASCADE,
  CONSTRAINT `memory_ibfk_3` FOREIGN KEY (`dataset_id`) REFERENCES `datasets` (`id`) ON DELETE CASCADE,
  CONSTRAINT `memory_ibfk_4` FOREIGN KEY (`method_id`) REFERENCES `methods` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `memory`
--

LOCK TABLES `memory` WRITE;
/*!40000 ALTER TABLE `memory` DISABLE KEYS */;
/*!40000 ALTER TABLE `memory` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `method_info`
--

DROP TABLE IF EXISTS `method_info`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `method_info` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `method_id` int(11) NOT NULL,
  `info` text NOT NULL,
  PRIMARY KEY (`id`),
  KEY `method_id` (`method_id`),
  CONSTRAINT `method_info_ibfk_1` FOREIGN KEY (`method_id`) REFERENCES `methods` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=84 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `method_info`
--

LOCK TABLES `method_info` WRITE;
/*!40000 ALTER TABLE `method_info` DISABLE KEYS */;
INSERT INTO `method_info` VALUES (1,82,'Sparse Coding\n\n  An implementation of Sparse Coding with Dictionary Learning, which achieves\n  sparsity via an l1-norm regularizer on the codes (LASSO) or an (l1+l2)-norm\n  regularizer on the codes (the Elastic Net).  Given a dense data matrix X with\n  n points and d dimensions, sparse coding seeks to find a dense dictionary\n  matrix D with k atoms in d dimensions, and a sparse coding matrix Z with n\n  points in k dimensions.\n  \n  The original data matrix X can then be reconstructed as D * Z.  Therefore,\n  this program finds a representation of each point in X as a sparse linear\n  combination of atoms in the dictionary D.\n  \n  The sparse coding is found with an algorithm which alternates between a\n  dictionary step, which updates the dictionary D, and a sparse coding step,\n  which updates the sparse coding matrix.\n  \n  Once a dictionary D is found, the sparse coding model may be used to encode\n  other matrices, and saved for future usage.\n  \n  To run this program, either an input matrix or an already-saved sparse coding\n  model must be specified.  An input matrix may be specified with the\n  --training_file (-t) option, along with the number of atoms in the dictionary\n  (--atoms, or -k).  It is also possible to specify an initial dictionary for\n  the optimization, with the --initial_dictionary (-i) option. An input model\n  may be specified with the --input_model_file (-m) option. There are also other\n  training options available.\n  \n  As an example, to build a sparse coding model on the dataset in data.csv using\n  200 atoms and an l1-regularization parameter of 0.1, saving the model into\n  model.xml, use \n  \n  $ sparse_coding -t data.csv -k 200 -l 0.1 -M model.xml\n  \n  Then, this model could be used to encode a new matrix, otherdata.csv, and save\n  the output codes to codes.csv:\n  \n  $ sparse_coding -m model.xml -T otherdata.csv -c codes.csv\n\n'),(2,80,'Sparse Coding\n\n  An implementation of Sparse Coding with Dictionary Learning, which achieves\n  sparsity via an l1-norm regularizer on the codes (LASSO) or an (l1+l2)-norm\n  regularizer on the codes (the Elastic Net).  Given a dense data matrix X with\n  n points and d dimensions, sparse coding seeks to find a dense dictionary\n  matrix D with k atoms in d dimensions, and a sparse coding matrix Z with n\n  points in k dimensions.\n  \n  The original data matrix X can then be reconstructed as D * Z.  Therefore,\n  this program finds a representation of each point in X as a sparse linear\n  combination of atoms in the dictionary D.\n  \n  The sparse coding is found with an algorithm which alternates between a\n  dictionary step, which updates the dictionary D, and a sparse coding step,\n  which updates the sparse coding matrix.\n  \n  Once a dictionary D is found, the sparse coding model may be used to encode\n  other matrices, and saved for future usage.\n  \n  To run this program, either an input matrix or an already-saved sparse coding\n  model must be specified.  An input matrix may be specified with the\n  --training_file (-t) option, along with the number of atoms in the dictionary\n  (--atoms, or -k).  It is also possible to specify an initial dictionary for\n  the optimization, with the --initial_dictionary (-i) option. An input model\n  may be specified with the --input_model_file (-m) option. There are also other\n  training options available.\n  \n  As an example, to build a sparse coding model on the dataset in data.csv using\n  200 atoms and an l1-regularization parameter of 0.1, saving the model into\n  model.xml, use \n  \n  $ sparse_coding -t data.csv -k 200 -l 0.1 -M model.xml\n  \n  Then, this model could be used to encode a new matrix, otherdata.csv, and save\n  the output codes to codes.csv:\n  \n  $ sparse_coding -m model.xml -T otherdata.csv -c codes.csv\n\n'),(3,81,'Sparse Coding\n\n  An implementation of Sparse Coding with Dictionary Learning, which achieves\n  sparsity via an l1-norm regularizer on the codes (LASSO) or an (l1+l2)-norm\n  regularizer on the codes (the Elastic Net).  Given a dense data matrix X with\n  n points and d dimensions, sparse coding seeks to find a dense dictionary\n  matrix D with k atoms in d dimensions, and a sparse coding matrix Z with n\n  points in k dimensions.\n  \n  The original data matrix X can then be reconstructed as D * Z.  Therefore,\n  this program finds a representation of each point in X as a sparse linear\n  combination of atoms in the dictionary D.\n  \n  The sparse coding is found with an algorithm which alternates between a\n  dictionary step, which updates the dictionary D, and a sparse coding step,\n  which updates the sparse coding matrix.\n  \n  Once a dictionary D is found, the sparse coding model may be used to encode\n  other matrices, and saved for future usage.\n  \n  To run this program, either an input matrix or an already-saved sparse coding\n  model must be specified.  An input matrix may be specified with the\n  --training_file (-t) option, along with the number of atoms in the dictionary\n  (--atoms, or -k).  It is also possible to specify an initial dictionary for\n  the optimization, with the --initial_dictionary (-i) option. An input model\n  may be specified with the --input_model_file (-m) option. There are also other\n  training options available.\n  \n  As an example, to build a sparse coding model on the dataset in data.csv using\n  200 atoms and an l1-regularization parameter of 0.1, saving the model into\n  model.xml, use \n  \n  $ sparse_coding -t data.csv -k 200 -l 0.1 -M model.xml\n  \n  Then, this model could be used to encode a new matrix, otherdata.csv, and save\n  the output codes to codes.csv:\n  \n  $ sparse_coding -m model.xml -T otherdata.csv -c codes.csv\n\n'),(4,66,'Parametric Naive Bayes Classifier\n\n  This program trains the Naive Bayes classifier on the given labeled training\n  set, or loads a model from the given model file, and then may use that trained\n  model to classify the points in a given test set.\n  \n  Labels are expected to be the last row of the training set (--training_file),\n  but labels can also be passed in separately as their own file (--labels_file).\n   If training is not desired, a pre-existing model can be loaded with the\n  --input_model_file (-m) option.\n  \n  The \'--incremental_variance\' option can be used to force the training to use\n  an incremental algorithm for calculating variance.  This is slower, but can\n  help avoid loss of precision in some cases.\n  \n  If classifying a test set is desired, the test set should be in the file\n  specified with the --test_file (-T) option, and the classifications will be\n  saved to the file specified with the --output_file (-o) option.  If saving a\n  trained model is desired, the --output_model_file (-M) option should be\n  given.\n\n'),(5,67,'Parametric Naive Bayes Classifier\n\n  This program trains the Naive Bayes classifier on the given labeled training\n  set, or loads a model from the given model file, and then may use that trained\n  model to classify the points in a given test set.\n  \n  Labels are expected to be the last row of the training set (--training_file),\n  but labels can also be passed in separately as their own file (--labels_file).\n   If training is not desired, a pre-existing model can be loaded with the\n  --input_model_file (-m) option.\n  \n  The \'--incremental_variance\' option can be used to force the training to use\n  an incremental algorithm for calculating variance.  This is slower, but can\n  help avoid loss of precision in some cases.\n  \n  If classifying a test set is desired, the test set should be in the file\n  specified with the --test_file (-T) option, and the classifications will be\n  saved to the file specified with the --output_file (-o) option.  If saving a\n  trained model is desired, the --output_model_file (-M) option should be\n  given.\n\n'),(6,39,'Hidden Markov Model (HMM) Viterbi State Prediction\n\n  This utility takes an already-trained HMM (--model_file) and evaluates the\n  most probably hidden state sequence of a given sequence of observations\n  (--input_file), using the Viterbi algorithm.  The computed state sequence is\n  saved to the specified output file (--output_file).\n\nRequired input options:\n\n  --input_file (-i) [string]    File containing observations,\n  --model_file (-m) [string]    File containing HMM.\n\n'),(7,84,'Neighborhood Components Analysis (NCA)\n\n  This program implements Neighborhood Components Analysis, both a linear\n  dimensionality reduction technique and a distance learning technique.  The\n  method seeks to improve k-nearest-neighbor classification on a dataset by\n  scaling the dimensions.  The method is nonparametric, and does not require a\n  value of k.  It works by using stochastic (\"soft\") neighbor assignments and\n  using optimization techniques over the gradient of the accuracy of the\n  neighbor assignments.\n  \n  To work, this algorithm needs labeled data.  It can be given as the last row\n  of the input dataset (--input_file), or alternatively in a separate file\n  (--labels_file).\n  \n  This implementation of NCA uses stochastic gradient descent, mini-batch\n  stochastic gradient descent, or the L_BFGS optimizer.  These optimizers do not\n  guarantee global convergence for a nonconvex objective function (NCA\'s\n  objective function is nonconvex), so the final results could depend on the\n  random seed or other optimizer parameters.\n  \n  Stochastic gradient descent, specified by --optimizer \"sgd\", depends primarily\n  on two parameters: the step size (--step_size) and the maximum number of\n  iterations (--max_iterations).  In addition, a normalized starting point can\n  be used (--normalize), which is necessary if many warnings of the form\n  \'Denominator of p_i is 0!\' are given.  Tuning the step size can be a tedious\n  affair.  In general, the step size is too large if the objective is not mostly\n  uniformly decreasing, or if zero-valued denominator warnings are being issued.\n   The step size is too small if the objective is changing very slowly.  Setting\n  the termination condition can be done easily once a good step size parameter\n  is found; either increase the maximum iterations to a large number and allow\n  SGD to find a minimum, or set the maximum iterations to 0 (allowing infinite\n  iterations) and set the tolerance (--tolerance) to define the maximum allowed\n  difference between objectives for SGD to terminate.  Be careful---setting the\n  tolerance instead of the maximum iterations can take a very long time and may\n  actually never converge due to the properties of the SGD optimizer. Note that\n  a single iteration of SGD refers to a single point, so to take a single pass\n  over the dataset, set --max_iterations equal to the number of points in the\n  dataset.\n  \n  The mini-batch SGD optimizer, specified by --optimizer \"minibatch-sgd\", has\n  the same parameters as SGD, but the batch size may also be specified with the\n  --batch_size (-b) option.  Each iteration of mini-batch SGD refers to a single\n  mini-batch.\n  \n  The L-BFGS optimizer, specified by --optimizer \"lbfgs\", uses a back-tracking\n  line search algorithm to minimize a function.  The following parameters are\n  used by L-BFGS: --num_basis (specifies the number of memory points used by\n  L-BFGS), --max_iterations, --armijo_constant, --wolfe, --tolerance (the\n  optimization is terminated when the gradient norm is below this value),\n  --max_line_search_trials, --min_step and --max_step (which both refer to the\n  line search routine).  For more details on the L-BFGS optimizer, consult\n  either the mlpack L-BFGS documentation (in lbfgs.hpp) or the vast set of\n  published literature on L-BFGS.\n  \n  By default, the SGD optimizer is used.\n\nRequired input options:\n\n  --input_file (-i) [string]    Input dataset to run NCA on.\n\n'),(8,88,'Neighborhood Components Analysis (NCA)\n\n  This program implements Neighborhood Components Analysis, both a linear\n  dimensionality reduction technique and a distance learning technique.  The\n  method seeks to improve k-nearest-neighbor classification on a dataset by\n  scaling the dimensions.  The method is nonparametric, and does not require a\n  value of k.  It works by using stochastic (\"soft\") neighbor assignments and\n  using optimization techniques over the gradient of the accuracy of the\n  neighbor assignments.\n  \n  To work, this algorithm needs labeled data.  It can be given as the last row\n  of the input dataset (--input_file), or alternatively in a separate file\n  (--labels_file).\n  \n  This implementation of NCA uses stochastic gradient descent, mini-batch\n  stochastic gradient descent, or the L_BFGS optimizer.  These optimizers do not\n  guarantee global convergence for a nonconvex objective function (NCA\'s\n  objective function is nonconvex), so the final results could depend on the\n  random seed or other optimizer parameters.\n  \n  Stochastic gradient descent, specified by --optimizer \"sgd\", depends primarily\n  on two parameters: the step size (--step_size) and the maximum number of\n  iterations (--max_iterations).  In addition, a normalized starting point can\n  be used (--normalize), which is necessary if many warnings of the form\n  \'Denominator of p_i is 0!\' are given.  Tuning the step size can be a tedious\n  affair.  In general, the step size is too large if the objective is not mostly\n  uniformly decreasing, or if zero-valued denominator warnings are being issued.\n   The step size is too small if the objective is changing very slowly.  Setting\n  the termination condition can be done easily once a good step size parameter\n  is found; either increase the maximum iterations to a large number and allow\n  SGD to find a minimum, or set the maximum iterations to 0 (allowing infinite\n  iterations) and set the tolerance (--tolerance) to define the maximum allowed\n  difference between objectives for SGD to terminate.  Be careful---setting the\n  tolerance instead of the maximum iterations can take a very long time and may\n  actually never converge due to the properties of the SGD optimizer. Note that\n  a single iteration of SGD refers to a single point, so to take a single pass\n  over the dataset, set --max_iterations equal to the number of points in the\n  dataset.\n  \n  The mini-batch SGD optimizer, specified by --optimizer \"minibatch-sgd\", has\n  the same parameters as SGD, but the batch size may also be specified with the\n  --batch_size (-b) option.  Each iteration of mini-batch SGD refers to a single\n  mini-batch.\n  \n  The L-BFGS optimizer, specified by --optimizer \"lbfgs\", uses a back-tracking\n  line search algorithm to minimize a function.  The following parameters are\n  used by L-BFGS: --num_basis (specifies the number of memory points used by\n  L-BFGS), --max_iterations, --armijo_constant, --wolfe, --tolerance (the\n  optimization is terminated when the gradient norm is below this value),\n  --max_line_search_trials, --min_step and --max_step (which both refer to the\n  line search routine).  For more details on the L-BFGS optimizer, consult\n  either the mlpack L-BFGS documentation (in lbfgs.hpp) or the vast set of\n  published literature on L-BFGS.\n  \n  By default, the SGD optimizer is used.\n\nRequired input options:\n\n  --input_file (-i) [string]    Input dataset to run NCA on.\n\n'),(9,87,'Neighborhood Components Analysis (NCA)\n\n  This program implements Neighborhood Components Analysis, both a linear\n  dimensionality reduction technique and a distance learning technique.  The\n  method seeks to improve k-nearest-neighbor classification on a dataset by\n  scaling the dimensions.  The method is nonparametric, and does not require a\n  value of k.  It works by using stochastic (\"soft\") neighbor assignments and\n  using optimization techniques over the gradient of the accuracy of the\n  neighbor assignments.\n  \n  To work, this algorithm needs labeled data.  It can be given as the last row\n  of the input dataset (--input_file), or alternatively in a separate file\n  (--labels_file).\n  \n  This implementation of NCA uses stochastic gradient descent, mini-batch\n  stochastic gradient descent, or the L_BFGS optimizer.  These optimizers do not\n  guarantee global convergence for a nonconvex objective function (NCA\'s\n  objective function is nonconvex), so the final results could depend on the\n  random seed or other optimizer parameters.\n  \n  Stochastic gradient descent, specified by --optimizer \"sgd\", depends primarily\n  on two parameters: the step size (--step_size) and the maximum number of\n  iterations (--max_iterations).  In addition, a normalized starting point can\n  be used (--normalize), which is necessary if many warnings of the form\n  \'Denominator of p_i is 0!\' are given.  Tuning the step size can be a tedious\n  affair.  In general, the step size is too large if the objective is not mostly\n  uniformly decreasing, or if zero-valued denominator warnings are being issued.\n   The step size is too small if the objective is changing very slowly.  Setting\n  the termination condition can be done easily once a good step size parameter\n  is found; either increase the maximum iterations to a large number and allow\n  SGD to find a minimum, or set the maximum iterations to 0 (allowing infinite\n  iterations) and set the tolerance (--tolerance) to define the maximum allowed\n  difference between objectives for SGD to terminate.  Be careful---setting the\n  tolerance instead of the maximum iterations can take a very long time and may\n  actually never converge due to the properties of the SGD optimizer. Note that\n  a single iteration of SGD refers to a single point, so to take a single pass\n  over the dataset, set --max_iterations equal to the number of points in the\n  dataset.\n  \n  The mini-batch SGD optimizer, specified by --optimizer \"minibatch-sgd\", has\n  the same parameters as SGD, but the batch size may also be specified with the\n  --batch_size (-b) option.  Each iteration of mini-batch SGD refers to a single\n  mini-batch.\n  \n  The L-BFGS optimizer, specified by --optimizer \"lbfgs\", uses a back-tracking\n  line search algorithm to minimize a function.  The following parameters are\n  used by L-BFGS: --num_basis (specifies the number of memory points used by\n  L-BFGS), --max_iterations, --armijo_constant, --wolfe, --tolerance (the\n  optimization is terminated when the gradient norm is below this value),\n  --max_line_search_trials, --min_step and --max_step (which both refer to the\n  line search routine).  For more details on the L-BFGS optimizer, consult\n  either the mlpack L-BFGS documentation (in lbfgs.hpp) or the vast set of\n  published literature on L-BFGS.\n  \n  By default, the SGD optimizer is used.\n\nRequired input options:\n\n  --input_file (-i) [string]    Input dataset to run NCA on.\n\n'),(10,86,'Neighborhood Components Analysis (NCA)\n\n  This program implements Neighborhood Components Analysis, both a linear\n  dimensionality reduction technique and a distance learning technique.  The\n  method seeks to improve k-nearest-neighbor classification on a dataset by\n  scaling the dimensions.  The method is nonparametric, and does not require a\n  value of k.  It works by using stochastic (\"soft\") neighbor assignments and\n  using optimization techniques over the gradient of the accuracy of the\n  neighbor assignments.\n  \n  To work, this algorithm needs labeled data.  It can be given as the last row\n  of the input dataset (--input_file), or alternatively in a separate file\n  (--labels_file).\n  \n  This implementation of NCA uses stochastic gradient descent, mini-batch\n  stochastic gradient descent, or the L_BFGS optimizer.  These optimizers do not\n  guarantee global convergence for a nonconvex objective function (NCA\'s\n  objective function is nonconvex), so the final results could depend on the\n  random seed or other optimizer parameters.\n  \n  Stochastic gradient descent, specified by --optimizer \"sgd\", depends primarily\n  on two parameters: the step size (--step_size) and the maximum number of\n  iterations (--max_iterations).  In addition, a normalized starting point can\n  be used (--normalize), which is necessary if many warnings of the form\n  \'Denominator of p_i is 0!\' are given.  Tuning the step size can be a tedious\n  affair.  In general, the step size is too large if the objective is not mostly\n  uniformly decreasing, or if zero-valued denominator warnings are being issued.\n   The step size is too small if the objective is changing very slowly.  Setting\n  the termination condition can be done easily once a good step size parameter\n  is found; either increase the maximum iterations to a large number and allow\n  SGD to find a minimum, or set the maximum iterations to 0 (allowing infinite\n  iterations) and set the tolerance (--tolerance) to define the maximum allowed\n  difference between objectives for SGD to terminate.  Be careful---setting the\n  tolerance instead of the maximum iterations can take a very long time and may\n  actually never converge due to the properties of the SGD optimizer. Note that\n  a single iteration of SGD refers to a single point, so to take a single pass\n  over the dataset, set --max_iterations equal to the number of points in the\n  dataset.\n  \n  The mini-batch SGD optimizer, specified by --optimizer \"minibatch-sgd\", has\n  the same parameters as SGD, but the batch size may also be specified with the\n  --batch_size (-b) option.  Each iteration of mini-batch SGD refers to a single\n  mini-batch.\n  \n  The L-BFGS optimizer, specified by --optimizer \"lbfgs\", uses a back-tracking\n  line search algorithm to minimize a function.  The following parameters are\n  used by L-BFGS: --num_basis (specifies the number of memory points used by\n  L-BFGS), --max_iterations, --armijo_constant, --wolfe, --tolerance (the\n  optimization is terminated when the gradient norm is below this value),\n  --max_line_search_trials, --min_step and --max_step (which both refer to the\n  line search routine).  For more details on the L-BFGS optimizer, consult\n  either the mlpack L-BFGS documentation (in lbfgs.hpp) or the vast set of\n  published literature on L-BFGS.\n  \n  By default, the SGD optimizer is used.\n\nRequired input options:\n\n  --input_file (-i) [string]    Input dataset to run NCA on.\n\n'),(11,89,'Neighborhood Components Analysis (NCA)\n\n  This program implements Neighborhood Components Analysis, both a linear\n  dimensionality reduction technique and a distance learning technique.  The\n  method seeks to improve k-nearest-neighbor classification on a dataset by\n  scaling the dimensions.  The method is nonparametric, and does not require a\n  value of k.  It works by using stochastic (\"soft\") neighbor assignments and\n  using optimization techniques over the gradient of the accuracy of the\n  neighbor assignments.\n  \n  To work, this algorithm needs labeled data.  It can be given as the last row\n  of the input dataset (--input_file), or alternatively in a separate file\n  (--labels_file).\n  \n  This implementation of NCA uses stochastic gradient descent, mini-batch\n  stochastic gradient descent, or the L_BFGS optimizer.  These optimizers do not\n  guarantee global convergence for a nonconvex objective function (NCA\'s\n  objective function is nonconvex), so the final results could depend on the\n  random seed or other optimizer parameters.\n  \n  Stochastic gradient descent, specified by --optimizer \"sgd\", depends primarily\n  on two parameters: the step size (--step_size) and the maximum number of\n  iterations (--max_iterations).  In addition, a normalized starting point can\n  be used (--normalize), which is necessary if many warnings of the form\n  \'Denominator of p_i is 0!\' are given.  Tuning the step size can be a tedious\n  affair.  In general, the step size is too large if the objective is not mostly\n  uniformly decreasing, or if zero-valued denominator warnings are being issued.\n   The step size is too small if the objective is changing very slowly.  Setting\n  the termination condition can be done easily once a good step size parameter\n  is found; either increase the maximum iterations to a large number and allow\n  SGD to find a minimum, or set the maximum iterations to 0 (allowing infinite\n  iterations) and set the tolerance (--tolerance) to define the maximum allowed\n  difference between objectives for SGD to terminate.  Be careful---setting the\n  tolerance instead of the maximum iterations can take a very long time and may\n  actually never converge due to the properties of the SGD optimizer. Note that\n  a single iteration of SGD refers to a single point, so to take a single pass\n  over the dataset, set --max_iterations equal to the number of points in the\n  dataset.\n  \n  The mini-batch SGD optimizer, specified by --optimizer \"minibatch-sgd\", has\n  the same parameters as SGD, but the batch size may also be specified with the\n  --batch_size (-b) option.  Each iteration of mini-batch SGD refers to a single\n  mini-batch.\n  \n  The L-BFGS optimizer, specified by --optimizer \"lbfgs\", uses a back-tracking\n  line search algorithm to minimize a function.  The following parameters are\n  used by L-BFGS: --num_basis (specifies the number of memory points used by\n  L-BFGS), --max_iterations, --armijo_constant, --wolfe, --tolerance (the\n  optimization is terminated when the gradient norm is below this value),\n  --max_line_search_trials, --min_step and --max_step (which both refer to the\n  line search routine).  For more details on the L-BFGS optimizer, consult\n  either the mlpack L-BFGS documentation (in lbfgs.hpp) or the vast set of\n  published literature on L-BFGS.\n  \n  By default, the SGD optimizer is used.\n\nRequired input options:\n\n  --input_file (-i) [string]    Input dataset to run NCA on.\n\n'),(12,85,'Neighborhood Components Analysis (NCA)\n\n  This program implements Neighborhood Components Analysis, both a linear\n  dimensionality reduction technique and a distance learning technique.  The\n  method seeks to improve k-nearest-neighbor classification on a dataset by\n  scaling the dimensions.  The method is nonparametric, and does not require a\n  value of k.  It works by using stochastic (\"soft\") neighbor assignments and\n  using optimization techniques over the gradient of the accuracy of the\n  neighbor assignments.\n  \n  To work, this algorithm needs labeled data.  It can be given as the last row\n  of the input dataset (--input_file), or alternatively in a separate file\n  (--labels_file).\n  \n  This implementation of NCA uses stochastic gradient descent, mini-batch\n  stochastic gradient descent, or the L_BFGS optimizer.  These optimizers do not\n  guarantee global convergence for a nonconvex objective function (NCA\'s\n  objective function is nonconvex), so the final results could depend on the\n  random seed or other optimizer parameters.\n  \n  Stochastic gradient descent, specified by --optimizer \"sgd\", depends primarily\n  on two parameters: the step size (--step_size) and the maximum number of\n  iterations (--max_iterations).  In addition, a normalized starting point can\n  be used (--normalize), which is necessary if many warnings of the form\n  \'Denominator of p_i is 0!\' are given.  Tuning the step size can be a tedious\n  affair.  In general, the step size is too large if the objective is not mostly\n  uniformly decreasing, or if zero-valued denominator warnings are being issued.\n   The step size is too small if the objective is changing very slowly.  Setting\n  the termination condition can be done easily once a good step size parameter\n  is found; either increase the maximum iterations to a large number and allow\n  SGD to find a minimum, or set the maximum iterations to 0 (allowing infinite\n  iterations) and set the tolerance (--tolerance) to define the maximum allowed\n  difference between objectives for SGD to terminate.  Be careful---setting the\n  tolerance instead of the maximum iterations can take a very long time and may\n  actually never converge due to the properties of the SGD optimizer. Note that\n  a single iteration of SGD refers to a single point, so to take a single pass\n  over the dataset, set --max_iterations equal to the number of points in the\n  dataset.\n  \n  The mini-batch SGD optimizer, specified by --optimizer \"minibatch-sgd\", has\n  the same parameters as SGD, but the batch size may also be specified with the\n  --batch_size (-b) option.  Each iteration of mini-batch SGD refers to a single\n  mini-batch.\n  \n  The L-BFGS optimizer, specified by --optimizer \"lbfgs\", uses a back-tracking\n  line search algorithm to minimize a function.  The following parameters are\n  used by L-BFGS: --num_basis (specifies the number of memory points used by\n  L-BFGS), --max_iterations, --armijo_constant, --wolfe, --tolerance (the\n  optimization is terminated when the gradient norm is below this value),\n  --max_line_search_trials, --min_step and --max_step (which both refer to the\n  line search routine).  For more details on the L-BFGS optimizer, consult\n  either the mlpack L-BFGS documentation (in lbfgs.hpp) or the vast set of\n  published literature on L-BFGS.\n  \n  By default, the SGD optimizer is used.\n\nRequired input options:\n\n  --input_file (-i) [string]    Input dataset to run NCA on.\n\n'),(13,79,'Range Search\n\n  This program implements range search with a Euclidean distance metric. For a\n  given query point, a given range, and a given set of reference points, the\n  program will return all of the reference points with distance to the query\n  point in the given range.  This is performed for an entire set of query\n  points. You may specify a separate set of reference and query points, or only\n  a reference set -- which is then used as both the reference and query set. \n  The given range is taken to be inclusive (that is, points with a distance\n  exactly equal to the minimum and maximum of the range are included in the\n  results).\n  \n  For example, the following will calculate the points within the range [2, 5]\n  of each point in \'input.csv\' and store the distances in \'distances.csv\' and\n  the neighbors in \'neighbors.csv\':\n  \n  $ range_search --min=2 --max=5 --reference_file=input.csv\n    --distances_file=distances.csv --neighbors_file=neighbors.csv\n  \n  The output files are organized such that line i corresponds to the points\n  found for query point i.  Because sometimes 0 points may be found in the given\n  range, lines of the output files may be empty.  The points are not ordered in\n  any specific manner.\n  \n  Because the number of points returned for each query point may differ, the\n  resultant CSV-like files may not be loadable by many programs.  However, at\n  this time a better way to store this non-square result is not known.  As a\n  result, any output files will be written as CSVs in this manner, regardless of\n  the given extension.\n\n'),(14,77,'Range Search\n\n  This program implements range search with a Euclidean distance metric. For a\n  given query point, a given range, and a given set of reference points, the\n  program will return all of the reference points with distance to the query\n  point in the given range.  This is performed for an entire set of query\n  points. You may specify a separate set of reference and query points, or only\n  a reference set -- which is then used as both the reference and query set. \n  The given range is taken to be inclusive (that is, points with a distance\n  exactly equal to the minimum and maximum of the range are included in the\n  results).\n  \n  For example, the following will calculate the points within the range [2, 5]\n  of each point in \'input.csv\' and store the distances in \'distances.csv\' and\n  the neighbors in \'neighbors.csv\':\n  \n  $ range_search --min=2 --max=5 --reference_file=input.csv\n    --distances_file=distances.csv --neighbors_file=neighbors.csv\n  \n  The output files are organized such that line i corresponds to the points\n  found for query point i.  Because sometimes 0 points may be found in the given\n  range, lines of the output files may be empty.  The points are not ordered in\n  any specific manner.\n  \n  Because the number of points returned for each query point may differ, the\n  resultant CSV-like files may not be loadable by many programs.  However, at\n  this time a better way to store this non-square result is not known.  As a\n  result, any output files will be written as CSVs in this manner, regardless of\n  the given extension.\n\n'),(15,78,'Range Search\n\n  This program implements range search with a Euclidean distance metric. For a\n  given query point, a given range, and a given set of reference points, the\n  program will return all of the reference points with distance to the query\n  point in the given range.  This is performed for an entire set of query\n  points. You may specify a separate set of reference and query points, or only\n  a reference set -- which is then used as both the reference and query set. \n  The given range is taken to be inclusive (that is, points with a distance\n  exactly equal to the minimum and maximum of the range are included in the\n  results).\n  \n  For example, the following will calculate the points within the range [2, 5]\n  of each point in \'input.csv\' and store the distances in \'distances.csv\' and\n  the neighbors in \'neighbors.csv\':\n  \n  $ range_search --min=2 --max=5 --reference_file=input.csv\n    --distances_file=distances.csv --neighbors_file=neighbors.csv\n  \n  The output files are organized such that line i corresponds to the points\n  found for query point i.  Because sometimes 0 points may be found in the given\n  range, lines of the output files may be empty.  The points are not ordered in\n  any specific manner.\n  \n  Because the number of points returned for each query point may differ, the\n  resultant CSV-like files may not be loadable by many programs.  However, at\n  this time a better way to store this non-square result is not known.  As a\n  result, any output files will be written as CSVs in this manner, regardless of\n  the given extension.\n\n'),(16,70,'Perceptron\n\n  This program implements a perceptron, which is a single level neural network.\n  The perceptron makes its predictions based on a linear predictor function\n  combining a set of weights with the feature vector.  The perceptron learning\n  rule is able to converge, given enough iterations using the --max_iterations\n  (-n) parameter, if the data supplied is linearly separable.  The perceptron is\n  parameterized by a matrix of weight vectors that denote the numerical weights\n  of the neural network.\n  \n  This program allows loading a perceptron from a model (-m) or training a\n  perceptron given training data (-t), or both those things at once.  In\n  addition, this program allows classification on a test dataset (-T) and will\n  save the classification results to the given output file (-o).  The perceptron\n  model itself may be saved with a file specified using the -M option.\n  \n  The training data given with the -t option should have class labels as its\n  last dimension (so, if the training data is in CSV format, labels should be\n  the last column).  Alternately, the -l (--labels_file) option may be used to\n  specify a separate file of labels.\n  \n  All these options make it easy to train a perceptron, and then re-use that\n  perceptron for later classification.  The invocation below trains a perceptron\n  on \'training_data.csv\' (and \'training_labels.csv)\' and saves the model to\n  \'perceptron.xml\'.\n  \n  $ perceptron -t training_data.csv -l training_labels.csv -M perceptron.xml\n  \n  Then, this model can be re-used for classification on \'test_data.csv\'.  The\n  example below does precisely that, saving the predicted classes to\n  \'predictions.csv\'.\n  \n  $ perceptron -m perceptron.xml -T test_data.csv -o predictions.csv\n  \n  Note that all of the options may be specified at once: predictions may be\n  calculated right after training a model, and model training can occur even if\n  an existing perceptron model is passed with -m (--input_model_file).  However,\n  note that the number of classes and the dimensionality of all data must match.\n   So you cannot pass a perceptron model trained on 2 classes and then re-train\n  with a 4-class dataset.  Similarly, attempting classification on a\n  3-dimensional dataset with a perceptron that has been trained on 8 dimensions\n  will cause an error.\n\n'),(17,60,'K-Rank-Approximate-Nearest-Neighbors (kRANN)\n\n  This program will calculate the k rank-approximate-nearest-neighbors of a set\n  of points. You may specify a separate set of reference points and query\n  points, or just a reference set which will be used as both the reference and\n  query set. You must specify the rank approximation (in %) (and optionally the\n  success probability).\n  \n  For example, the following will return 5 neighbors from the top 0.1% of the\n  data (with probability 0.95) for each point in \'input.csv\' and store the\n  distances in \'distances.csv\' and the neighbors in the file \'neighbors.csv\':\n  \n  $ allkrann -k 5 -r input.csv -d distances.csv -n neighbors.csv --tau 0.1\n  \n  Note that tau must be set such that the number of points in the corresponding\n  percentile of the data is greater than k.  Thus, if we choose tau = 0.1 with a\n  dataset of 1000 points and k = 5, then we are attempting to choose 5 nearest\n  neighbors out of the closest 1 point -- this is invalid and the program will\n  terminate with an error message.\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(18,61,'K-Rank-Approximate-Nearest-Neighbors (kRANN)\n\n  This program will calculate the k rank-approximate-nearest-neighbors of a set\n  of points. You may specify a separate set of reference points and query\n  points, or just a reference set which will be used as both the reference and\n  query set. You must specify the rank approximation (in %) (and optionally the\n  success probability).\n  \n  For example, the following will return 5 neighbors from the top 0.1% of the\n  data (with probability 0.95) for each point in \'input.csv\' and store the\n  distances in \'distances.csv\' and the neighbors in the file \'neighbors.csv\':\n  \n  $ allkrann -k 5 -r input.csv -d distances.csv -n neighbors.csv --tau 0.1\n  \n  Note that tau must be set such that the number of points in the corresponding\n  percentile of the data is greater than k.  Thus, if we choose tau = 0.1 with a\n  dataset of 1000 points and k = 5, then we are attempting to choose 5 nearest\n  neighbors out of the closest 1 point -- this is invalid and the program will\n  terminate with an error message.\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(19,59,'K-Rank-Approximate-Nearest-Neighbors (kRANN)\n\n  This program will calculate the k rank-approximate-nearest-neighbors of a set\n  of points. You may specify a separate set of reference points and query\n  points, or just a reference set which will be used as both the reference and\n  query set. You must specify the rank approximation (in %) (and optionally the\n  success probability).\n  \n  For example, the following will return 5 neighbors from the top 0.1% of the\n  data (with probability 0.95) for each point in \'input.csv\' and store the\n  distances in \'distances.csv\' and the neighbors in the file \'neighbors.csv\':\n  \n  $ allkrann -k 5 -r input.csv -d distances.csv -n neighbors.csv --tau 0.1\n  \n  Note that tau must be set such that the number of points in the corresponding\n  percentile of the data is greater than k.  Thus, if we choose tau = 0.1 with a\n  dataset of 1000 points and k = 5, then we are attempting to choose 5 nearest\n  neighbors out of the closest 1 point -- this is invalid and the program will\n  terminate with an error message.\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(20,57,'K-Rank-Approximate-Nearest-Neighbors (kRANN)\n\n  This program will calculate the k rank-approximate-nearest-neighbors of a set\n  of points. You may specify a separate set of reference points and query\n  points, or just a reference set which will be used as both the reference and\n  query set. You must specify the rank approximation (in %) (and optionally the\n  success probability).\n  \n  For example, the following will return 5 neighbors from the top 0.1% of the\n  data (with probability 0.95) for each point in \'input.csv\' and store the\n  distances in \'distances.csv\' and the neighbors in the file \'neighbors.csv\':\n  \n  $ allkrann -k 5 -r input.csv -d distances.csv -n neighbors.csv --tau 0.1\n  \n  Note that tau must be set such that the number of points in the corresponding\n  percentile of the data is greater than k.  Thus, if we choose tau = 0.1 with a\n  dataset of 1000 points and k = 5, then we are attempting to choose 5 nearest\n  neighbors out of the closest 1 point -- this is invalid and the program will\n  terminate with an error message.\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(21,58,'K-Rank-Approximate-Nearest-Neighbors (kRANN)\n\n  This program will calculate the k rank-approximate-nearest-neighbors of a set\n  of points. You may specify a separate set of reference points and query\n  points, or just a reference set which will be used as both the reference and\n  query set. You must specify the rank approximation (in %) (and optionally the\n  success probability).\n  \n  For example, the following will return 5 neighbors from the top 0.1% of the\n  data (with probability 0.95) for each point in \'input.csv\' and store the\n  distances in \'distances.csv\' and the neighbors in the file \'neighbors.csv\':\n  \n  $ allkrann -k 5 -r input.csv -d distances.csv -n neighbors.csv --tau 0.1\n  \n  Note that tau must be set such that the number of points in the corresponding\n  percentile of the data is greater than k.  Thus, if we choose tau = 0.1 with a\n  dataset of 1000 points and k = 5, then we are attempting to choose 5 nearest\n  neighbors out of the closest 1 point -- this is invalid and the program will\n  terminate with an error message.\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(22,34,'Non-negative Matrix Factorization\n\n  This program performs non-negative matrix factorization on the given dataset,\n  storing the resulting decomposed matrices in the specified files.  For an\n  input dataset V, NMF decomposes V into two matrices W and H such that \n  \n  V = W * H\n  \n  where all elements in W and H are non-negative.  If V is of size (n x m), then\n  W will be of size (n x r) and H will be of size (r x m), where r is the rank\n  of the factorization (specified by --rank).\n  \n  '),(23,35,'Non-negative Matrix Factorization\n\n  This program performs non-negative matrix factorization on the given dataset,\n  storing the resulting decomposed matrices in the specified files.  For an\n  input dataset V, NMF decomposes V into two matrices W and H such that \n  \n  V = W * H\n  \n  where all elements in W and H are non-negative.  If V is of size (n x m), then\n  W will be of size (n x r) and H will be of size (r x m), where r is the rank\n  of the factorization (specified by --rank).\n  \n  '),(24,33,'Non-negative Matrix Factorization\n\n  This program performs non-negative matrix factorization on the given dataset,\n  storing the resulting decomposed matrices in the specified files.  For an\n  input dataset V, NMF decomposes V into two matrices W and H such that \n  \n  V = W * H\n  \n  where all elements in W and H are non-negative.  If V is of size (n x m), then\n  W will be of size (n x r) and H will be of size (r x m), where r is the rank\n  of the factorization (specified by --rank).\n  \n  '),(25,41,'Density Estimation With Density Estimation Trees\n\n  This program performs a number of functions related to Density Estimation\n  Trees.  The optimal Density Estimation Tree (DET) can be trained on a set of\n  data (specified by --training_file or -t) using cross-validation (with number\n  of folds specified by --folds).  This trained density estimation tree may then\n  be saved to a model file with the --output_model_file (-M) option.\n  \n  The variable importances of each dimension may be saved with the --vi_file\n  (-i) option, and the density estimates on each training point may be saved to\n  the file specified with the --training_set_estimates_file (-e) option.\n  \n  This program also can provide density estimates for a set of test points,\n  specified in the --test_file (-T) file.  The density estimation tree used for\n  this task will be the tree that was trained on the given training points, or a\n  tree stored in the file given with the --input_model_file (-m) parameter.  The\n  density estimates for the test points may be saved into the file specified\n  with the --test_set_estimates_file (-E) option.\n\n'),(26,42,'Density Estimation With Density Estimation Trees\n\n  This program performs a number of functions related to Density Estimation\n  Trees.  The optimal Density Estimation Tree (DET) can be trained on a set of\n  data (specified by --training_file or -t) using cross-validation (with number\n  of folds specified by --folds).  This trained density estimation tree may then\n  be saved to a model file with the --output_model_file (-M) option.\n  \n  The variable importances of each dimension may be saved with the --vi_file\n  (-i) option, and the density estimates on each training point may be saved to\n  the file specified with the --training_set_estimates_file (-e) option.\n  \n  This program also can provide density estimates for a set of test points,\n  specified in the --test_file (-T) file.  The density estimation tree used for\n  this task will be the tree that was trained on the given training points, or a\n  tree stored in the file given with the --input_model_file (-m) parameter.  The\n  density estimates for the test points may be saved into the file specified\n  with the --test_set_estimates_file (-E) option.\n\n'),(27,69,'Hidden Markov Model (HMM) Training\n\n  This program allows a Hidden Markov Model to be trained on labeled or\n  unlabeled data.  It support three types of HMMs: discrete HMMs, Gaussian HMMs,\n  or GMM HMMs.\n  \n  Either one input sequence can be specified (with --input_file), or, a file\n  containing files in which input sequences can be found (when --input_file and\n  --batch are used together).  In addition, labels can be provided in the file\n  specified by --labels_file, and if --batch is used, the file given to\n  --labels_file should contain a list of files of labels corresponding to the\n  sequences in the file given to --input_file.\n  \n  The HMM is trained with the Baum-Welch algorithm if no labels are provided. \n  The tolerance of the Baum-Welch algorithm can be set with the --tolerance\n  option.  By default, the transition matrix is randomly initialized and the\n  emission distributions are initialized to fit the extent of the data.\n  \n  Optionally, a pre-created HMM model can be used as a guess for the transition\n  matrix and emission probabilities; this is specifiable with --model_file.\n\n'),(28,68,'Hidden Markov Model (HMM) Training\n\n  This program allows a Hidden Markov Model to be trained on labeled or\n  unlabeled data.  It support three types of HMMs: discrete HMMs, Gaussian HMMs,\n  or GMM HMMs.\n  \n  Either one input sequence can be specified (with --input_file), or, a file\n  containing files in which input sequences can be found (when --input_file and\n  --batch are used together).  In addition, labels can be provided in the file\n  specified by --labels_file, and if --batch is used, the file given to\n  --labels_file should contain a list of files of labels corresponding to the\n  sequences in the file given to --input_file.\n  \n  The HMM is trained with the Baum-Welch algorithm if no labels are provided. \n  The tolerance of the Baum-Welch algorithm can be set with the --tolerance\n  option.  By default, the transition matrix is randomly initialized and the\n  emission distributions are initialized to fit the extent of the data.\n  \n  Optionally, a pre-created HMM model can be used as a guess for the transition\n  matrix and emission probabilities; this is specifiable with --model_file.\n\n'),(29,107,'LARS\n\n  An implementation of LARS: Least Angle Regression (Stagewise/laSso).  This is\n  a stage-wise homotopy-based algorithm for L1-regularized linear regression\n  (LASSO) and L1+L2-regularized linear regression (Elastic Net).\n  \n  This program is able to train a LARS/LASSO/Elastic Net model or load a model\n  from file, output regression predictions for a test set, and save the trained\n  model to a file.  The LARS algorithm is described in more detail below:\n  \n  Let X be a matrix where each row is a point and each column is a dimension,\n  and let y be a vector of targets.\n  \n  The Elastic Net problem is to solve\n  \n    min_beta 0.5 || X * beta - y ||_2^2 + lambda_1 ||beta||_1 +\n        0.5 lambda_2 ||beta||_2^2\n  \n  If --lambda1 > 0 and --lambda2 = 0, the problem is the LASSO.\n  If --lambda1 > 0 and --lambda2 > 0, the problem is the Elastic Net.\n  If --lambda1 = 0 and --lambda2 > 0, the problem is ridge regression.\n  If --lambda1 = 0 and --lambda2 = 0, the problem is unregularized linear\n  regression.\n  \n  For efficiency reasons, it is not recommended to use this algorithm with\n  --lambda_1 = 0.  In that case, use the \'linear_regression\' program, which\n  implements both unregularized linear regression and ridge regression.\n  \n  To train a LARS/LASSO/Elastic Net model, the --input_file and --responses_file\n  parameters must be given.  The --lambda1 --lambda2, and --use_cholesky\n  arguments control the training parameters.  A trained model can be saved with\n  the --output_model_file, or, if training is not desired at all, a model can be\n  loaded with --input_model_file.  Any output predictions from a test file can\n  be saved into the file specified by the --output_predictions option.\n\n'),(30,108,'LARS\n\n  An implementation of LARS: Least Angle Regression (Stagewise/laSso).  This is\n  a stage-wise homotopy-based algorithm for L1-regularized linear regression\n  (LASSO) and L1+L2-regularized linear regression (Elastic Net).\n  \n  This program is able to train a LARS/LASSO/Elastic Net model or load a model\n  from file, output regression predictions for a test set, and save the trained\n  model to a file.  The LARS algorithm is described in more detail below:\n  \n  Let X be a matrix where each row is a point and each column is a dimension,\n  and let y be a vector of targets.\n  \n  The Elastic Net problem is to solve\n  \n    min_beta 0.5 || X * beta - y ||_2^2 + lambda_1 ||beta||_1 +\n        0.5 lambda_2 ||beta||_2^2\n  \n  If --lambda1 > 0 and --lambda2 = 0, the problem is the LASSO.\n  If --lambda1 > 0 and --lambda2 > 0, the problem is the Elastic Net.\n  If --lambda1 = 0 and --lambda2 > 0, the problem is ridge regression.\n  If --lambda1 = 0 and --lambda2 = 0, the problem is unregularized linear\n  regression.\n  \n  For efficiency reasons, it is not recommended to use this algorithm with\n  --lambda_1 = 0.  In that case, use the \'linear_regression\' program, which\n  implements both unregularized linear regression and ridge regression.\n  \n  To train a LARS/LASSO/Elastic Net model, the --input_file and --responses_file\n  parameters must be given.  The --lambda1 --lambda2, and --use_cholesky\n  arguments control the training parameters.  A trained model can be saved with\n  the --output_model_file, or, if training is not desired at all, a model can be\n  loaded with --input_model_file.  Any output predictions from a test file can\n  be saved into the file specified by the --output_predictions option.\n\n'),(31,48,'All K-Furthest-Neighbors\n\n  This program will calculate the all k-furthest-neighbors of a set of points.\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 furthest neighbors of\n  eachpoint in \'input.csv\' and store the distances in \'distances.csv\' and the\n  neighbors in the file \'neighbors.csv\':\n  \n  $ mlpack_kfn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th furthest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(32,47,'All K-Furthest-Neighbors\n\n  This program will calculate the all k-furthest-neighbors of a set of points.\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 furthest neighbors of\n  eachpoint in \'input.csv\' and store the distances in \'distances.csv\' and the\n  neighbors in the file \'neighbors.csv\':\n  \n  $ mlpack_kfn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th furthest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(33,46,'All K-Furthest-Neighbors\n\n  This program will calculate the all k-furthest-neighbors of a set of points.\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 furthest neighbors of\n  eachpoint in \'input.csv\' and store the distances in \'distances.csv\' and the\n  neighbors in the file \'neighbors.csv\':\n  \n  $ mlpack_kfn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th furthest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(34,37,'Hidden Markov Model (HMM) Sequence Generator\n\n  This utility takes an already-trained HMM (--model_file) and generates a\n  random observation sequence and hidden state sequence based on its parameters,\n  saving them to the specified files (--output_file and --state_file)\n\nRequired input options:\n\n  --length (-l) [int]           Length of sequence to generate.\n  --model_file (-m) [string]    File containing HMM.\n\n'),(35,11,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(36,10,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(37,7,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(38,8,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(39,5,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(40,14,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(41,13,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(42,12,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(43,6,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(44,15,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(45,9,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(46,16,'Kernel Principal Components Analysis\n\n  This program performs Kernel Principal Components Analysis (KPCA) on the\n  specified dataset with the specified kernel.  This will transform the data\n  onto the kernel principal components, and optionally reduce the dimensionality\n  by ignoring the kernel principal components with the smallest eigenvalues.\n  \n  For the case where a linear kernel is used, this reduces to regular PCA.\n  \n  For example, the following will perform KPCA on the \'input.csv\' file using the\n  gaussian kernel and store the transformed date in the \'transformed.csv\' file.\n  \n  $ kernel_pca -i input.csv -k gaussian -o transformed.csv\n  \n  The kernels that are supported are listed below:\n  \n   * \'linear\': the standard linear dot product (same as normal PCA):\n      K(x, y) = x^T y\n  \n   * \'gaussian\': a Gaussian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y || ^ 2) / (2 * (bandwidth ^ 2)))\n  \n   * \'polynomial\': polynomial kernel; requires offset and degree:\n      K(x, y) = (x^T y + offset) ^ degree\n  \n   * \'hyptan\': hyperbolic tangent kernel; requires scale and offset:\n      K(x, y) = tanh(scale * (x^T y) + offset)\n  \n   * \'laplacian\': Laplacian kernel; requires bandwidth:\n      K(x, y) = exp(-(|| x - y ||) / bandwidth)\n  \n   * \'epanechnikov\': Epanechnikov kernel; requires bandwidth:\n      K(x, y) = max(0, 1 - || x - y ||^2 / bandwidth^2)\n  \n   * \'cosine\': cosine distance:\n      K(x, y) = 1 - (x^T y) / (|| x || * || y ||)\n  \n  The parameters for each of the kernels should be specified with the options\n  --bandwidth, --kernel_scale, --offset, or --degree (or a combination of those\n  options).\n  \n  '),(47,100,'FastMKS (Fast Max-Kernel Search)\n\n  This program will find the k maximum kernel of a set of points, using a query\n  set and a reference set (which can optionally be the same set). More\n  specifically, for each point in the query set, the k points in the reference\n  set with maximum kernel evaluations are found.  The kernel function used is\n  specified by --kernel.\n  \n  For example, the following command will calculate, for each point in\n  \'query.csv\', the five points in \'reference.csv\' with maximum kernel evaluation\n  using the linear kernel.  The kernel evaluations are stored in \'kernels.csv\'\n  and the indices are stored in \'indices.csv\'.\n  \n  $ fastmks --k 5 --reference_file reference.csv --query_file query.csv\n    --indices_file indices.csv --kernels_file kernels.csv --kernel linear\n  \n  The output files are organized such that row i and column j in the indices\n  output file corresponds to the index of the point in the reference set that\n  has i\'th largest kernel evaluation with the point in the query set with index\n  j.  Row i and column j in the kernels output file corresponds to the kernel\n  evaluation between those two points.\n  \n  This executable performs FastMKS using a cover tree.  The base used to build\n  the cover tree can be specified with the --base option.\n\n'),(48,98,'FastMKS (Fast Max-Kernel Search)\n\n  This program will find the k maximum kernel of a set of points, using a query\n  set and a reference set (which can optionally be the same set). More\n  specifically, for each point in the query set, the k points in the reference\n  set with maximum kernel evaluations are found.  The kernel function used is\n  specified by --kernel.\n  \n  For example, the following command will calculate, for each point in\n  \'query.csv\', the five points in \'reference.csv\' with maximum kernel evaluation\n  using the linear kernel.  The kernel evaluations are stored in \'kernels.csv\'\n  and the indices are stored in \'indices.csv\'.\n  \n  $ fastmks --k 5 --reference_file reference.csv --query_file query.csv\n    --indices_file indices.csv --kernels_file kernels.csv --kernel linear\n  \n  The output files are organized such that row i and column j in the indices\n  output file corresponds to the index of the point in the reference set that\n  has i\'th largest kernel evaluation with the point in the query set with index\n  j.  Row i and column j in the kernels output file corresponds to the kernel\n  evaluation between those two points.\n  \n  This executable performs FastMKS using a cover tree.  The base used to build\n  the cover tree can be specified with the --base option.\n\n'),(49,104,'FastMKS (Fast Max-Kernel Search)\n\n  This program will find the k maximum kernel of a set of points, using a query\n  set and a reference set (which can optionally be the same set). More\n  specifically, for each point in the query set, the k points in the reference\n  set with maximum kernel evaluations are found.  The kernel function used is\n  specified by --kernel.\n  \n  For example, the following command will calculate, for each point in\n  \'query.csv\', the five points in \'reference.csv\' with maximum kernel evaluation\n  using the linear kernel.  The kernel evaluations are stored in \'kernels.csv\'\n  and the indices are stored in \'indices.csv\'.\n  \n  $ fastmks --k 5 --reference_file reference.csv --query_file query.csv\n    --indices_file indices.csv --kernels_file kernels.csv --kernel linear\n  \n  The output files are organized such that row i and column j in the indices\n  output file corresponds to the index of the point in the reference set that\n  has i\'th largest kernel evaluation with the point in the query set with index\n  j.  Row i and column j in the kernels output file corresponds to the kernel\n  evaluation between those two points.\n  \n  This executable performs FastMKS using a cover tree.  The base used to build\n  the cover tree can be specified with the --base option.\n\n'),(50,105,'FastMKS (Fast Max-Kernel Search)\n\n  This program will find the k maximum kernel of a set of points, using a query\n  set and a reference set (which can optionally be the same set). More\n  specifically, for each point in the query set, the k points in the reference\n  set with maximum kernel evaluations are found.  The kernel function used is\n  specified by --kernel.\n  \n  For example, the following command will calculate, for each point in\n  \'query.csv\', the five points in \'reference.csv\' with maximum kernel evaluation\n  using the linear kernel.  The kernel evaluations are stored in \'kernels.csv\'\n  and the indices are stored in \'indices.csv\'.\n  \n  $ fastmks --k 5 --reference_file reference.csv --query_file query.csv\n    --indices_file indices.csv --kernels_file kernels.csv --kernel linear\n  \n  The output files are organized such that row i and column j in the indices\n  output file corresponds to the index of the point in the reference set that\n  has i\'th largest kernel evaluation with the point in the query set with index\n  j.  Row i and column j in the kernels output file corresponds to the kernel\n  evaluation between those two points.\n  \n  This executable performs FastMKS using a cover tree.  The base used to build\n  the cover tree can be specified with the --base option.\n\n'),(51,99,'FastMKS (Fast Max-Kernel Search)\n\n  This program will find the k maximum kernel of a set of points, using a query\n  set and a reference set (which can optionally be the same set). More\n  specifically, for each point in the query set, the k points in the reference\n  set with maximum kernel evaluations are found.  The kernel function used is\n  specified by --kernel.\n  \n  For example, the following command will calculate, for each point in\n  \'query.csv\', the five points in \'reference.csv\' with maximum kernel evaluation\n  using the linear kernel.  The kernel evaluations are stored in \'kernels.csv\'\n  and the indices are stored in \'indices.csv\'.\n  \n  $ fastmks --k 5 --reference_file reference.csv --query_file query.csv\n    --indices_file indices.csv --kernels_file kernels.csv --kernel linear\n  \n  The output files are organized such that row i and column j in the indices\n  output file corresponds to the index of the point in the reference set that\n  has i\'th largest kernel evaluation with the point in the query set with index\n  j.  Row i and column j in the kernels output file corresponds to the kernel\n  evaluation between those two points.\n  \n  This executable performs FastMKS using a cover tree.  The base used to build\n  the cover tree can be specified with the --base option.\n\n'),(52,103,'FastMKS (Fast Max-Kernel Search)\n\n  This program will find the k maximum kernel of a set of points, using a query\n  set and a reference set (which can optionally be the same set). More\n  specifically, for each point in the query set, the k points in the reference\n  set with maximum kernel evaluations are found.  The kernel function used is\n  specified by --kernel.\n  \n  For example, the following command will calculate, for each point in\n  \'query.csv\', the five points in \'reference.csv\' with maximum kernel evaluation\n  using the linear kernel.  The kernel evaluations are stored in \'kernels.csv\'\n  and the indices are stored in \'indices.csv\'.\n  \n  $ fastmks --k 5 --reference_file reference.csv --query_file query.csv\n    --indices_file indices.csv --kernels_file kernels.csv --kernel linear\n  \n  The output files are organized such that row i and column j in the indices\n  output file corresponds to the index of the point in the reference set that\n  has i\'th largest kernel evaluation with the point in the query set with index\n  j.  Row i and column j in the kernels output file corresponds to the kernel\n  evaluation between those two points.\n  \n  This executable performs FastMKS using a cover tree.  The base used to build\n  the cover tree can be specified with the --base option.\n\n'),(53,101,'FastMKS (Fast Max-Kernel Search)\n\n  This program will find the k maximum kernel of a set of points, using a query\n  set and a reference set (which can optionally be the same set). More\n  specifically, for each point in the query set, the k points in the reference\n  set with maximum kernel evaluations are found.  The kernel function used is\n  specified by --kernel.\n  \n  For example, the following command will calculate, for each point in\n  \'query.csv\', the five points in \'reference.csv\' with maximum kernel evaluation\n  using the linear kernel.  The kernel evaluations are stored in \'kernels.csv\'\n  and the indices are stored in \'indices.csv\'.\n  \n  $ fastmks --k 5 --reference_file reference.csv --query_file query.csv\n    --indices_file indices.csv --kernels_file kernels.csv --kernel linear\n  \n  The output files are organized such that row i and column j in the indices\n  output file corresponds to the index of the point in the reference set that\n  has i\'th largest kernel evaluation with the point in the query set with index\n  j.  Row i and column j in the kernels output file corresponds to the kernel\n  evaluation between those two points.\n  \n  This executable performs FastMKS using a cover tree.  The base used to build\n  the cover tree can be specified with the --base option.\n\n'),(54,102,'FastMKS (Fast Max-Kernel Search)\n\n  This program will find the k maximum kernel of a set of points, using a query\n  set and a reference set (which can optionally be the same set). More\n  specifically, for each point in the query set, the k points in the reference\n  set with maximum kernel evaluations are found.  The kernel function used is\n  specified by --kernel.\n  \n  For example, the following command will calculate, for each point in\n  \'query.csv\', the five points in \'reference.csv\' with maximum kernel evaluation\n  using the linear kernel.  The kernel evaluations are stored in \'kernels.csv\'\n  and the indices are stored in \'indices.csv\'.\n  \n  $ fastmks --k 5 --reference_file reference.csv --query_file query.csv\n    --indices_file indices.csv --kernels_file kernels.csv --kernel linear\n  \n  The output files are organized such that row i and column j in the indices\n  output file corresponds to the index of the point in the reference set that\n  has i\'th largest kernel evaluation with the point in the query set with index\n  j.  Row i and column j in the kernels output file corresponds to the kernel\n  evaluation between those two points.\n  \n  This executable performs FastMKS using a cover tree.  The base used to build\n  the cover tree can be specified with the --base option.\n\n'),(55,106,'FastMKS (Fast Max-Kernel Search)\n\n  This program will find the k maximum kernel of a set of points, using a query\n  set and a reference set (which can optionally be the same set). More\n  specifically, for each point in the query set, the k points in the reference\n  set with maximum kernel evaluations are found.  The kernel function used is\n  specified by --kernel.\n  \n  For example, the following command will calculate, for each point in\n  \'query.csv\', the five points in \'reference.csv\' with maximum kernel evaluation\n  using the linear kernel.  The kernel evaluations are stored in \'kernels.csv\'\n  and the indices are stored in \'indices.csv\'.\n  \n  $ fastmks --k 5 --reference_file reference.csv --query_file query.csv\n    --indices_file indices.csv --kernels_file kernels.csv --kernel linear\n  \n  The output files are organized such that row i and column j in the indices\n  output file corresponds to the index of the point in the reference set that\n  has i\'th largest kernel evaluation with the point in the query set with index\n  j.  Row i and column j in the kernels output file corresponds to the kernel\n  evaluation between those two points.\n  \n  This executable performs FastMKS using a cover tree.  The base used to build\n  the cover tree can be specified with the --base option.\n\n'),(56,90,'Principal Components Analysis\n\n  This program performs principal components analysis on the given dataset using\n  the exact, randomized or QUIC SVD method. It will transform the data onto its\n  principal components, optionally performing dimensionality reduction by\n  ignoring the principal components with the smallest eigenvalues.\n\nRequired input options:\n\n  --input_file (-i) [string]    Input dataset to perform PCA on.\n\n'),(57,32,'Hidden Markov Model (HMM) Sequence Log-Likelihood\n\n  This utility takes an already-trained HMM (--model_file) and evaluates the\n  log-likelihood of a given sequence of observations (--input_file).  The\n  computed log-likelihood is given directly to stdout.\n\nRequired input options:\n\n  --input_file (-i) [string]    File containing observations,\n  --model_file (-m) [string]    File containing HMM.\n\n'),(58,38,'All K-Approximate-Nearest-Neighbor Search with LSH\n\n  This program will calculate the k approximate-nearest-neighbors of a set of\n  points using locality-sensitive hashing. You may specify a separate set of\n  reference points and query points, or just a reference set which will be used\n  as both the reference and query set. \n  \n  For example, the following will return 5 neighbors from the data for each\n  point in \'input.csv\' and store the distances in \'distances.csv\' and the\n  neighbors in the file \'neighbors.csv\':\n  \n  $ lsh -k 5 -r input.csv -d distances.csv -n neighbors.csv \n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n  \n  Because this is approximate-nearest-neighbors search, results may be different\n  from run to run.  Thus, the --seed option can be specified to set the random\n  seed.\n\n'),(59,27,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(60,22,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(61,17,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(62,18,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(63,19,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(64,20,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(65,25,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(66,23,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(67,28,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(68,29,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(69,26,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(70,24,'k-Nearest-Neighbors\n\n  This program will calculate the k-nearest-neighbors of a set of points using\n  kd-trees or cover trees (cover tree support is experimental and may be slow).\n  You may specify a separate set of reference points and query points, or just a\n  reference set which will be used as both the reference and query set.\n  \n  For example, the following will calculate the 5 nearest neighbors of eachpoint\n  in \'input.csv\' and store the distances in \'distances.csv\' and the neighbors in\n  the file \'neighbors.csv\':\n  \n  $ mlpack_knn --k=5 --reference_file=input.csv --distances_file=distances.csv\n   --neighbors_file=neighbors.csv\n  \n  The output files are organized such that row i and column j in the neighbors\n  output file corresponds to the index of the point in the reference set which\n  is the i\'th nearest neighbor from the point in the query set with index j. \n  Row i and column j in the distances output file corresponds to the distance\n  between those two points.\n\n'),(71,96,'Local Coordinate Coding\n\n  An implementation of Local Coordinate Coding (LCC), which codes data that\n  approximately lives on a manifold using a variation of l1-norm regularized\n  sparse coding.  Given a dense data matrix X with n points and d dimensions,\n  LCC seeks to find a dense dictionary matrix D with k atoms in d dimensions,\n  and a coding matrix Z with n points in k dimensions.  Because of the\n  regularization method used, the atoms in D should lie close to the manifold on\n  which the data points lie.\n  \n  The original data matrix X can then be reconstructed as D * Z.  Therefore,\n  this program finds a representation of each point in X as a sparse linear\n  combination of atoms in the dictionary D.\n  \n  The coding is found with an algorithm which alternates between a dictionary\n  step, which updates the dictionary D, and a coding step, which updates the\n  coding matrix Z.\n  \n  To run this program, the input matrix X must be specified (with -i), along\n  with the number of atoms in the dictionary (-k).  An initial dictionary may\n  also be specified with the --initial_dictionary option.  The l1-norm\n  regularization parameter is specified with -l.  For example, to run LCC on the\n  dataset in data.csv using 200 atoms and an l1-regularization parameter of 0.1,\n  saving the dictionary into dict.csv and the codes into codes.csv, use \n  \n  $ local_coordinate_coding -i data.csv -k 200 -l 0.1 -d dict.csv -c codes.csv\n  \n  The maximum number of iterations may be specified with the -n option.\n  '),(72,95,'Local Coordinate Coding\n\n  An implementation of Local Coordinate Coding (LCC), which codes data that\n  approximately lives on a manifold using a variation of l1-norm regularized\n  sparse coding.  Given a dense data matrix X with n points and d dimensions,\n  LCC seeks to find a dense dictionary matrix D with k atoms in d dimensions,\n  and a coding matrix Z with n points in k dimensions.  Because of the\n  regularization method used, the atoms in D should lie close to the manifold on\n  which the data points lie.\n  \n  The original data matrix X can then be reconstructed as D * Z.  Therefore,\n  this program finds a representation of each point in X as a sparse linear\n  combination of atoms in the dictionary D.\n  \n  The coding is found with an algorithm which alternates between a dictionary\n  step, which updates the dictionary D, and a coding step, which updates the\n  coding matrix Z.\n  \n  To run this program, the input matrix X must be specified (with -i), along\n  with the number of atoms in the dictionary (-k).  An initial dictionary may\n  also be specified with the --initial_dictionary option.  The l1-norm\n  regularization parameter is specified with -l.  For example, to run LCC on the\n  dataset in data.csv using 200 atoms and an l1-regularization parameter of 0.1,\n  saving the dictionary into dict.csv and the codes into codes.csv, use \n  \n  $ local_coordinate_coding -i data.csv -k 200 -l 0.1 -d dict.csv -c codes.csv\n  \n  The maximum number of iterations may be specified with the -n option.\n  '),(73,56,'K-Means Clustering\n\n  This program performs K-Means clustering on the given dataset, storing the\n  learned cluster assignments either as a column of labels in the file\n  containing the input dataset or in a separate file.  Empty clusters are not\n  allowed by default; when a cluster becomes empty, the point furthest from the\n  centroid of the cluster with maximum variance is taken to fill that cluster.\n  \n  '),(74,54,'K-Means Clustering\n\n  This program performs K-Means clustering on the given dataset, storing the\n  learned cluster assignments either as a column of labels in the file\n  containing the input dataset or in a separate file.  Empty clusters are not\n  allowed by default; when a cluster becomes empty, the point furthest from the\n  centroid of the cluster with maximum variance is taken to fill that cluster.\n  \n  '),(75,51,'K-Means Clustering\n\n  This program performs K-Means clustering on the given dataset, storing the\n  learned cluster assignments either as a column of labels in the file\n  containing the input dataset or in a separate file.  Empty clusters are not\n  allowed by default; when a cluster becomes empty, the point furthest from the\n  centroid of the cluster with maximum variance is taken to fill that cluster.\n  \n  '),(76,52,'K-Means Clustering\n\n  This program performs K-Means clustering on the given dataset, storing the\n  learned cluster assignments either as a column of labels in the file\n  containing the input dataset or in a separate file.  Empty clusters are not\n  allowed by default; when a cluster becomes empty, the point furthest from the\n  centroid of the cluster with maximum variance is taken to fill that cluster.\n  \n  '),(77,53,'K-Means Clustering\n\n  This program performs K-Means clustering on the given dataset, storing the\n  learned cluster assignments either as a column of labels in the file\n  containing the input dataset or in a separate file.  Empty clusters are not\n  allowed by default; when a cluster becomes empty, the point furthest from the\n  centroid of the cluster with maximum variance is taken to fill that cluster.\n  \n  '),(78,49,'K-Means Clustering\n\n  This program performs K-Means clustering on the given dataset, storing the\n  learned cluster assignments either as a column of labels in the file\n  containing the input dataset or in a separate file.  Empty clusters are not\n  allowed by default; when a cluster becomes empty, the point furthest from the\n  centroid of the cluster with maximum variance is taken to fill that cluster.\n  \n  '),(79,55,'K-Means Clustering\n\n  This program performs K-Means clustering on the given dataset, storing the\n  learned cluster assignments either as a column of labels in the file\n  containing the input dataset or in a separate file.  Empty clusters are not\n  allowed by default; when a cluster becomes empty, the point furthest from the\n  centroid of the cluster with maximum variance is taken to fill that cluster.\n  \n  '),(80,50,'K-Means Clustering\n\n  This program performs K-Means clustering on the given dataset, storing the\n  learned cluster assignments either as a column of labels in the file\n  containing the input dataset or in a separate file.  Empty clusters are not\n  allowed by default; when a cluster becomes empty, the point furthest from the\n  centroid of the cluster with maximum variance is taken to fill that cluster.\n  \n  '),(81,62,'Simple Linear Regression and Prediction\n\n  An implementation of simple linear regression and simple ridge regression\n  using ordinary least squares. This solves the problem\n  \n    y = X * b + e\n  \n  where X (--input_file) and y (the last column of --input_file, or\n  --input_responses) are known and b is the desired variable.  If the covariance\n  matrix (X\'X) is not invertible, or if the solution is overdetermined, then\n  specify a Tikhonov regularization constant (--lambda) greater than 0, which\n  will regularize the covariance matrix to make it invertible.  The calculated b\n  is saved to disk (--output_file).\n  \n  '),(82,3,'Fast Euclidean Minimum Spanning Tree\n\n  This program can compute the Euclidean minimum spanning tree of a set of input\n  points using the dual-tree Boruvka algorithm.\n  \n  The output is saved in a three-column matrix, where each row indicates an\n  edge.  The first column corresponds to the lesser index of the edge; the\n  second column corresponds to the greater index of the edge; and the third\n  column corresponds to the distance between the two points.\n\nRequired input options:\n\n  --input_file (-i) [string]    Data input file.\n\n'),(83,4,'Fast Euclidean Minimum Spanning Tree\n\n  This program can compute the Euclidean minimum spanning tree of a set of input\n  points using the dual-tree Boruvka algorithm.\n  \n  The output is saved in a three-column matrix, where each row indicates an\n  edge.  The first column corresponds to the lesser index of the edge; the\n  second column corresponds to the greater index of the edge; and the third\n  column corresponds to the distance between the two points.\n\nRequired input options:\n\n  --input_file (-i) [string]    Data input file.\n\n');
/*!40000 ALTER TABLE `method_info` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `methods`
--

DROP TABLE IF EXISTS `methods`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `methods` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` text NOT NULL,
  `parameters` text NOT NULL,
  `alias` text NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=314 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `methods`
--

LOCK TABLES `methods` WRITE;
/*!40000 ALTER TABLE `methods` DISABLE KEYS */;
INSERT INTO `methods` VALUES (1,'QDA','','None'),(2,'LDA','','None'),(3,'EMST','','None'),(4,'EMST','-n','None'),(5,'KPCA','-k cosine','None'),(6,'KPCA','-k cosine --nystroem_method -s kmeans','None'),(7,'KPCA','-k laplacian','None'),(8,'KPCA','-k polynomial','None'),(9,'KPCA','-k polynomial --nystroem_method -s random','None'),(10,'KPCA','-k hyptan','None'),(11,'KPCA','-k gaussian','None'),(12,'KPCA','-k gaussian --nystroem_method -s kmeans','None'),(13,'KPCA','-k cosine --nystroem_method -s random','None'),(14,'KPCA','-k gaussian --nystroem_method -s random','None'),(15,'KPCA','-k polynomial --nystroem_method -s kmeans','None'),(16,'KPCA','-k linear','None'),(17,'ALLKNN','-k 3 -s 42 -e 0.0','None'),(18,'ALLKNN','-k 3 -s 42 -N','None'),(19,'ALLKNN','-k 3 -s 42 --tree_type=cover','None'),(20,'ALLKNN','-k 4','None'),(21,'ALLKNN','-k 3 -s 42','None'),(22,'ALLKNN','-k 3','None'),(23,'ALLKNN','-k 3 -s 42 -S','None'),(24,'ALLKNN','-k 3 -s 42 -e 0.15','None'),(25,'ALLKNN','-k 3 -s 42 -e 0.05','None'),(26,'ALLKNN','-k 3 -s 42 --tree_type=cover -S','None'),(27,'ALLKNN','-k 3 -s 42 -e 0.10','None'),(28,'ALLKNN','-k 3 -s 42 -e 0.25','None'),(29,'ALLKNN','-k 3 -s 42 -e 0.20','None'),(30,'ANN','','None'),(31,'ANN','-k 10 -n 10','None'),(32,'HMMLOGLIK','','None'),(33,'NMF','-r 6 -s 42 -u multdiv','None'),(34,'NMF','-r 6 -s 42 -u multdist','None'),(35,'NMF','-r 6 -s 42 -u als','None'),(36,'NMF','-r 6 -u alspgrad','None'),(37,'HMMGENERATE','-l 10000','None'),(38,'LSH','-k 3 -s 42','None'),(39,'HMMVITERBI','','None'),(40,'SVR','-c 1.0 -e 1.0 -g 0.1','None'),(41,'DET','','None'),(42,'DET','-f 20','None'),(43,'ADABOOST','','None'),(44,'ElasticNet','','None'),(45,'DTC','','None'),(46,'ALLKFN','-k 3','None'),(47,'ALLKFN','-k 3 -N','None'),(48,'ALLKFN','-k 3 -s','None'),(49,'KMEANS','-c 26','None'),(50,'KMEANS','-c 5','None'),(51,'KMEANS','-c 2','None'),(52,'KMEANS','-c 7','None'),(53,'KMEANS','-c 3','None'),(54,'KMEANS','-c 75','None'),(55,'KMEANS','-c 10','None'),(56,'KMEANS','-c 6','None'),(57,'ALLKRANN','-k 3 -T 10 -N','None'),(58,'ALLKRANN','-k 3 -T 10 -s','None'),(59,'ALLKRANN','-k 3 -T 10 -L','None'),(60,'ALLKRANN','-k 3 -T 10 -X','None'),(61,'ALLKRANN','-k 3 -T 10','None'),(62,'LinearRegression','','None'),(63,'KNC','','None'),(64,'DecisionStump','','None'),(65,'LASSO','-l 0.01','None'),(66,'NBC','','None'),(67,'NBC','-I','None'),(68,'HMMTRAIN','-t gaussian -n 20 -s 42','None'),(69,'HMMTRAIN','-t discrete -n 20 -s 42','None'),(70,'PERCEPTRON','-n 10000','None'),(71,'LinearRidgeRegression','-t 5.0','None'),(72,'LinearRidgeRegression','-t 50.0','None'),(73,'LinearRidgeRegression','-t 1.0','None'),(74,'LogisticRegression','','None'),(75,'RANDOMFOREST','','None'),(76,'RANDOMFOREST','-e 50 -c entropy -d 10 --min_samples_split 4 --min_samples_leaf 2 --n_jobs 2','None'),(77,'RANGESEARCH','-M 0.02','None'),(78,'RANGESEARCH','-M 0.02 -s','None'),(79,'RANGESEARCH','-M 0.02 -N','None'),(80,'SparseCoding','-k 12 -s 42 -n 100','None'),(81,'SparseCoding','-k 12 -s 42 -N','None'),(82,'SparseCoding','-k 12 -s 42','None'),(83,'SVM','','None'),(84,'NCA','-n 2000 -O lbfgs -s 42','None'),(85,'NCA','-n 2000 -O lbfgs -s 42 -B 5','None'),(86,'NCA','-n 2000 -N -s 42','None'),(87,'NCA','-n 2000 -O sgd -s 42','None'),(88,'NCA','-n 2000 -O lbfgs -s 42 -w 0.5','None'),(89,'NCA','-n 2000 -O lbfgs -s 42 -w 0.5 -B 5','None'),(90,'PCA','','None'),(91,'PCA','-d 2 -s','None'),(92,'PCA','-d 2','None'),(93,'GMM','-s 42','None'),(94,'GMM','-P -s 42','None'),(95,'LocalCoordinateCoding','-k 12 -s 42 -N','None'),(96,'LocalCoordinateCoding','-k 12 -s 42','None'),(97,'Golub','','None'),(98,'FastMKS','-k 10 -K cosine','None'),(99,'FastMKS','-k 10 -K triangular','None'),(100,'FastMKS','-k 10 -K linear','None'),(101,'FastMKS','-k 10 -K polynomial -d 10','None'),(102,'FastMKS','-k 10 -K epanechnikov','None'),(103,'FastMKS','-k 1 -K linear','None'),(104,'FastMKS','-k 10 -S -K linear','None'),(105,'FastMKS','-k 10 -K hyptan','None'),(106,'FastMKS','-k 10 -K gaussian','None'),(107,'LARS','--lambda1 0.01 --lambda2 0.005 --use_cholesky','None'),(108,'LARS','-l 0.01','None'),(109,'DECISIONTREE','--stumps 10 -m 10','None'),(110,'DECISIONTREE','','None'),(111,'ALLKFN','{\"k\": 3}','None'),(112,'ALLKFN','{\"k\": 3, \"single_mode\": true}','None'),(113,'ALLKFN','{\"naive_mode\": true, \"k\": 3}','None'),(114,'PERCEPTRON','{\"max_iterations\": 10000}','None'),(115,'LinearRidgeRegression','{\"alpha\": 50.0}','None'),(116,'SVM','{}','None'),(117,'ALLKNN','{\"k\": 3, \"naive_mode\": true, \"seed\": 42}','None'),(118,'ALLKNN','{\"k\": 3, \"seed\": 42}','None'),(119,'ALLKNN','{\"single_mode\": true, \"k\": 3, \"seed\": 42}','None'),(120,'ALLKNN','{\"k\": 3}','None'),(121,'KNC','{\"k\": 5}','None'),(122,'Golub','{}','None'),(123,'HMMTRAIN','{\"seed\": 42, \"states\": 20, \"type\": \"gaussian\"}','None'),(124,'HMMTRAIN','{\"seed\": 42, \"states\": 20, \"type\": \"discrete\"}','None'),(125,'SVR','{}','None'),(126,'SVR','{\"c\": 1.0, \"epsilon\": 1.0, \"gamma\": 0.1}','None'),(127,'DecisionStump','{}','None'),(128,'NCA','{\"optimizer\": \"lbfgs\", \"seed\": 42, \"wolfe\": 0.5, \"max_iterations\": 2000}','None'),(129,'NCA','{\"optimizer\": \"lbfgs\", \"seed\": 42, \"num_basis\": 5, \"wolfe\": 0.5, \"max_iterations\": 2000}','None'),(130,'NCA','{\"optimizer\": \"sgd\", \"seed\": 42, \"max_iterations\": 2000}','None'),(131,'NCA','{\"optimizer\": \"lbfgs\", \"seed\": 42, \"num_basis\": 5, \"max_iterations\": 2000}','None'),(132,'NCA','{\"optimizer\": \"lbfgs\", \"seed\": 42, \"max_iterations\": 2000}','None'),(133,'NCA','{\"normalize\": true, \"seed\": 42, \"max_iterations\": 2000}','None'),(134,'PERCEPTRON','{}','None'),(135,'SVC','{}','None'),(136,'ALLKRANN','{\"naive_mode\": true, \"k\": 3, \"tau\": 10}','None'),(137,'ALLKRANN','{\"k\": 3, \"single_mode\": true, \"tau\": 10}','None'),(138,'ALLKRANN','{\"k\": 3, \"tau\": 10}','None'),(139,'ALLKRANN','{\"sample_at_leaves\": true, \"k\": 3, \"tau\": 10}','None'),(140,'ALLKRANN','{\"first_leaf_exact\": true, \"k\": 3, \"tau\": 10}','None'),(141,'SparseCoding','{\"seed\": 42, \"atoms\": 12}','None'),(142,'SparseCoding','{\"seed\": 42, \"normalize\": true, \"atoms\": 12}','None'),(143,'SparseCoding','{\"seed\": 42, \"atoms\": 12, \"max_iterations\": 100}','None'),(144,'RANGESEARCH','{\"max\": 0.02, \"single_mode\": true}','None'),(145,'RANGESEARCH','{\"naive_mode\": true, \"max\": 0.02}','None'),(146,'RANGESEARCH','{\"max\": 0.02}','None'),(147,'LSH','{\"k\": 3, \"seed\": 42}','None'),(148,'ADABOOST','{}','None'),(149,'KMEANS','{\"clusters\": 10}','None'),(150,'KMEANS','{\"centroids\": 75}','None'),(151,'KMEANS','{\"clusters\": 3}','None'),(152,'KMEANS','{\"clusters\": 7}','None'),(153,'KMEANS','{\"clusters\": 26}','None'),(154,'KMEANS','{\"clusters\": 6}','None'),(155,'KMEANS','{\"clusters\": 75}','None'),(156,'KMEANS','{\"clusters\": 5}','None'),(157,'KMEANS','{\"clusters\": 2}','None'),(158,'FastMKS','{\"k\": 10, \"kernel\": \"polynomial\", \"degree\": 10}','None'),(159,'FastMKS','{\"k\": 10, \"kernel\": \"gaussian\"}','None'),(160,'FastMKS','{\"k\": 10, \"kernel\": \"triangular\"}','None'),(161,'FastMKS','{\"k\": 10, \"single_mode\": true, \"kernel\": \"linear\"}','None'),(162,'FastMKS','{\"k\": 10, \"kernel\": \"linear\"}','None'),(163,'FastMKS','{\"k\": 10, \"kernel\": \"hyptan\"}','None'),(164,'FastMKS','{\"k\": 1, \"kernel\": \"linear\"}','None'),(165,'FastMKS','{\"k\": 10, \"kernel\": \"cosine\"}','None'),(166,'FastMKS','{\"k\": 10, \"kernel\": \"epanechnikov\"}','None'),(167,'NBC','{}','None'),(168,'NBC','{\"incremental\": true}','None'),(169,'DTC','{}','None'),(170,'LDA','{}','None'),(171,'ICA','{}','None'),(172,'DECISIONTREE','{}','None'),(173,'DECISIONTREE','{\"minimum_leaf_size\": 10, \"stumps\": 10}','None'),(174,'PCA','{}','None'),(175,'PCA','{\"new_dimensionality\": 2}','None'),(176,'PCA','{\"scaled\": true, \"new_dimensionality\": 2}','None'),(177,'PCA','{\"whiten\": true, \"new_dimensionality\": 2}','None'),(178,'QDA','{}','None'),(179,'ANN','{}','None'),(180,'ANN','{\"num\": 10000, \"k\": 3, \"sample_pct\": 0.5}','None'),(181,'ANN','{\"k\": 10, \"trees\": 10}','None'),(182,'ANN','{\"num_trees\": 10, \"k\": 10}','None'),(183,'RANDOMFOREST','{\"num_trees\": 50, \"minimum_leaf_size\": 2, \"max_depth\": 10, \"minimum_samples_split\": 4, \"num_jobs\": 2, \"fitness_function\": \"entropy\"}','None'),(184,'RANDOMFOREST','{}','None'),(185,'RANDOMFOREST','{\"num_trees\": 50}','None'),(186,'FastMKS','{\"k\": 10, \"kernel\": \"linear\", \"single_mode\": true}','None'),(187,'HMMLOGLIK','{}','None'),(188,'ANN','{\"k\": 3, \"sample_pct\": 0.5, \"num\": 10000}','None'),(189,'ANN','{\"k\": 10, \"num_trees\": 10}','None'),(190,'LARS','{\"lambda1\": 0.01}','None'),(191,'LARS','{\"lambda1\": 0.01, \"lambda2\": 0.005, \"use_cholesky\": true}','None'),(192,'LARS','{}','None'),(193,'LinearRidgeRegression','{\"alpha\": 5.0}','None'),(194,'LinearRidgeRegression','{\"alpha\": 1.0}','None'),(195,'LASSO','{\"lambda1\": 0.01}','None'),(196,'LASSO','{}','None'),(197,'ALLKFN','{\"k\": 3, \"naive_mode\": true}','None'),(198,'DET','{\"folds\": 20}','None'),(199,'DET','{}','None'),(200,'LocalCoordinateCoding','{\"seed\": 42, \"atoms\": 12, \"normalize\": true}','None'),(201,'LocalCoordinateCoding','{\"seed\": 42, \"atoms\": 12}','None'),(202,'NCA','{\"max_iterations\": 2000, \"seed\": 42, \"normalize\": true}','None'),(203,'NCA','{\"max_iterations\": 2000, \"seed\": 42, \"optimizer\": \"sgd\"}','None'),(204,'NCA','{\"max_iterations\": 2000, \"seed\": 42, \"optimizer\": \"lbfgs\"}','None'),(205,'NCA','{\"max_iterations\": 2000, \"seed\": 42, \"optimizer\": \"lbfgs\", \"wolfe\": 0.5, \"num_basis\": 5}','None'),(206,'NCA','{\"max_iterations\": 2000, \"seed\": 42, \"optimizer\": \"lbfgs\", \"wolfe\": 0.5}','None'),(207,'NCA','{\"max_iterations\": 2000, \"seed\": 42, \"optimizer\": \"lbfgs\", \"num_basis\": 5}','None'),(208,'LogisticRegression','{}','None'),(209,'HMMGENERATE','{\"length\": 10000}','None'),(210,'HMMTRAIN','{\"states\": 20, \"seed\": 42, \"type\": \"discrete\"}','None'),(211,'HMMTRAIN','{\"states\": 20, \"seed\": 42, \"type\": \"gaussian\"}','None'),(212,'HMMVITERBI','{}','None'),(213,'KPCA','{\"kernel\": \"hyptan\"}','None'),(214,'KPCA','{\"kernel\": \"laplacian\"}','None'),(215,'KPCA','{\"kernel\": \"gaussian\"}','None'),(216,'KPCA','{\"kernel\": \"polynomial\"}','None'),(217,'KPCA','{\"nystroem\": true, \"sampling_scheme\": \"kmeans\", \"kernel\": \"gaussian\"}','None'),(218,'KPCA','{\"nystroem\": true, \"sampling_scheme\": \"random\", \"kernel\": \"gaussian\"}','None'),(219,'KPCA','{\"nystroem\": true, \"sampling_scheme\": \"random\", \"kernel\": \"polynomial\"}','None'),(220,'KPCA','{\"nystroem\": true, \"sampling_scheme\": \"random\", \"kernel\": \"cosine\"}','None'),(221,'KPCA','{\"nystroem\": true, \"sampling_scheme\": \"kmeans\", \"kernel\": \"polynomial\"}','None'),(222,'KPCA','{\"kernel\": \"cosine\"}','None'),(223,'KPCA','{\"nystroem\": true, \"sampling_scheme\": \"kmeans\", \"kernel\": \"cosine\"}','None'),(224,'KPCA','{\"kernel\": \"linear\"}','None'),(225,'ElasticNet','{}','None'),(226,'NMF','{\"rank\": 6}','None'),(227,'NMF','{\"update_rules\": \"multdiv\", \"rank\": 6, \"seed\": 42}','None'),(228,'NMF','{\"update_rules\": \"als\", \"rank\": 6, \"seed\": 42}','None'),(229,'NMF','{\"update_rules\": \"multdist\", \"rank\": 6, \"seed\": 42}','None'),(230,'HMMTRAIN','{\"states\": 20, \"type\": \"gaussian\", \"seed\": 42}','None'),(231,'HMMTRAIN','{\"states\": 20, \"type\": \"discrete\", \"seed\": 42}','None'),(232,'NCA','{\"optimizer\": \"lbfgs\", \"num_basis\": 5, \"max_iterations\": 2000, \"seed\": 42}','None'),(233,'NCA','{\"normalize\": true, \"max_iterations\": 2000, \"seed\": 42}','None'),(234,'NCA','{\"optimizer\": \"lbfgs\", \"max_iterations\": 2000, \"seed\": 42}','None'),(235,'NCA','{\"optimizer\": \"lbfgs\", \"wolfe\": 0.5, \"num_basis\": 5, \"max_iterations\": 2000, \"seed\": 42}','None'),(236,'NCA','{\"optimizer\": \"sgd\", \"max_iterations\": 2000, \"seed\": 42}','None'),(237,'NCA','{\"optimizer\": \"lbfgs\", \"wolfe\": 0.5, \"max_iterations\": 2000, \"seed\": 42}','None'),(238,'KPCA','{\"sampling_scheme\": \"random\", \"nystroem\": true, \"kernel\": \"cosine\"}','None'),(239,'KPCA','{\"sampling_scheme\": \"random\", \"nystroem\": true, \"kernel\": \"polynomial\"}','None'),(240,'KPCA','{\"sampling_scheme\": \"random\", \"nystroem\": true, \"kernel\": \"gaussian\"}','None'),(241,'KPCA','{\"sampling_scheme\": \"kmeans\", \"nystroem\": true, \"kernel\": \"gaussian\"}','None'),(242,'KPCA','{\"sampling_scheme\": \"kmeans\", \"nystroem\": true, \"kernel\": \"polynomial\"}','None'),(243,'KPCA','{\"sampling_scheme\": \"kmeans\", \"nystroem\": true, \"kernel\": \"cosine\"}','None'),(244,'ALLKRANN','{\"k\": 3, \"first_leaf_exact\": true, \"tau\": 10}','None'),(245,'ALLKRANN','{\"k\": 3, \"tau\": 10, \"sample_at_leaves\": true}','None'),(246,'ALLKRANN','{\"k\": 3, \"tau\": 10, \"naive_mode\": true}','None'),(247,'ALLKRANN','{\"k\": 3, \"tau\": 10, \"single_mode\": true}','None'),(248,'SparseCoding','{\"seed\": 42, \"max_iterations\": 100, \"atoms\": 12}','None'),(249,'SparseCoding','{\"normalize\": true, \"seed\": 42, \"atoms\": 12}','None'),(250,'LocalCoordinateCoding','{\"normalize\": true, \"seed\": 42, \"atoms\": 12}','None'),(251,'ALLKNN','{\"tree_type\": \"cover\", \"k\": 3, \"single_mode\": true, \"seed\": 42}','None'),(252,'ALLKNN','{\"k\": 3, \"epsilon\": 0.05, \"seed\": 42}','None'),(253,'ALLKNN','{\"k\": 3, \"single_mode\": true, \"seed\": 42}','None'),(254,'ALLKNN','{\"k\": 3, \"epsilon\": 0.1, \"seed\": 42}','None'),(255,'ALLKNN','{\"k\": 4}','None'),(256,'ALLKNN','{\"k\": 3, \"epsilon\": 0.15, \"seed\": 42}','None'),(257,'ALLKNN','{\"k\": 3, \"epsilon\": 0.0, \"seed\": 42}','None'),(258,'ALLKNN','{\"k\": 3, \"epsilon\": 0.2, \"seed\": 42}','None'),(259,'ALLKNN','{\"tree_type\": \"cover\", \"k\": 3, \"seed\": 42}','None'),(260,'ALLKNN','{\"k\": 3, \"epsilon\": 0.25, \"seed\": 42}','None'),(261,'GMM','{\"gaussians\": 3, \"seed\": 42}','None'),(262,'GMM','{\"gaussians\": 3}','None'),(263,'GMM','{\"gaussians\": 3, \"no_force_positive\": true, \"seed\": 42}','None'),(264,'LARS','{\"lambda2\": 0.005, \"use_cholesky\": true, \"lambda1\": 0.01}','None'),(265,'FastMKS','{\"k\": 10, \"degree\": 10, \"kernel\": \"polynomial\"}','None'),(266,'RANGESEARCH','{\"max\": 0.02, \"naive_mode\": true}','None'),(267,'PCA','{\"new_dimensionality\": 2, \"whiten\": true}','None'),(268,'PCA','{\"new_dimensionality\": 2, \"scaled\": true}','None'),(269,'RANDOMFOREST','{\"fitness_function\": \"entropy\", \"num_trees\": 50, \"minimum_samples_split\": 4, \"num_jobs\": 2, \"minimum_leaf_size\": 2, \"max_depth\": 10}','None'),(270,'ALLKRANN','{\"k\": 3, \"tau\": 10, \"first_leaf_exact\": true}','None'),(271,'ALLKNN','{\"k\": 3, \"tree_type\": \"cover\", \"seed\": 42}','None'),(272,'ALLKNN','{\"k\": 3, \"single_mode\": true, \"tree_type\": \"cover\", \"seed\": 42}','None'),(273,'NMF','{\"rank\": 6, \"update_rules\": \"multdist\", \"seed\": 42}','None'),(274,'NMF','{\"rank\": 6, \"update_rules\": \"als\", \"seed\": 42}','None'),(275,'NMF','{\"rank\": 6, \"update_rules\": \"multdiv\", \"seed\": 42}','None'),(276,'NCA','{\"num_basis\": 5, \"seed\": 42, \"optimizer\": \"lbfgs\", \"wolfe\": 0.5, \"max_iterations\": 2000}','None'),(277,'NCA','{\"seed\": 42, \"optimizer\": \"lbfgs\", \"max_iterations\": 2000}','None'),(278,'NCA','{\"seed\": 42, \"normalize\": true, \"max_iterations\": 2000}','None'),(279,'NCA','{\"seed\": 42, \"optimizer\": \"lbfgs\", \"num_basis\": 5, \"max_iterations\": 2000}','None'),(280,'NCA','{\"seed\": 42, \"optimizer\": \"lbfgs\", \"wolfe\": 0.5, \"max_iterations\": 2000}','None'),(281,'NCA','{\"seed\": 42, \"optimizer\": \"sgd\", \"max_iterations\": 2000}','None'),(282,'RANDOMFOREST','{\"fitness_function\": \"entropy\", \"minimum_leaf_size\": 2, \"minimum_samples_split\": 4, \"num_jobs\": 2, \"num_trees\": 50, \"max_depth\": 10}','None'),(283,'LARS','{\"use_cholesky\": true, \"lambda2\": 0.005, \"lambda1\": 0.01}','None'),(284,'SVR','{\"gamma\": 0.1, \"epsilon\": 1.0, \"c\": 1.0}','None'),(285,'DECISIONTREE','{\"stumps\": 10, \"minimum_leaf_size\": 10}','None'),(286,'SparseCoding','{\"normalize\": true, \"atoms\": 12, \"seed\": 42}','None'),(287,'SparseCoding','{\"max_iterations\": 100, \"atoms\": 12, \"seed\": 42}','None'),(288,'SparseCoding','{\"atoms\": 12, \"seed\": 42}','None'),(289,'RANGESEARCH','{\"single_mode\": true, \"max\": 0.02}','None'),(290,'EMST','{}','None'),(291,'EMST','{\"naive_mode\": true}','None'),(292,'LocalCoordinateCoding','{\"normalize\": true, \"atoms\": 12, \"seed\": 42}','None'),(293,'LocalCoordinateCoding','{\"atoms\": 12, \"seed\": 42}','None'),(294,'LinearRegression','{}','None'),(295,'ALLKNN','{\"k\": 3, \"seed\": 42, \"naive_mode\": true}','None'),(296,'ALLKNN','{\"k\": 3, \"seed\": 42, \"epsilon\": 0.15}','None'),(297,'ALLKNN','{\"k\": 3, \"seed\": 42, \"tree_type\": \"cover\", \"single_mode\": true}','None'),(298,'ALLKNN','{\"k\": 3, \"seed\": 42, \"tree_type\": \"cover\"}','None'),(299,'ALLKNN','{\"k\": 3, \"seed\": 42, \"epsilon\": 0.25}','None'),(300,'ALLKNN','{\"k\": 3, \"seed\": 42, \"epsilon\": 0.1}','None'),(301,'ALLKNN','{\"k\": 3, \"seed\": 42, \"single_mode\": true}','None'),(302,'ALLKNN','{\"k\": 3, \"seed\": 42, \"epsilon\": 0.2}','None'),(303,'ALLKNN','{\"k\": 3, \"seed\": 42, \"epsilon\": 0.05}','None'),(304,'ALLKNN','{\"k\": 3, \"seed\": 42, \"epsilon\": 0.0}','None'),(305,'SVR','{\"c\": 1.0, \"gamma\": 0.1, \"epsilon\": 1.0}','None'),(306,'GMM','{\"seed\": 42, \"no_force_positive\": true, \"gaussians\": 3}','None'),(307,'GMM','{\"seed\": 42, \"gaussians\": 3}','None'),(308,'KPCA','{\"kernel\": \"polynomial\", \"nystroem\": true, \"sampling_scheme\": \"kmeans\"}','None'),(309,'KPCA','{\"kernel\": \"cosine\", \"nystroem\": true, \"sampling_scheme\": \"random\"}','None'),(310,'KPCA','{\"kernel\": \"gaussian\", \"nystroem\": true, \"sampling_scheme\": \"random\"}','None'),(311,'KPCA','{\"kernel\": \"gaussian\", \"nystroem\": true, \"sampling_scheme\": \"kmeans\"}','None'),(312,'KPCA','{\"kernel\": \"polynomial\", \"nystroem\": true, \"sampling_scheme\": \"random\"}','None'),(313,'KPCA','{\"kernel\": \"cosine\", \"nystroem\": true, \"sampling_scheme\": \"kmeans\"}','None');
/*!40000 ALTER TABLE `methods` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `metrics`
--

DROP TABLE IF EXISTS `metrics`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `metrics` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `build_id` int(11) NOT NULL,
  `libary_id` int(11) NOT NULL,
  `metric` text NOT NULL,
  `dataset_id` int(11) NOT NULL,
  `method_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `build_id` (`build_id`),
  KEY `libary_id` (`libary_id`),
  KEY `dataset_id` (`dataset_id`),
  KEY `method_id` (`method_id`),
  CONSTRAINT `metrics_ibfk_1` FOREIGN KEY (`build_id`) REFERENCES `builds` (`id`) ON DELETE CASCADE,
  CONSTRAINT `metrics_ibfk_2` FOREIGN KEY (`libary_id`) REFERENCES `libraries` (`id`) ON DELETE CASCADE,
  CONSTRAINT `metrics_ibfk_3` FOREIGN KEY (`dataset_id`) REFERENCES `datasets` (`id`) ON DELETE CASCADE,
  CONSTRAINT `metrics_ibfk_4` FOREIGN KEY (`method_id`) REFERENCES `methods` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=1753 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `metrics`
--

LOCK TABLES `metrics` WRITE;
/*!40000 ALTER TABLE `metrics` DISABLE KEYS */;
INSERT INTO `metrics` VALUES (1,1,1,'{\"ComputingNeighbors\": 0.000229, \"TreeBuilding\": 2.5333333333333334e-05, \"Runtime\": 0.00025433333333333335}',1,17),(2,1,1,'{\"ComputingNeighbors\": 0.0063, \"TreeBuilding\": 0.0005523333333333334, \"Runtime\": 0.006852333333333333}',2,17),(3,1,1,'{\"ComputingNeighbors\": 0.024032333333333333, \"TreeBuilding\": 0.0019049999999999998, \"Runtime\": 0.02593733333333333}',1,17),(4,1,1,'{\"ComputingNeighbors\": 19.162212666666665, \"TreeBuilding\": 0.13276333333333334, \"Runtime\": 19.294976000000002}',3,17),(5,1,1,'{\"ComputingNeighbors\": 7.896034333333333, \"TreeBuilding\": 0.16930866666666666, \"Runtime\": 8.065343}',4,17),(6,1,1,'{\"ComputingNeighbors\": 4.242267, \"TreeBuilding\": 0.6530140000000001, \"Runtime\": 4.895281}',5,17),(7,1,1,'{\"ComputingNeighbors\": 620.0985390000001, \"TreeBuilding\": 2.1036636666666664, \"Runtime\": 622.2022026666667}',6,17),(8,1,1,'{\"ComputingNeighbors\": 4362.438722666667, \"TreeBuilding\": 43.500870666666664, \"Runtime\": 4405.9395933333335}',7,17),(9,1,1,'{\"ComputingNeighbors\": 1362.721244, \"TreeBuilding\": 10.516994333333335, \"Runtime\": 1373.2382383333334}',8,17),(10,1,1,'{\"ComputingNeighbors\": 6289.383333000001, \"TreeBuilding\": 1.8520393333333331, \"Runtime\": 6291.235372333333}',9,17),(11,1,1,'{\"ComputingNeighbors\": 0.00022666666666666666, \"TreeBuilding\": 2.5e-05, \"Runtime\": 0.0002516666666666667}',1,21),(12,1,1,'{\"ComputingNeighbors\": 0.0062766666666666665, \"TreeBuilding\": 0.000546, \"Runtime\": 0.006822666666666667}',2,21),(13,1,1,'{\"ComputingNeighbors\": 0.023973333333333333, \"TreeBuilding\": 0.0019056666666666666, \"Runtime\": 0.025879}',1,21),(14,1,1,'{\"ComputingNeighbors\": 19.150233666666665, \"TreeBuilding\": 0.13246333333333335, \"Runtime\": 19.282697000000002}',3,21),(15,1,1,'{\"ComputingNeighbors\": 7.766789, \"TreeBuilding\": 0.16945566666666667, \"Runtime\": 7.936244666666667}',4,21),(16,1,1,'{\"ComputingNeighbors\": 4.247122, \"TreeBuilding\": 0.6518656666666667, \"Runtime\": 4.898987666666667}',5,21),(17,1,1,'{\"ComputingNeighbors\": 616.4241263333333, \"TreeBuilding\": 2.097482, \"Runtime\": 618.5216083333333}',6,21),(18,1,1,'{\"ComputingNeighbors\": 4361.227372333334, \"TreeBuilding\": 43.65010733333333, \"Runtime\": 4404.877479666667}',7,21),(19,1,1,'{\"ComputingNeighbors\": 1368.2219086666666, \"TreeBuilding\": 10.520220333333334, \"Runtime\": 1378.742129}',8,21),(20,1,1,'{\"ComputingNeighbors\": 6288.4051033333335, \"TreeBuilding\": 1.8511976666666667, \"Runtime\": 6290.256301}',9,21),(21,1,1,'{\"ComputingNeighbors\": 0.00022100000000000003, \"TreeBuilding\": 2.466666666666667e-05, \"Runtime\": 0.00024566666666666666}',1,24),(22,1,1,'{\"ComputingNeighbors\": 0.0057663333333333325, \"TreeBuilding\": 0.000555, \"Runtime\": 0.006321333333333334}',2,24),(23,1,1,'{\"ComputingNeighbors\": 0.021865, \"TreeBuilding\": 0.0019126666666666667, \"Runtime\": 0.02377766666666667}',1,24),(24,1,1,'{\"ComputingNeighbors\": 16.75605566666667, \"TreeBuilding\": 0.13250199999999998, \"Runtime\": 16.888557666666667}',3,24),(25,1,1,'{\"ComputingNeighbors\": 5.514483333333334, \"TreeBuilding\": 0.16953966666666664, \"Runtime\": 5.684023000000001}',4,24),(26,1,1,'{\"ComputingNeighbors\": 3.7970729999999997, \"TreeBuilding\": 0.6524483333333334, \"Runtime\": 4.449521333333333}',5,24),(27,1,1,'{\"ComputingNeighbors\": 396.9525456666666, \"TreeBuilding\": 2.0976966666666663, \"Runtime\": 399.0502423333333}',6,24),(28,1,1,'{\"ComputingNeighbors\": 4333.217477666666, \"TreeBuilding\": 43.50904699999999, \"Runtime\": 4376.7265246666675}',7,24),(29,1,1,'{\"ComputingNeighbors\": 985.7323776666667, \"TreeBuilding\": 10.506139666666666, \"Runtime\": 996.2385173333333}',8,24),(30,1,1,'{\"ComputingNeighbors\": 6252.485295999999, \"TreeBuilding\": 1.852089, \"Runtime\": 6254.337384999999}',9,24),(31,1,1,'{\"ComputingNeighbors\": 0.00022633333333333335, \"TreeBuilding\": 2.8000000000000003e-05, \"Runtime\": 0.00025433333333333335}',1,25),(32,1,1,'{\"ComputingNeighbors\": 0.006072, \"TreeBuilding\": 0.0005653333333333334, \"Runtime\": 0.006637333333333333}',2,25),(33,1,1,'{\"ComputingNeighbors\": 0.023243666666666662, \"TreeBuilding\": 0.0019076666666666667, \"Runtime\": 0.02515133333333333}',1,25),(34,1,1,'{\"ComputingNeighbors\": 18.28080466666667, \"TreeBuilding\": 0.13249999999999998, \"Runtime\": 18.41330466666667}',3,25),(35,1,1,'{\"ComputingNeighbors\": 6.944460666666667, \"TreeBuilding\": 0.17040966666666668, \"Runtime\": 7.114870333333333}',4,25),(36,1,1,'{\"ComputingNeighbors\": 4.0804116666666665, \"TreeBuilding\": 0.6527716666666666, \"Runtime\": 4.733183333333334}',5,25),(37,1,1,'{\"ComputingNeighbors\": 530.2644963333333, \"TreeBuilding\": 2.1064073333333333, \"Runtime\": 532.3709036666667}',6,25),(38,1,1,'{\"ComputingNeighbors\": 4350.185976333333, \"TreeBuilding\": 43.512515, \"Runtime\": 4393.698491333333}',7,25),(39,1,1,'{\"ComputingNeighbors\": 1218.0315313333333, \"TreeBuilding\": 10.515280333333335, \"Runtime\": 1228.5468116666666}',8,25),(40,1,1,'{\"ComputingNeighbors\": 6279.920400333333, \"TreeBuilding\": 1.8532373333333334, \"Runtime\": 6281.773637666666}',9,25),(41,1,1,'{\"ComputingNeighbors\": 0.000224, \"TreeBuilding\": 2.5e-05, \"Runtime\": 0.00024900000000000004}',1,27),(42,1,1,'{\"ComputingNeighbors\": 0.005874999999999999, \"TreeBuilding\": 0.0005476666666666667, \"Runtime\": 0.006422666666666666}',2,27),(43,1,1,'{\"ComputingNeighbors\": 0.022509666666666667, \"TreeBuilding\": 0.001915, \"Runtime\": 0.024424666666666667}',1,27),(44,1,1,'{\"ComputingNeighbors\": 17.469753666666666, \"TreeBuilding\": 0.13265033333333331, \"Runtime\": 17.602404}',3,27),(45,1,1,'{\"ComputingNeighbors\": 6.162289666666666, \"TreeBuilding\": 0.17161099999999999, \"Runtime\": 6.3339006666666675}',4,27),(46,1,1,'{\"ComputingNeighbors\": 3.923700333333333, \"TreeBuilding\": 0.6525606666666667, \"Runtime\": 4.576261}',5,27),(47,1,1,'{\"ComputingNeighbors\": 456.7175326666666, \"TreeBuilding\": 2.101049666666667, \"Runtime\": 458.8185823333333}',6,27),(48,1,1,'{\"ComputingNeighbors\": 4335.695121, \"TreeBuilding\": 43.47631633333333, \"Runtime\": 4379.171437333333}',7,27),(49,1,1,'{\"ComputingNeighbors\": 1093.4983583333333, \"TreeBuilding\": 10.498949333333334, \"Runtime\": 1103.9973076666668}',8,27),(50,1,1,'{\"ComputingNeighbors\": 6264.68043, \"TreeBuilding\": 1.8535899999999998, \"Runtime\": 6266.53402}',9,27),(51,1,1,'{\"ComputingNeighbors\": 0.00022099999999999998, \"TreeBuilding\": 2.5333333333333334e-05, \"Runtime\": 0.0002463333333333334}',1,28),(52,1,1,'{\"ComputingNeighbors\": 0.0054986666666666665, \"TreeBuilding\": 0.0005473333333333333, \"Runtime\": 0.006046}',2,28),(53,1,1,'{\"ComputingNeighbors\": 0.020670333333333332, \"TreeBuilding\": 0.0019216666666666664, \"Runtime\": 0.022592}',1,28),(54,1,1,'{\"ComputingNeighbors\": 15.386619000000001, \"TreeBuilding\": 0.13238899999999998, \"Runtime\": 15.519008}',3,28),(55,1,1,'{\"ComputingNeighbors\": 4.489312333333333, \"TreeBuilding\": 0.17080266666666666, \"Runtime\": 4.660115}',4,28),(56,1,1,'{\"ComputingNeighbors\": 3.5689046666666666, \"TreeBuilding\": 0.653891, \"Runtime\": 4.222795666666667}',5,28),(57,1,1,'{\"ComputingNeighbors\": 306.294925, \"TreeBuilding\": 2.0994646666666665, \"Runtime\": 308.39438966666665}',6,28),(58,1,1,'{\"ComputingNeighbors\": 4292.144107666666, \"TreeBuilding\": 43.542371333333335, \"Runtime\": 4335.686479}',7,28),(59,1,1,'{\"ComputingNeighbors\": 809.2690796666666, \"TreeBuilding\": 10.500525000000001, \"Runtime\": 819.7696046666666}',8,28),(60,1,1,'{\"ComputingNeighbors\": 6212.705110666667, \"TreeBuilding\": 1.8507836666666666, \"Runtime\": 6214.5558943333335}',9,28),(61,1,1,'{\"ComputingNeighbors\": 0.00022233333333333336, \"TreeBuilding\": 2.5e-05, \"Runtime\": 0.0002473333333333333}',1,29),(62,1,1,'{\"ComputingNeighbors\": 0.005593333333333333, \"TreeBuilding\": 0.000549, \"Runtime\": 0.006142333333333333}',2,29),(63,1,1,'{\"ComputingNeighbors\": 0.021278666666666668, \"TreeBuilding\": 0.001906, \"Runtime\": 0.02318466666666667}',1,29),(64,1,1,'{\"ComputingNeighbors\": 16.023111, \"TreeBuilding\": 0.13258933333333334, \"Runtime\": 16.155700333333332}',3,29),(65,1,1,'{\"ComputingNeighbors\": 4.954959333333334, \"TreeBuilding\": 0.17157999999999998, \"Runtime\": 5.126539333333334}',4,29),(66,1,1,'{\"ComputingNeighbors\": 3.676626666666667, \"TreeBuilding\": 0.652057, \"Runtime\": 4.328683666666667}',5,29),(67,1,1,'{\"ComputingNeighbors\": 347.706641, \"TreeBuilding\": 2.106935, \"Runtime\": 349.813576}',6,29),(68,1,1,'{\"ComputingNeighbors\": 4319.813183333333, \"TreeBuilding\": 43.51822066666667, \"Runtime\": 4363.331404}',7,29),(69,1,1,'{\"ComputingNeighbors\": 891.2640166666666, \"TreeBuilding\": 10.497449000000001, \"Runtime\": 901.7614656666668}',8,29),(70,1,1,'{\"ComputingNeighbors\": 6235.070644333333, \"TreeBuilding\": 1.8540946666666667, \"Runtime\": 6236.924738999999}',9,29),(71,2,2,'{\"ComputingNeighbors\": 0.000574, \"TreeBuilding\": 0.000253, \"Runtime\": 0.000827}',1,17),(72,2,2,'{\"ComputingNeighbors\": 0.011935000000000001, \"TreeBuilding\": 0.0034296666666666664, \"Runtime\": 0.015364666666666665}',2,17),(73,2,2,'{\"ComputingNeighbors\": 0.04045766666666667, \"TreeBuilding\": 0.010543666666666666, \"Runtime\": 0.051001333333333336}',1,17),(74,2,2,'{\"Runtime\": \"failure\"}',3,17),(75,3,1,'{\"TreeBuilding\": 2.5333333333333334e-05, \"ComputingNeighbors\": 0.00022966666666666667, \"Runtime\": 0.00025499999999999996}',1,17),(76,3,1,'{\"TreeBuilding\": 0.0005503333333333333, \"ComputingNeighbors\": 0.006281666666666667, \"Runtime\": 0.006832}',2,17),(77,3,1,'{\"TreeBuilding\": 0.0019119999999999999, \"ComputingNeighbors\": 0.024099333333333334, \"Runtime\": 0.026011333333333334}',1,17),(78,4,2,'{\"ComputingNeighbors\": 0.0005716666666666666, \"TreeBuilding\": 0.00025399999999999994, \"Runtime\": 0.0008256666666666667}',1,21),(79,4,2,'{\"ComputingNeighbors\": 0.011944666666666666, \"TreeBuilding\": 0.0034076666666666665, \"Runtime\": 0.015352333333333334}',2,21),(80,4,2,'{\"ComputingNeighbors\": 0.040434333333333336, \"TreeBuilding\": 0.010486, \"Runtime\": 0.05092033333333334}',1,21),(81,3,1,'{\"TreeBuilding\": 0.13255833333333333, \"ComputingNeighbors\": 19.148528666666667, \"Runtime\": 19.281087}',3,17),(82,5,3,'{\"Runtime\": 20.772047666666666}',10,82),(83,3,1,'{\"TreeBuilding\": 0.16934666666666667, \"ComputingNeighbors\": 7.879899666666667, \"Runtime\": 8.049246333333334}',4,17),(84,4,2,'{\"ComputingNeighbors\": 29.03879566666667, \"TreeBuilding\": 0.689509, \"Runtime\": 29.72830466666667}',3,21),(85,5,3,'{\"Runtime\": 20.787808666666667}',10,80),(86,3,1,'{\"TreeBuilding\": 0.651854, \"ComputingNeighbors\": 4.245625999999999, \"Runtime\": 4.897480000000001}',5,17),(87,4,2,'{\"ComputingNeighbors\": 8.058857000000001, \"TreeBuilding\": 0.7009013333333334, \"Runtime\": 8.759758333333334}',4,21),(88,5,3,'{\"Runtime\": 15.819679666666667}',10,81),(89,5,3,'{\"Training\": 6.233333333333334e-05, \"Runtime\": 0.008256333333333334, \"Testing\": 6.233333333333334e-05}',11,66),(90,5,3,'{\"Training\": 6.166666666666667e-05, \"Runtime\": 0.017462666666666665, \"Testing\": 6.166666666666667e-05}',12,66),(91,5,3,'{\"Training\": 1.174527, \"Runtime\": 1.195171, \"Testing\": 1.174527}',13,66),(92,5,3,'{\"Training\": 6.1e-05, \"Runtime\": 0.010600666666666666, \"Testing\": 6.1e-05}',11,67),(93,5,3,'{\"Training\": 6.1e-05, \"Runtime\": 0.006315, \"Testing\": 6.1e-05}',12,67),(94,5,3,'{\"Training\": 1.1752053333333334, \"Runtime\": 1.205615, \"Testing\": 1.1752053333333334}',13,67),(95,5,3,'{\"Runtime\": \"failure\"}',14,39),(96,5,3,'{\"Runtime\": 0.1545173333333333}',11,84),(97,5,3,'{\"Runtime\": 6.321281666666667}',15,84),(98,5,3,'{\"Runtime\": 0.5522830000000001}',1,84),(99,4,2,'{\"ComputingNeighbors\": 7.3748846666666665, \"TreeBuilding\": 2.985230666666667, \"Runtime\": 10.360115333333335}',5,21),(100,5,3,'{\"Runtime\": 14.457007333333332}',16,84),(101,4,2,'{\"ComputingNeighbors\": 371.1167643333333, \"TreeBuilding\": 4.017601666666667, \"Runtime\": 375.134366}',6,21),(102,3,1,'{\"TreeBuilding\": 2.1018909999999997, \"ComputingNeighbors\": 620.2270963333334, \"Runtime\": 622.3289873333333}',6,17),(103,5,3,'{\"Runtime\": \">9000\"}',17,84),(104,3,1,'{\"TreeBuilding\": 43.584436333333336, \"ComputingNeighbors\": 4367.886337, \"Runtime\": 4411.470773333333}',7,17),(105,5,3,'{\"Runtime\": \">9000\"}',13,84),(106,4,2,'{\"ComputingNeighbors\": 5810.423760999999, \"TreeBuilding\": 6.5875406666666665, \"Runtime\": 5817.011301666666}',7,21),(107,3,1,'{\"TreeBuilding\": 10.509290333333334, \"ComputingNeighbors\": 1362.1632636666666, \"Runtime\": 1372.672554}',8,17),(108,4,2,'{\"ComputingNeighbors\": 1413.7043993333334, \"TreeBuilding\": 70.12764566666667, \"Runtime\": 1483.832045}',8,21),(109,5,3,'{\"Runtime\": \">9000\"}',18,84),(110,5,3,'{\"Runtime\": \">9000\"}',13,84),(111,3,1,'{\"TreeBuilding\": 1.8551016666666669, \"ComputingNeighbors\": 6293.154218, \"Runtime\": 6295.009319666667}',9,17),(112,3,1,'{\"TreeBuilding\": 2.5333333333333334e-05, \"ComputingNeighbors\": 0.00022366666666666664, \"Runtime\": 0.00024900000000000004}',1,21),(113,3,1,'{\"TreeBuilding\": 0.0005656666666666667, \"ComputingNeighbors\": 0.006172333333333333, \"Runtime\": 0.006738}',2,21),(114,3,1,'{\"TreeBuilding\": 0.0019213333333333333, \"ComputingNeighbors\": 0.024066, \"Runtime\": 0.025987333333333335}',1,21),(115,3,1,'{\"TreeBuilding\": 0.13274333333333332, \"ComputingNeighbors\": 19.11145266666667, \"Runtime\": 19.244196000000002}',3,21),(116,3,1,'{\"TreeBuilding\": 0.16943833333333333, \"ComputingNeighbors\": 7.781180333333334, \"Runtime\": 7.950618666666667}',4,21),(117,3,1,'{\"TreeBuilding\": 0.652087, \"ComputingNeighbors\": 4.242384666666667, \"Runtime\": 4.894471666666667}',5,21),(118,3,1,'{\"TreeBuilding\": 2.0975863333333336, \"ComputingNeighbors\": 616.6391306666666, \"Runtime\": 618.7367169999999}',6,21),(119,4,2,'{\"ComputingNeighbors\": 6230.0482883333325, \"TreeBuilding\": 52.06364933333333, \"Runtime\": 6282.1119376666675}',9,21),(120,4,2,'{\"ComputingNeighbors\": 0.0005683333333333333, \"TreeBuilding\": 0.00028033333333333333, \"Runtime\": 0.0008486666666666667}',1,24),(121,4,2,'{\"ComputingNeighbors\": 0.011423333333333332, \"TreeBuilding\": 0.0034126666666666667, \"Runtime\": 0.014836}',2,24),(122,4,2,'{\"ComputingNeighbors\": 0.03854466666666667, \"TreeBuilding\": 0.010520333333333333, \"Runtime\": 0.049065}',1,24),(123,4,2,'{\"ComputingNeighbors\": 27.555835000000002, \"TreeBuilding\": 0.6899716666666666, \"Runtime\": 28.245806666666667}',3,24),(124,4,2,'{\"ComputingNeighbors\": 6.683185333333334, \"TreeBuilding\": 0.6992706666666667, \"Runtime\": 7.382456}',4,24),(125,4,2,'{\"ComputingNeighbors\": 7.060815666666667, \"TreeBuilding\": 2.9836476666666667, \"Runtime\": 10.044463333333333}',5,24),(126,4,2,'{\"ComputingNeighbors\": 295.12688, \"TreeBuilding\": 4.019524666666666, \"Runtime\": 299.1464046666667}',6,24),(127,5,3,'{\"Runtime\": \">9000\"}',4,84),(128,3,1,'{\"TreeBuilding\": 43.59960866666666, \"ComputingNeighbors\": 4362.414668666667, \"Runtime\": 4406.014277333334}',7,21),(129,5,3,'{\"Runtime\": \">9000\"}',3,84),(130,3,1,'{\"TreeBuilding\": 10.512217, \"ComputingNeighbors\": 1367.952953, \"Runtime\": 1378.4651700000002}',8,21),(131,4,2,'{\"ComputingNeighbors\": 5796.484532333333, \"TreeBuilding\": 6.592972333333333, \"Runtime\": 5803.077504666668}',7,24),(132,5,3,'{\"Runtime\": \">9000\"}',5,84),(133,4,2,'{\"ComputingNeighbors\": 1174.469376, \"TreeBuilding\": 70.15858200000001, \"Runtime\": 1244.627958}',8,24),(134,5,3,'{\"Runtime\": \">9000\"}',6,84),(135,3,1,'{\"TreeBuilding\": 1.8603046666666667, \"ComputingNeighbors\": 6290.246456666667, \"Runtime\": 6292.1067613333325}',9,21),(136,3,1,'{\"TreeBuilding\": 2.5e-05, \"ComputingNeighbors\": 0.00022666666666666668, \"Runtime\": 0.0002516666666666667}',1,25),(137,3,1,'{\"TreeBuilding\": 0.0005553333333333334, \"ComputingNeighbors\": 0.006062333333333333, \"Runtime\": 0.006617666666666667}',2,25),(138,3,1,'{\"TreeBuilding\": 0.0019133333333333335, \"ComputingNeighbors\": 0.023280333333333333, \"Runtime\": 0.02519366666666667}',1,25),(139,3,1,'{\"TreeBuilding\": 0.13262933333333332, \"ComputingNeighbors\": 18.271767333333333, \"Runtime\": 18.404396666666667}',3,25),(140,3,1,'{\"TreeBuilding\": 0.170662, \"ComputingNeighbors\": 6.947341333333334, \"Runtime\": 7.118003333333334}',4,25),(141,3,1,'{\"TreeBuilding\": 0.6521659999999999, \"ComputingNeighbors\": 4.074734333333333, \"Runtime\": 4.726900333333333}',5,25),(142,3,1,'{\"TreeBuilding\": 2.1048566666666666, \"ComputingNeighbors\": 529.806815, \"Runtime\": 531.9116716666666}',6,25),(143,5,3,'{\"Runtime\": \">9000\"}',7,84),(144,4,2,'{\"ComputingNeighbors\": 6217.946672666668, \"TreeBuilding\": 52.066341, \"Runtime\": 6270.013013666667}',9,24),(145,4,2,'{\"ComputingNeighbors\": 0.0005696666666666667, \"TreeBuilding\": 0.0002523333333333333, \"Runtime\": 0.0008220000000000001}',1,25),(146,4,2,'{\"ComputingNeighbors\": 0.011818666666666667, \"TreeBuilding\": 0.0034113333333333335, \"Runtime\": 0.01523}',2,25),(147,4,2,'{\"ComputingNeighbors\": 0.03989, \"TreeBuilding\": 0.010478000000000001, \"Runtime\": 0.050368}',1,25),(148,4,2,'{\"ComputingNeighbors\": 28.546756666666667, \"TreeBuilding\": 0.6915823333333333, \"Runtime\": 29.238338999999996}',3,25),(149,4,2,'{\"ComputingNeighbors\": 7.538616666666667, \"TreeBuilding\": 0.698546, \"Runtime\": 8.237162666666668}',4,25),(150,4,2,'{\"ComputingNeighbors\": 7.245636999999999, \"TreeBuilding\": 2.9832699999999996, \"Runtime\": 10.228907}',5,25),(151,4,2,'{\"ComputingNeighbors\": 342.5650633333333, \"TreeBuilding\": 4.015721333333333, \"Runtime\": 346.58078466666666}',6,25),(152,5,3,'{\"Runtime\": \">9000\"}',8,84),(153,3,1,'{\"TreeBuilding\": 43.510904333333336, \"ComputingNeighbors\": 4351.771702333333, \"Runtime\": 4395.282606666667}',7,25),(154,3,1,'{\"TreeBuilding\": 10.507781333333334, \"ComputingNeighbors\": 1217.349225, \"Runtime\": 1227.8570063333334}',8,25),(155,5,3,'{\"Runtime\": \">9000\"}',9,84),(156,4,2,'{\"ComputingNeighbors\": 5815.4108006666675, \"TreeBuilding\": 6.591587999999999, \"Runtime\": 5822.002388666667}',7,25),(157,4,2,'{\"ComputingNeighbors\": 1322.7683183333334, \"TreeBuilding\": 70.08397066666667, \"Runtime\": 1392.8522890000002}',8,25),(158,5,3,'{\"Runtime\": \">9000\"}',19,84),(159,5,3,'{\"Runtime\": 0.1710826666666667}',11,88),(160,5,3,'{\"Runtime\": 2.527809}',15,88),(161,5,3,'{\"Runtime\": 0.5349983333333334}',1,88),(162,5,3,'{\"Runtime\": 6.792966}',16,88),(163,3,1,'{\"TreeBuilding\": 1.8664360000000002, \"ComputingNeighbors\": 6278.731495333333, \"Runtime\": 6280.597931333334}',9,25),(164,3,1,'{\"TreeBuilding\": 2.5e-05, \"ComputingNeighbors\": 0.00022199999999999998, \"Runtime\": 0.000247}',1,24),(165,3,1,'{\"TreeBuilding\": 0.0005566666666666666, \"ComputingNeighbors\": 0.005744, \"Runtime\": 0.006300666666666667}',2,24),(166,3,1,'{\"TreeBuilding\": 0.0019096666666666665, \"ComputingNeighbors\": 0.021822666666666667, \"Runtime\": 0.023732333333333338}',1,24),(167,3,1,'{\"TreeBuilding\": 0.13246233333333335, \"ComputingNeighbors\": 16.72659733333333, \"Runtime\": 16.859059666666667}',3,24),(168,3,1,'{\"TreeBuilding\": 0.1712296666666667, \"ComputingNeighbors\": 5.496934333333333, \"Runtime\": 5.668164}',4,24),(169,3,1,'{\"TreeBuilding\": 0.651819, \"ComputingNeighbors\": 3.794617, \"Runtime\": 4.446435999999999}',5,24),(170,3,1,'{\"TreeBuilding\": 2.098466666666667, \"ComputingNeighbors\": 396.5435166666666, \"Runtime\": 398.64198333333326}',6,24),(171,5,3,'{\"Runtime\": \">9000\"}',17,88),(172,4,2,'{\"ComputingNeighbors\": 6226.717428666666, \"TreeBuilding\": 52.069613333333336, \"Runtime\": 6278.787042}',9,25),(173,4,2,'{\"ComputingNeighbors\": 0.0005686666666666666, \"TreeBuilding\": 0.00025266666666666666, \"Runtime\": 0.0008213333333333333}',1,29),(174,4,2,'{\"ComputingNeighbors\": 0.011292000000000002, \"TreeBuilding\": 0.0034236666666666664, \"Runtime\": 0.014715666666666667}',2,29),(175,4,2,'{\"ComputingNeighbors\": 0.03799233333333333, \"TreeBuilding\": 0.010533333333333334, \"Runtime\": 0.04852566666666667}',1,29),(176,4,2,'{\"ComputingNeighbors\": 27.134416333333334, \"TreeBuilding\": 0.6900236666666668, \"Runtime\": 27.82444}',3,29),(177,4,2,'{\"ComputingNeighbors\": 6.315415333333334, \"TreeBuilding\": 0.6981016666666666, \"Runtime\": 7.013517}',4,29),(178,4,2,'{\"ComputingNeighbors\": 6.981972666666667, \"TreeBuilding\": 2.984486, \"Runtime\": 9.966458666666666}',5,29),(179,5,3,'{\"Runtime\": \">9000\"}',13,88),(180,4,2,'{\"ComputingNeighbors\": 275.48909066666664, \"TreeBuilding\": 4.015955666666666, \"Runtime\": 279.50504633333327}',6,29),(181,3,1,'{\"TreeBuilding\": 43.43587733333334, \"ComputingNeighbors\": 4328.171749666667, \"Runtime\": 4371.607627}',7,24),(182,3,1,'{\"TreeBuilding\": 10.489726333333332, \"ComputingNeighbors\": 984.9345116666667, \"Runtime\": 995.424238}',8,24),(183,5,3,'{\"Runtime\": \">9000\"}',18,88),(184,4,2,'{\"ComputingNeighbors\": 5795.404826333334, \"TreeBuilding\": 6.595918, \"Runtime\": 5802.000744333333}',7,29),(185,5,3,'{\"Runtime\": \">9000\"}',13,88),(186,4,2,'{\"ComputingNeighbors\": 1109.2632586666668, \"TreeBuilding\": 70.13955133333333, \"Runtime\": 1179.40281}',8,29),(187,3,1,'{\"TreeBuilding\": 1.871901, \"ComputingNeighbors\": 6247.540945333333, \"Runtime\": 6249.412846333333}',9,24),(188,3,1,'{\"TreeBuilding\": 2.5333333333333334e-05, \"ComputingNeighbors\": 0.00022133333333333334, \"Runtime\": 0.0002466666666666667}',1,28),(189,3,1,'{\"TreeBuilding\": 0.0005466666666666667, \"ComputingNeighbors\": 0.0055056666666666665, \"Runtime\": 0.006052333333333333}',2,28),(190,3,1,'{\"TreeBuilding\": 0.0019056666666666666, \"ComputingNeighbors\": 0.020661000000000002, \"Runtime\": 0.02256666666666667}',1,28),(191,3,1,'{\"TreeBuilding\": 0.13253233333333334, \"ComputingNeighbors\": 15.381126666666665, \"Runtime\": 15.513658999999999}',3,28),(192,3,1,'{\"TreeBuilding\": 0.17159966666666668, \"ComputingNeighbors\": 4.489545333333333, \"Runtime\": 4.661145}',4,28),(193,3,1,'{\"TreeBuilding\": 0.6514540000000001, \"ComputingNeighbors\": 3.5737846666666666, \"Runtime\": 4.225238666666667}',5,28),(194,3,1,'{\"TreeBuilding\": 2.1012403333333336, \"ComputingNeighbors\": 306.2567916666667, \"Runtime\": 308.35803200000004}',6,28),(195,5,3,'{\"Runtime\": \">9000\"}',4,88),(196,5,3,'{\"Runtime\": \">9000\"}',3,88),(197,3,1,'{\"TreeBuilding\": 43.490638, \"ComputingNeighbors\": 4300.138373666666, \"Runtime\": 4343.629011666667}',7,28),(198,4,2,'{\"ComputingNeighbors\": 6215.220759666667, \"TreeBuilding\": 52.050887666666675, \"Runtime\": 6267.271647333332}',9,29),(199,4,2,'{\"ComputingNeighbors\": 0.000592, \"TreeBuilding\": 0.0002523333333333333, \"Runtime\": 0.0008443333333333332}',1,28),(200,4,2,'{\"ComputingNeighbors\": 0.011185666666666665, \"TreeBuilding\": 0.003403, \"Runtime\": 0.014588666666666666}',2,28),(201,4,2,'{\"ComputingNeighbors\": 0.037568, \"TreeBuilding\": 0.010481666666666667, \"Runtime\": 0.048049666666666664}',1,28),(202,4,2,'{\"ComputingNeighbors\": 26.683406, \"TreeBuilding\": 0.6899843333333333, \"Runtime\": 27.373390333333333}',3,28),(203,4,2,'{\"ComputingNeighbors\": 5.990892333333334, \"TreeBuilding\": 0.6987196666666667, \"Runtime\": 6.689612}',4,28),(204,4,2,'{\"ComputingNeighbors\": 6.900646666666667, \"TreeBuilding\": 2.9824479999999998, \"Runtime\": 9.883094666666667}',5,28),(205,4,2,'{\"ComputingNeighbors\": 257.95222566666666, \"TreeBuilding\": 4.029767666666666, \"Runtime\": 261.9819933333334}',6,28),(206,3,1,'{\"TreeBuilding\": 10.475104333333334, \"ComputingNeighbors\": 809.6374140000001, \"Runtime\": 820.1125183333334}',8,28),(207,5,3,'{\"Runtime\": \">9000\"}',5,88),(208,6,4,'{\"Runtime\": 0.0012718041737874348}',15,65),(209,6,4,'{\"Runtime\": 0.004234711329142253}',20,65),(210,6,4,'{\"Runtime\": 1.5097521146138508}',18,65),(211,6,4,'{\"Runtime\": 0.11190501848856609}',13,65),(212,6,4,'{\"Runtime\": 0.018039226531982422}',15,40),(213,6,4,'{\"Runtime\": 0.013962507247924805}',20,40),(214,6,4,'{\"Runtime\": 216.92064452171326}',21,40),(215,6,4,'{\"Runtime\": \"failure\"}',6,54),(216,6,4,'{\"Runtime\": 0.025563398996988933}',2,50),(217,6,4,'{\"Runtime\": 7.542338291803996}',5,52),(218,6,4,'{\"Runtime\": 0.03426233927408854}',1,52),(219,6,4,'{\"Runtime\": 0.657507578531901}',3,49),(220,6,4,'{\"Runtime\": 0.010685046513875326}',22,51),(221,6,4,'{\"Runtime\": 0.0015427271525065105}',1,53),(222,6,4,'{\"Runtime\": 0.0014495054880777996}',11,53),(223,6,4,'{\"Runtime\": 11.933173418045044}',7,55),(224,6,4,'{\"Runtime\": 0.4555022716522217}',4,55),(225,6,4,'{\"Runtime\": 0.011542717615763346}',23,56),(226,6,4,'{\"Runtime\": \"failure\"}',10,81),(227,6,4,'{\"Runtime\": \"failure\"}',10,80),(228,6,4,'{\"Runtime\": \"failure\"}',10,82),(229,6,4,'{\"Precision\": 1, \"Runtime\": 0.001238107681274414, \"ACC\": 1, \"Recall\": 1, \"MSE\": 0, \"MCC\": 1}',11,45),(230,6,4,'{\"Precision\": 0.6087383406971035, \"Runtime\": 0.016290982564290363, \"ACC\": 0.7006946541830262, \"Recall\": 0.7006946541830262, \"MSE\": 0.07051282051282051, \"MCC\": 0.29545357457740323}',24,45),(231,6,4,'{\"Precision\": 0.5529465930018417, \"Runtime\": 0.7417275905609131, \"ACC\": 0.5704109266454727, \"Recall\": 0.5704109266454728, \"MSE\": 0.13591022443890274, \"MCC\": 0.12211500605544547}',25,45),(232,6,4,'{\"Precision\": 0.7844761667594411, \"Runtime\": 5.42990771929423, \"ACC\": 0.8573437446604929, \"Recall\": 0.8573437446604929, \"MSE\": 0.023462434227551107, \"MCC\": 0.6376700673435486}',26,45),(233,6,4,'{\"Precision\": 0.764320764844863, \"Runtime\": 3.4033658504486084, \"ACC\": 0.7379942078403464, \"Recall\": 0.7379942078403464, \"MSE\": 0.07387456714120816, \"MCC\": 0.5016246048391471}',3,45),(234,6,4,'{\"Precision\": 0.7973573477734494, \"Runtime\": 0.04091811180114746, \"ACC\": 0.7483651324843902, \"Recall\": 0.7483651324843902, \"MSE\": 0.020922746781115876, \"MCC\": 0.5435188941516556}',27,45),(235,6,4,'{\"Precision\": 0.8947706623502331, \"Runtime\": 35.85412128766378, \"ACC\": 0.8525478516689061, \"Recall\": 0.8525478516689065, \"MSE\": 0.010164190774042221, \"MCC\": 0.7461247855780851}',28,45),(236,6,4,'{\"Precision\": 0.5307915758896151, \"Runtime\": 0.009739160537719727, \"ACC\": 0.5574421965317918, \"Recall\": 0.557442196531792, \"MSE\": 0.015086206896551725, \"MCC\": 0.08411268046554826}',29,45),(237,6,4,'{\"Precision\": 0.8717392359594194, \"Runtime\": 0.010916550954182943, \"ACC\": 0.9013731656184487, \"Recall\": 0.9013731656184486, \"MSE\": 0.04079696394686907, \"MCC\": 0.7725442483679897}',30,45),(238,6,4,'{\"Precision\": 0.6020850040096231, \"Runtime\": 0.0180966059366862, \"ACC\": 0.6059816153461908, \"Recall\": 0.6059816153461908, \"MSE\": 0.14511494252873564, \"MCC\": 0.20803012885216612}',31,45),(239,6,4,'{\"Precision\": 0.735369989967993, \"Runtime\": 0.06334781646728516, \"ACC\": 0.7549223431576374, \"Recall\": 0.7549223431576374, \"MSE\": 0.09836829836829837, \"MCC\": 0.4899023141469343}',32,45),(240,6,4,'{\"Precision\": 0.7228260869565218, \"Runtime\": 0.0014400482177734375, \"ACC\": 0.781786941580756, \"Recall\": 0.781786941580756, \"MSE\": 0.13392857142857142, \"MCC\": 0.5011565884945975}',33,45),(241,6,4,'{\"Runtime\": 0.007478952407836914}',11,66),(242,6,4,'{\"Runtime\": 0.0022522608439127603}',12,66),(243,6,4,'{\"Runtime\": 0.01575891176859538}',13,66),(244,6,4,'{\"Runtime\": 0.009610255559285482}',1,21),(245,6,4,'{\"Runtime\": 0.01829051971435547}',2,21),(246,6,4,'{\"Runtime\": 0.08056990305582683}',1,21),(247,6,4,'{\"Runtime\": 50.636067390441895}',3,21),(248,6,4,'{\"Runtime\": 60.90866653124491}',4,21),(249,6,4,'{\"Runtime\": 52.49190219243368}',5,21),(250,6,4,'{\"Runtime\": 1142.6027266979218}',6,21),(251,5,3,'{\"Runtime\": \">9000\"}',6,88),(252,4,2,'{\"ComputingNeighbors\": 5791.593315000001, \"TreeBuilding\": 6.594479666666666, \"Runtime\": 5798.187794666667}',7,28),(253,3,1,'{\"TreeBuilding\": 1.8743726666666667, \"ComputingNeighbors\": 6213.166305, \"Runtime\": 6215.040677666667}',9,28),(254,3,1,'{\"TreeBuilding\": 2.5333333333333334e-05, \"ComputingNeighbors\": 0.000223, \"Runtime\": 0.0002483333333333333}',1,27),(255,3,1,'{\"TreeBuilding\": 0.0005459999999999999, \"ComputingNeighbors\": 0.005929666666666666, \"Runtime\": 0.006475666666666667}',2,27),(256,3,1,'{\"TreeBuilding\": 0.0019116666666666665, \"ComputingNeighbors\": 0.022518, \"Runtime\": 0.024429666666666666}',1,27),(257,3,1,'{\"TreeBuilding\": 0.13269999999999998, \"ComputingNeighbors\": 17.47527033333333, \"Runtime\": 17.60797033333333}',3,27),(258,3,1,'{\"TreeBuilding\": 0.17170766666666668, \"ComputingNeighbors\": 6.161877, \"Runtime\": 6.333584666666667}',4,27),(259,3,1,'{\"TreeBuilding\": 0.6523936666666666, \"ComputingNeighbors\": 3.9261133333333333, \"Runtime\": 4.578506999999999}',5,27),(260,6,4,'{\"Runtime\": \">9000\"}',7,21),(261,3,1,'{\"TreeBuilding\": 2.0967363333333338, \"ComputingNeighbors\": 456.519469, \"Runtime\": 458.6162053333333}',6,27),(262,4,2,'{\"ComputingNeighbors\": 1051.2090513333333, \"TreeBuilding\": 70.127927, \"Runtime\": 1121.336978333333}',8,28),(263,5,3,'{\"Runtime\": \">9000\"}',7,88),(264,5,3,'{\"Runtime\": \">9000\"}',8,88),(265,3,1,'{\"TreeBuilding\": 43.516319333333335, \"ComputingNeighbors\": 4346.080329333333, \"Runtime\": 4389.596648666667}',7,27),(266,6,4,'{\"Runtime\": 4578.865880012512}',8,21),(267,3,1,'{\"TreeBuilding\": 10.523032666666667, \"ComputingNeighbors\": 1093.714740333333, \"Runtime\": 1104.2377729999998}',8,27),(268,4,2,'{\"ComputingNeighbors\": 6211.172431666666, \"TreeBuilding\": 52.046100333333335, \"Runtime\": 6263.218531999999}',9,28),(269,4,2,'{\"ComputingNeighbors\": 0.0005836666666666667, \"TreeBuilding\": 0.000253, \"Runtime\": 0.0008366666666666667}',1,17),(270,4,2,'{\"ComputingNeighbors\": 0.011992666666666665, \"TreeBuilding\": 0.0033996666666666667, \"Runtime\": 0.015392333333333333}',2,17),(271,4,2,'{\"ComputingNeighbors\": 0.040496333333333336, \"TreeBuilding\": 0.010485, \"Runtime\": 0.05098133333333333}',1,17),(272,4,2,'{\"ComputingNeighbors\": 29.097133, \"TreeBuilding\": 0.689924, \"Runtime\": 29.787057}',3,17),(273,5,3,'{\"Runtime\": \">9000\"}',9,88),(274,4,2,'{\"ComputingNeighbors\": 8.045783333333333, \"TreeBuilding\": 0.6988376666666666, \"Runtime\": 8.744621}',4,17),(275,4,2,'{\"ComputingNeighbors\": 7.355651333333333, \"TreeBuilding\": 2.984059333333333, \"Runtime\": 10.339710666666667}',5,17),(276,4,2,'{\"ComputingNeighbors\": 371.6076933333334, \"TreeBuilding\": 4.024193333333334, \"Runtime\": 375.63188666666673}',6,17),(277,6,4,'{\"Runtime\": \">9000\"}',9,21),(278,6,4,'{\"LFT\": 2.1999999999999997, \"Precision\": 0.8933706816059757, \"Runtime\": 0.21965972582499185, \"ACC\": 0.88, \"Recall\": 0.88, \"MSE\": 0.12, \"MCC\": 0.82838215464777}',11,70),(279,6,4,'{\"LFT\": 0.8368034946108033, \"Precision\": 0.546951263932396, \"Runtime\": 0.8141485055287679, \"ACC\": 0.6946541830262761, \"Recall\": 0.6946541830262761, \"MSE\": 0.16666666666666666, \"MCC\": 0.19119895316461966}',24,70),(280,6,4,'{\"LFT\": 0.6818367171379962, \"Precision\": 0.5421459945618071, \"Runtime\": 8.213579098383585, \"ACC\": 0.5806664311786176, \"Recall\": 0.5806664311786177, \"MSE\": 0.1770573566084788, \"MCC\": 0.11661504139302807}',25,70),(281,6,4,'{\"LFT\": 0.8498615859182843, \"Precision\": 0.87333279105431, \"Runtime\": 177.61372033754984, \"ACC\": 0.8281623683359663, \"Recall\": 0.8281623683359665, \"MSE\": 0.01604416458207539, \"MCC\": 0.700039350007866}',26,70),(282,6,4,'{\"LFT\": 0.9960231365855222, \"Precision\": 0.9072224487022694, \"Runtime\": 54.65005008379618, \"ACC\": 0.9105621287138134, \"Recall\": 0.9105621287138131, \"MSE\": 0.028472489419007308, \"MCC\": 0.81777775807185}',3,70),(283,6,4,'{\"LFT\": 0.6974450586001787, \"Precision\": 0.7870372440061336, \"Runtime\": 4.148547967274983, \"ACC\": 0.6860330015790922, \"Recall\": 0.6860330015790922, \"MSE\": 0.022532188841201718, \"MCC\": 0.462161876694525}',27,70),(284,5,3,'{\"Runtime\": \">9000\"}',19,88),(285,5,3,'{\"Runtime\": 0.106043}',11,87),(286,5,3,'{\"Runtime\": 0.6014863333333333}',15,87),(287,5,3,'{\"Runtime\": 0.2928846666666667}',1,87),(288,5,3,'{\"Runtime\": 4.002859666666667}',16,87),(289,6,4,'{\"LFT\": 0.9353335128595414, \"Precision\": 0.9773738379814075, \"Runtime\": 1459.1059532960255, \"ACC\": 0.9177822976065085, \"Recall\": 0.9177822976065085, \"MSE\": 0.004300234558248631, \"MCC\": 0.8931703954993351}',28,70),(290,6,4,'{\"LFT\": 0.5, \"Precision\": 0.9971264367816092, \"Runtime\": 1.597768783569336, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.005747126436781609, \"MCC\": 0}',29,70),(291,6,4,'{\"LFT\": 0.5665032335203564, \"Precision\": 0.8326107899807322, \"Runtime\": 2.569661855697632, \"ACC\": 0.5579035639412998, \"Recall\": 0.5579035639412998, \"MSE\": 0.0872865275142315, \"MCC\": 0.27755612149772924}',30,70),(292,6,4,'{\"LFT\": 0.523401093458179, \"Precision\": 0.571353328456474, \"Runtime\": 1.8339544137318928, \"ACC\": 0.5140009301417604, \"Recall\": 0.5140009301417605, \"MSE\": 0.10919540229885057, \"MCC\": 0.06321433277671083}',31,70),(293,6,4,'{\"LFT\": 0.9113706498939109, \"Precision\": 0.5810097169412758, \"Runtime\": 5.356623490651448, \"ACC\": 0.6704628836981777, \"Recall\": 0.6704628836981779, \"MSE\": 0.24988344988344988, \"MCC\": 0.2350246791924883}',32,70),(294,6,4,'{\"LFT\": 1.1037495227185947, \"Precision\": 0.7843434343434343, \"Runtime\": 0.11419820785522461, \"ACC\": 0.8869415807560138, \"Recall\": 0.8869415807560137, \"MSE\": 0.09821428571428571, \"MCC\": 0.6633982151391192}',33,70),(295,6,4,'{\"Runtime\": 0.0370949904123942}',34,8),(296,6,4,'{\"Runtime\": 0.7193452517191569}',35,8),(297,6,4,'{\"Runtime\": 1.2120672067006428}',36,8),(298,6,4,'{\"Runtime\": \"failure\"}',37,8),(299,6,4,'{\"Runtime\": 96.86880207061768}',22,8),(300,6,4,'{\"Runtime\": \"failure\"}',21,8),(301,6,4,'{\"Runtime\": 0.1829667091369629}',18,8),(302,6,4,'{\"Runtime\": 13.169975678126017}',13,8),(303,6,4,'{\"Runtime\": 0.28887176513671875}',10,8),(304,6,4,'{\"Runtime\": 456.6322275797526}',3,8),(305,5,3,'{\"Runtime\": 668.8232846666666}',17,87),(306,6,4,'{\"Runtime\": \"failure\"}',5,8),(307,6,4,'{\"Runtime\": 0.006909529368082683}',34,16),(308,6,4,'{\"Runtime\": 0.7206800778706869}',35,16),(309,6,4,'{\"Runtime\": 1.202898343404134}',36,16),(310,6,4,'{\"Runtime\": \"failure\"}',37,16),(311,6,4,'{\"Runtime\": 96.85123785336812}',22,16),(312,6,4,'{\"Runtime\": \"failure\"}',21,16),(313,6,4,'{\"Runtime\": 0.16700243949890137}',18,16),(314,6,4,'{\"Runtime\": 13.747834205627441}',13,16),(315,6,4,'{\"Runtime\": 0.2563480536142985}',10,16),(316,6,4,'{\"Runtime\": 459.74583927790326}',3,16),(317,6,4,'{\"Runtime\": \"failure\"}',5,16),(318,6,4,'{\"Runtime\": 0.01195200284322103}',34,10),(319,6,4,'{\"Runtime\": 0.42904074986775714}',35,10),(320,6,4,'{\"Runtime\": 3.8433106740315757}',36,10),(321,6,4,'{\"Runtime\": \"failure\"}',37,10),(322,6,4,'{\"Runtime\": 98.03637480735779}',22,10),(323,6,4,'{\"Runtime\": \"failure\"}',21,10),(324,6,4,'{\"Runtime\": 0.15371100107828775}',18,10),(325,6,4,'{\"Runtime\": 8.744084199269613}',13,10),(326,6,4,'{\"Runtime\": 0.28585290908813477}',10,10),(327,6,4,'{\"Runtime\": 462.6307324568431}',3,10),(328,6,4,'{\"Runtime\": \"failure\"}',5,10),(329,6,4,'{\"Precision\": 0.980125383486728, \"Runtime\": 0.023879607518513996, \"ACC\": 0.98, \"Recall\": 0.98, \"MSE\": 0.02, \"MCC\": 0.9700708921921396}',11,1),(330,6,4,'{\"Precision\": 0.982371794871795, \"Runtime\": 0.022516171137491863, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.035256410256410256, \"MCC\": 0}',24,1),(331,6,4,'{\"Precision\": 0.9644638403990026, \"Runtime\": 0.8504151503245035, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.07107231920199501, \"MCC\": 0}',25,1),(332,6,4,'{\"Precision\": 0.5521285310361229, \"Runtime\": 12.625955581665039, \"ACC\": 0.8422494670350323, \"Recall\": 0.8422494670350323, \"MSE\": 0.22272060726300355, \"MCC\": 0.26714012775644336}',26,1),(333,6,4,'{\"Precision\": 0.9574836475567525, \"Runtime\": 11.73205010096232, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.0850327048864948, \"MCC\": 0}',3,1),(334,6,4,'{\"Precision\": 0.6521799846810743, \"Runtime\": 0.006667772928873698, \"ACC\": 0.812586379242905, \"Recall\": 0.8125863792429048, \"MSE\": 0.04560085836909871, \"MCC\": 0.43620816317303274}',27,1),(335,3,1,'{\"TreeBuilding\": 1.8793309999999999, \"ComputingNeighbors\": 6265.700435, \"Runtime\": 6267.579766}',9,27),(336,3,1,'{\"TreeBuilding\": 2.466666666666667e-05, \"ComputingNeighbors\": 0.00022266666666666667, \"Runtime\": 0.00024733333333333335}',1,29),(337,3,1,'{\"TreeBuilding\": 0.0005793333333333333, \"ComputingNeighbors\": 0.005655999999999999, \"Runtime\": 0.006235333333333333}',2,29),(338,3,1,'{\"TreeBuilding\": 0.001907, \"ComputingNeighbors\": 0.021398666666666667, \"Runtime\": 0.02330566666666667}',1,29),(339,3,1,'{\"TreeBuilding\": 0.13266, \"ComputingNeighbors\": 16.057141333333334, \"Runtime\": 16.189801333333335}',3,29),(340,3,1,'{\"TreeBuilding\": 0.17150166666666666, \"ComputingNeighbors\": 4.953849333333333, \"Runtime\": 5.125351000000001}',4,29),(341,3,1,'{\"TreeBuilding\": 0.6536473333333334, \"ComputingNeighbors\": 3.6790966666666667, \"Runtime\": 4.332744000000001}',5,29),(342,3,1,'{\"TreeBuilding\": 2.1049826666666664, \"ComputingNeighbors\": 347.7763726666667, \"Runtime\": 349.8813553333334}',6,29),(343,4,2,'{\"ComputingNeighbors\": 5823.9020986666665, \"TreeBuilding\": 6.599329333333333, \"Runtime\": 5830.5014280000005}',7,17),(344,4,2,'{\"ComputingNeighbors\": 1414.513763, \"TreeBuilding\": 70.14742166666667, \"Runtime\": 1484.6611846666667}',8,17),(345,6,4,'{\"Precision\": 0.5118432385874246, \"Runtime\": 2785.4767500559487, \"ACC\": 0.5471434278865361, \"Recall\": 0.5471434278865361, \"MSE\": 0.8862392494136043, \"MCC\": 0.04725805177064731}',28,1),(346,6,4,'{\"Precision\": 0.9971264367816092, \"Runtime\": 0.005207379659016927, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.005747126436781609, \"MCC\": 0}',29,1),(347,6,4,'{\"Precision\": 0.5647668393782384, \"Runtime\": 0.02580722173055013, \"ACC\": 0.6477987421383647, \"Recall\": 0.6477987421383647, \"MSE\": 0.6375711574952562, \"MCC\": 0.195677871946535}',30,1),(348,6,4,'{\"Precision\": 0.5927195345975375, \"Runtime\": 0.0053208669026692705, \"ACC\": 0.7396636561461126, \"Recall\": 0.7396636561461127, \"MSE\": 0.3074712643678161, \"MCC\": 0.29813756997608887}',31,1),(349,6,4,'{\"Precision\": 0.6371794871794871, \"Runtime\": 0.040352423985799156, \"ACC\": 0.7944233826586767, \"Recall\": 0.7944233826586768, \"MSE\": 0.2107226107226107, \"MCC\": 0.4019395409599171}',32,1),(350,6,4,'{\"Precision\": 0.6198695818948984, \"Runtime\": 0.0017984708150227864, \"ACC\": 0.7147766323024055, \"Recall\": 0.7147766323024053, \"MSE\": 0.25, \"MCC\": 0.32090612406050256}',33,1),(351,6,4,'{\"Precision\": 0.9667867146858743, \"Runtime\": 0.04799509048461914, \"ACC\": 0.9666666666666667, \"Recall\": 0.9666666666666667, \"MSE\": 0.03333333333333333, \"MCC\": 0.9500681414622171}',11,63),(352,6,4,'{\"Precision\": 0.48231511254019294, \"Runtime\": 0.00822154680887858, \"ACC\": 0.4983388704318938, \"Recall\": 0.4983388704318937, \"MSE\": 0.038461538461538464, \"MCC\": -0.010840090307394688}',24,63),(353,6,4,'{\"Precision\": 0.8011769651113912, \"Runtime\": 0.868245522181193, \"ACC\": 0.5506181561285765, \"Recall\": 0.5506181561285765, \"MSE\": 0.06733167082294264, \"MCC\": 0.24694147195106161}',25,63),(354,6,4,'{\"Precision\": 0.9533154422726966, \"Runtime\": 182.0512326558431, \"ACC\": 0.7434608944348206, \"Recall\": 0.7434608944348206, \"MSE\": 0.016130423531441385, \"MCC\": 0.6644233079500657}',26,63),(355,6,4,'{\"Precision\": 0.9352344376934539, \"Runtime\": 21.715215524037678, \"ACC\": 0.9334472483436023, \"Recall\": 0.9334472483436022, \"MSE\": 0.020392458637937667, \"MCC\": 0.8686798475908203}',3,63),(356,6,4,'{\"Precision\": 0.8906862426644145, \"Runtime\": 0.12206689516703288, \"ACC\": 0.7613687397684964, \"Recall\": 0.7613687397684964, \"MSE\": 0.01555793991416309, \"MCC\": 0.6391030304734503}',27,63),(357,3,1,'{\"TreeBuilding\": 43.53413466666667, \"ComputingNeighbors\": 4315.942640666667, \"Runtime\": 4359.4767753333335}',7,29),(358,6,4,'{\"Precision\": 0.9917485265225934, \"Runtime\": 458.31860605875653, \"ACC\": 0.6181818181818182, \"Recall\": 0.6181818181818182, \"MSE\": 0.01641907740422205, \"MCC\": 0.48214410689199605}',28,63),(359,6,4,'{\"Precision\": 0.9971264367816092, \"Runtime\": 0.030348857243855793, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.005747126436781609, \"MCC\": 0}',29,63),(360,6,4,'{\"Precision\": 0.6809648548351888, \"Runtime\": 0.05743861198425293, \"ACC\": 0.569517819706499, \"Recall\": 0.569517819706499, \"MSE\": 0.09867172675521822, \"MCC\": 0.22432371387479666}',30,63),(361,6,4,'{\"Precision\": 0.6108427267847557, \"Runtime\": 0.028918902079264324, \"ACC\": 0.567581688838873, \"Recall\": 0.567581688838873, \"MSE\": 0.12212643678160921, \"MCC\": 0.17310041792693143}',31,63),(362,6,4,'{\"Precision\": 0.8547746764857411, \"Runtime\": 0.36417357126871747, \"ACC\": 0.8303778892014186, \"Recall\": 0.8303778892014186, \"MSE\": 0.05641025641025641, \"MCC\": 0.6847180697468859}',32,63),(363,6,4,'{\"Precision\": 0.8158508158508159, \"Runtime\": 0.0028733412424723306, \"ACC\": 0.7793814432989693, \"Recall\": 0.7793814432989691, \"MSE\": 0.08928571428571429, \"MCC\": 0.5941139850215877}',33,63),(364,6,4,'{\"Simple MSE\": 0.13392857142857142, \"Runtime\": 0.02641161282857259}',33,71),(365,6,4,'{\"Simple MSE\": 0.022800429184549355, \"Runtime\": 0.003057718276977539}',27,71),(366,6,4,'{\"Simple MSE\": 0.094876660341556, \"Runtime\": 0.00600584348042806}',30,73),(367,6,4,'{\"Simple MSE\": 0.012766324506167515, \"Runtime\": 1.9956865310668945}',26,73),(368,6,4,'{\"Simple MSE\": 0.09985632183908046, \"Runtime\": 0.002474069595336914}',31,72),(369,6,4,'{\"Simple MSE\": 0.005747126436781609, \"Runtime\": 0.0024623870849609375}',29,72),(370,6,4,'{\"Precision\": 0.9604700854700855, \"Runtime\": 0.1559281349182129, \"ACC\": 0.96, \"Recall\": 0.96, \"MSE\": 0.04, \"MCC\": 0.9402672577997083}',11,43),(371,6,4,'{\"Precision\": 0.888599348534202, \"Runtime\": 0.23753086725870767, \"ACC\": 0.6801570522500754, \"Recall\": 0.6801570522500754, \"MSE\": 0.025641025641025644, \"MCC\": 0.5291839496365004}',24,43),(372,6,4,'{\"Precision\": 0.597405652451524, \"Runtime\": 3.7435665130615234, \"ACC\": 0.568256210997292, \"Recall\": 0.568256210997292, \"MSE\": 0.09476309226932668, \"MCC\": 0.16307716904656033}',25,43),(373,6,4,'{\"Precision\": 0.9207593016484704, \"Runtime\": 7.619949022928874, \"ACC\": 0.7693328934674661, \"Recall\": 0.7693328934674661, \"MSE\": 0.01604416458207539, \"MCC\": 0.6732735555963355}',26,43),(374,6,4,'{\"Precision\": 0.9046014062049911, \"Runtime\": 21.534658829371136, \"ACC\": 0.8795187027389076, \"Recall\": 0.8795187027389075, \"MSE\": 0.03270488649480569, \"MCC\": 0.7837188291964307}',3,43),(375,6,4,'{\"Precision\": 0.8385737954703472, \"Runtime\": 0.4238940080006917, \"ACC\": 0.7652556246106595, \"Recall\": 0.7652556246106595, \"MSE\": 0.017972103004291844, \"MCC\": 0.5993616724292224}',27,43),(376,3,1,'{\"TreeBuilding\": 10.488896, \"ComputingNeighbors\": 891.9860503333333, \"Runtime\": 902.4749463333334}',8,29),(377,6,4,'{\"Precision\": 0.99741653418124, \"Runtime\": 138.80278452237448, \"ACC\": 0.8818181818181818, \"Recall\": 0.8818181818181818, \"MSE\": 0.0050820953870211105, \"MCC\": 0.8716023788113075}',28,43),(378,6,4,'{\"Precision\": 0.622478386167147, \"Runtime\": 0.2550033728281657, \"ACC\": 0.5614161849710982, \"Recall\": 0.5614161849710982, \"MSE\": 0.007183908045977012, \"MCC\": 0.173460718548069}',29,43),(379,6,4,'{\"Precision\": 0.9019718865905477, \"Runtime\": 0.230574369430542, \"ACC\": 0.905566037735849, \"Recall\": 0.905566037735849, \"MSE\": 0.03320683111954459, \"MCC\": 0.8075299259488347}',30,43),(380,6,4,'{\"Precision\": 0.6172438672438673, \"Runtime\": 0.2556811173756917, \"ACC\": 0.5055980754103819, \"Recall\": 0.5055980754103819, \"MSE\": 0.10129310344827586, \"MCC\": 0.05123826734915941}',31,43),(381,6,4,'{\"Precision\": 0.8020908540728785, \"Runtime\": 0.636100689570109, \"ACC\": 0.767656230891525, \"Recall\": 0.767656230891525, \"MSE\": 0.07552447552447553, \"MCC\": 0.5687055455609639}',32,43),(382,6,4,'{\"Precision\": 0.7135416666666666, \"Runtime\": 0.10550713539123535, \"ACC\": 0.7254295532646049, \"Recall\": 0.7254295532646048, \"MSE\": 0.13392857142857142, \"MCC\": 0.438810221029756}',33,43),(383,6,4,'{\"Runtime\": 0.03250932693481445}',15,108),(384,6,4,'{\"Runtime\": 0.5293389956156412}',20,108),(385,6,4,'{\"Runtime\": 0.7529606819152832}',18,108),(386,6,4,'{\"Runtime\": 0.9276309013366699}',13,108),(387,6,4,'{\"Runtime\": 0.14549358685811362}',11,30),(388,6,4,'{\"Runtime\": 0.4441870053609212}',24,30),(389,6,4,'{\"Runtime\": 3.5040279229482016}',25,30),(390,6,4,'{\"Runtime\": 306.6181705792745}',26,30),(391,6,4,'{\"Runtime\": 33.42839248975118}',3,30),(392,6,4,'{\"Runtime\": 9.625614722569784}',27,30),(393,5,3,'{\"Runtime\": 8293.487322666666}',13,87),(394,6,4,'{\"Runtime\": 608.6259069442749}',28,30),(395,6,4,'{\"Runtime\": 1.4280255635579426}',29,30),(396,6,4,'{\"Runtime\": 1.7644599278767903}',30,30),(397,6,4,'{\"Runtime\": 1.4143236478169758}',31,30),(398,6,4,'{\"Runtime\": 8.808982133865356}',32,30),(399,6,4,'{\"Runtime\": 0.15330799420674643}',33,30),(400,6,4,'{\"Precision\": 0.980125383486728, \"Runtime\": 0.013940811157226562, \"ACC\": 0.98, \"Recall\": 0.98, \"MSE\": 0.02, \"MCC\": 0.9700708921921396}',11,2),(401,6,4,'{\"Precision\": 0.802675585284281, \"Runtime\": 0.011635859807332357, \"ACC\": 0.8553307157958322, \"Recall\": 0.8553307157958322, \"MSE\": 0.025641025641025644, \"MCC\": 0.6558961270597227}',24,2),(402,6,4,'{\"Precision\": 0.6296272555366815, \"Runtime\": 0.7382191022237142, \"ACC\": 0.5952431414105734, \"Recall\": 0.5952431414105734, \"MSE\": 0.08977556109725686, \"MCC\": 0.22222607434542593}',25,2),(403,6,4,'{\"Precision\": 0.873684252396246, \"Runtime\": 9.92635210355123, \"ACC\": 0.8457361488253742, \"Recall\": 0.8457361488253742, \"MSE\": 0.015354092987147417, \"MCC\": 0.7188773311217073}',26,2),(404,6,4,'{\"Precision\": 0.9085099459981962, \"Runtime\": 9.721620241800943, \"ACC\": 0.9085099459981961, \"Recall\": 0.9085099459981962, \"MSE\": 0.028472489419007308, \"MCC\": 0.8170198919963921}',3,2),(405,6,4,'{\"Precision\": 0.7432618261826183, \"Runtime\": 0.007847785949707031, \"ACC\": 0.735774407116056, \"Recall\": 0.735774407116056, \"MSE\": 0.025482832618025753, \"MCC\": 0.47897771489778473}',27,2),(406,4,2,'{\"ComputingNeighbors\": 6234.517055666666, \"TreeBuilding\": 52.074006, \"Runtime\": 6286.591061666666}',9,17),(407,4,2,'{\"ComputingNeighbors\": 0.0005693333333333333, \"TreeBuilding\": 0.0002563333333333333, \"Runtime\": 0.0008256666666666667}',1,27),(408,4,2,'{\"ComputingNeighbors\": 0.011566333333333333, \"TreeBuilding\": 0.003546333333333333, \"Runtime\": 0.015112666666666668}',2,27),(409,4,2,'{\"ComputingNeighbors\": 0.03913566666666667, \"TreeBuilding\": 0.010513333333333333, \"Runtime\": 0.049649}',1,27),(410,4,2,'{\"ComputingNeighbors\": 28.050000666666666, \"TreeBuilding\": 0.6892073333333334, \"Runtime\": 28.739208}',3,27),(411,4,2,'{\"ComputingNeighbors\": 7.078430999999999, \"TreeBuilding\": 0.6987003333333334, \"Runtime\": 7.777131333333333}',4,27),(412,4,2,'{\"ComputingNeighbors\": 7.142470333333333, \"TreeBuilding\": 2.986333666666667, \"Runtime\": 10.128804}',5,27),(413,4,2,'{\"ComputingNeighbors\": 317.330504, \"TreeBuilding\": 4.019855333333333, \"Runtime\": 321.3503593333333}',6,27),(414,5,3,'{\"Runtime\": \">9000\"}',18,87),(415,6,4,'{\"Precision\": 0.5097700275672572, \"Runtime\": 2601.328735510508, \"ACC\": 0.6022300512112737, \"Recall\": 0.6022300512112737, \"MSE\": 0.32603596559812353, \"MCC\": 0.06320729130523672}',28,2),(416,6,4,'{\"Precision\": 0.49709724238026126, \"Runtime\": 0.005685249964396159, \"ACC\": 0.4949421965317919, \"Recall\": 0.4949421965317919, \"MSE\": 0.015804597701149427, \"MCC\": -0.007663309352096424}',29,2),(417,6,4,'{\"Precision\": 0.7554004832485846, \"Runtime\": 0.02232678731282552, \"ACC\": 0.5742348008385744, \"Recall\": 0.5742348008385744, \"MSE\": 0.09013282732447818, \"MCC\": 0.27538775577744434}',30,2),(418,6,4,'{\"Precision\": 0.9500718390804598, \"Runtime\": 0.005922794342041016, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.09985632183908046, \"MCC\": 0}',31,2),(419,6,4,'{\"Precision\": 0.9486940298507461, \"Runtime\": 0.03466216723124186, \"ACC\": 0.502262443438914, \"Recall\": 0.502262443438914, \"MSE\": 0.10256410256410257, \"MCC\": 0.0637226761495691}',32,2),(420,6,4,'{\"Precision\": 0.7908163265306123, \"Runtime\": 0.002538760503133138, \"ACC\": 0.7742268041237114, \"Recall\": 0.7742268041237113, \"MSE\": 0.09821428571428571, \"MCC\": 0.56479954607449}',33,2),(421,6,4,'{\"Precision\": 0.9871794871794872, \"Runtime\": 0.02571733792622884, \"ACC\": 0.9866666666666667, \"Recall\": 0.9866666666666667, \"MSE\": 0.013333333333333334, \"MCC\": 0.9802892811798277}',11,83),(422,6,4,'{\"Precision\": 0.982371794871795, \"Runtime\": 0.11783075332641602, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.035256410256410256, \"MCC\": 0}',24,83),(423,6,4,'{\"Precision\": 0.9644638403990026, \"Runtime\": 0.4630485375722249, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.07107231920199501, \"MCC\": 0}',25,83),(424,6,4,'{\"Precision\": 0.9863031455236778, \"Runtime\": 24.59723385175069, \"ACC\": 0.5310650887573964, \"Recall\": 0.5310650887573964, \"MSE\": 0.027344086949020963, \"MCC\": 0.24582148302127002}',26,83),(425,6,4,'{\"Precision\": 0.9677941077295106, \"Runtime\": 5.316251834233602, \"ACC\": 0.8695688989188223, \"Recall\": 0.8695688989188223, \"MSE\": 0.024624855713736054, \"MCC\": 0.8315819943079771}',3,83),(426,6,4,'{\"Precision\": 0.9026590538336053, \"Runtime\": 0.32536133130391437, \"ACC\": 0.7145508279368943, \"Recall\": 0.7145508279368943, \"MSE\": 0.01689914163090129, \"MCC\": 0.5878463519535916}',27,83),(427,6,4,'{\"Precision\": 0.9892494136043783, \"Runtime\": 61.55431834856669, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.021501172791243157, \"MCC\": 0}',28,83),(428,6,4,'{\"Precision\": 0.9971264367816092, \"Runtime\": 0.03881994883219401, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.005747126436781609, \"MCC\": 0}',29,83),(429,6,4,'{\"Precision\": 0.8734804862444019, \"Runtime\": 0.982433001200358, \"ACC\": 0.5489517819706499, \"Recall\": 0.5489517819706499, \"MSE\": 0.0872865275142315, \"MCC\": 0.2704258518184107}',30,83),(430,6,4,'{\"Precision\": 0.9500718390804598, \"Runtime\": 0.16046770413716635, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.09985632183908046, \"MCC\": 0}',31,83),(431,6,4,'{\"Precision\": 0.9484848484848486, \"Runtime\": 5.16009505589803, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.10303030303030303, \"MCC\": 0}',32,83),(432,6,4,'{\"Precision\": 0.9330357142857143, \"Runtime\": 0.002581914265950521, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.13392857142857142, \"MCC\": 0}',33,83),(433,6,4,'{}',11,44),(434,6,4,'{}',24,44),(435,6,4,'{}',25,44),(436,6,4,'{}',26,44),(437,6,4,'{}',3,44),(438,6,4,'{}',27,44),(439,6,4,'{}',28,44),(440,6,4,'{}',29,44),(441,6,4,'{}',30,44),(442,6,4,'{}',31,44),(443,6,4,'{}',32,44),(444,6,4,'{}',33,44),(445,6,4,'{\"Runtime\": \"failure\"}',16,36),(446,6,4,'{\"Runtime\": 0.43733032544453937}',38,36),(447,6,4,'{\"Runtime\": 2.8165144125620523}',39,36),(448,6,4,'{\"Runtime\": \"failure\"}',22,36),(449,3,1,'{\"TreeBuilding\": 1.8840686666666666, \"ComputingNeighbors\": 6233.3307233333335, \"Runtime\": 6235.214792}',9,29),(450,6,4,'{\"Runtime\": 685.8402881622314}',21,36),(451,6,4,'{\"Runtime\": 8.636096318562826}',10,36),(452,6,4,'{\"Runtime\": \"failure\"}',3,36),(453,5,3,'{\"Runtime\": \">9000\"}',13,87),(454,7,5,'{\"Runtime\": \"failure\"}',11,90),(455,7,5,'{\"Runtime\": \"failure\"}',1,90),(456,7,5,'{\"Runtime\": \"failure\"}',40,90),(457,7,5,'{\"Runtime\": \"failure\"}',15,90),(458,7,5,'{\"Runtime\": \"failure\"}',14,90),(459,7,5,'{\"Runtime\": \"failure\"}',16,90),(460,7,5,'{}',37,90),(461,7,5,'{\"Runtime\": \"failure\"}',41,90),(462,7,5,'{\"Runtime\": \"failure\"}',17,90),(463,7,5,'{\"Runtime\": \"failure\"}',42,90),(464,7,5,'{\"Runtime\": \"failure\"}',18,90),(465,7,5,'{\"Runtime\": \"failure\"}',13,90),(466,7,5,'{\"Runtime\": \"failure\"}',4,90),(467,7,5,'{\"Runtime\": \"failure\"}',3,90),(468,7,5,'{\"Runtime\": \"failure\"}',5,90),(469,7,5,'{\"Runtime\": \"failure\"}',6,90),(470,7,5,'{\"Runtime\": \"failure\"}',7,90),(471,7,5,'{\"Runtime\": \"failure\"}',8,90),(472,7,5,'{\"Runtime\": \"failure\"}',9,90),(473,7,5,'{\"Runtime\": \"failure\"}',19,90),(474,7,5,'{\"Runtime\": \"failure\"}',11,91),(475,7,5,'{\"Runtime\": \"failure\"}',1,91),(476,7,5,'{\"Runtime\": \"failure\"}',40,91),(477,7,5,'{\"Runtime\": \"failure\"}',15,91),(478,7,5,'{\"Runtime\": \"failure\"}',14,91),(479,7,5,'{\"Runtime\": \"failure\"}',16,91),(480,7,5,'{}',37,91),(481,7,5,'{\"Runtime\": \"failure\"}',41,91),(482,7,5,'{\"Runtime\": \"failure\"}',17,91),(483,7,5,'{\"Runtime\": \"failure\"}',42,91),(484,7,5,'{\"Runtime\": \"failure\"}',18,91),(485,7,5,'{\"Runtime\": \"failure\"}',13,91),(486,7,5,'{\"Runtime\": \"failure\"}',4,91),(487,7,5,'{\"Runtime\": \"failure\"}',3,91),(488,7,5,'{\"Runtime\": \"failure\"}',5,91),(489,7,5,'{\"Runtime\": \"failure\"}',6,91),(490,7,5,'{\"Runtime\": \"failure\"}',7,91),(491,7,5,'{\"Runtime\": \"failure\"}',8,91),(492,7,5,'{\"Runtime\": \"failure\"}',9,91),(493,7,5,'{\"Runtime\": \"failure\"}',19,91),(494,7,5,'{\"Runtime\": \"failure\"}',11,92),(495,7,5,'{\"Runtime\": \"failure\"}',1,92),(496,7,5,'{\"Runtime\": \"failure\"}',40,92),(497,7,5,'{\"Runtime\": \"failure\"}',15,92),(498,7,5,'{\"Runtime\": \"failure\"}',14,92),(499,7,5,'{\"Runtime\": \"failure\"}',16,92),(500,7,5,'{}',37,92),(501,7,5,'{\"Runtime\": \"failure\"}',41,92),(502,7,5,'{\"Runtime\": \"failure\"}',17,92),(503,7,5,'{\"Runtime\": \"failure\"}',42,92),(504,7,5,'{\"Runtime\": \"failure\"}',18,92),(505,7,5,'{\"Runtime\": \"failure\"}',13,92),(506,7,5,'{\"Runtime\": \"failure\"}',4,92),(507,7,5,'{\"Runtime\": \"failure\"}',3,92),(508,7,5,'{\"Runtime\": \"failure\"}',5,92),(509,7,5,'{\"Runtime\": \"failure\"}',6,92),(510,7,5,'{\"Runtime\": \"failure\"}',7,92),(511,7,5,'{\"Runtime\": \"failure\"}',8,92),(512,7,5,'{\"Runtime\": \"failure\"}',9,92),(513,7,5,'{\"Runtime\": \"failure\"}',19,92),(514,7,5,'{\"Runtime\": \"failure\"}',15,62),(515,7,5,'{\"Runtime\": \"failure\"}',20,62),(516,7,5,'{\"Runtime\": \"failure\"}',7,62),(517,7,5,'{\"Runtime\": \"failure\"}',9,62),(518,7,5,'{\"Runtime\": \"failure\"}',43,62),(519,7,5,'{\"Runtime\": \"failure\"}',21,62),(520,7,5,'{\"Runtime\": 10.344603666666666}',13,62),(521,7,5,'{\"Runtime\": 3.155120333333333}',18,62),(522,7,5,'{\"Runtime\": 0.06995799999999999}',11,66),(523,7,5,'{\"Runtime\": 0.06710633333333334}',12,66),(524,7,5,'{\"Runtime\": 1.3823416666666668}',13,66),(525,7,5,'{\"Runtime\": 0.04464366666666667}',1,21),(526,7,5,'{\"Runtime\": 0.112254}',2,21),(527,7,5,'{\"Runtime\": 0.9094723333333333}',1,21),(528,7,5,'{\"Runtime\": 258.04335066666664}',3,21),(529,7,5,'{\"Runtime\": 29.382852}',4,21),(530,7,5,'{\"Runtime\": 42.505024999999996}',5,21),(531,6,4,'{\"Runtime\": 1656.0708723862965}',7,36),(532,4,2,'{\"ComputingNeighbors\": 5801.060650666666, \"TreeBuilding\": 6.595484, \"Runtime\": 5807.656134666667}',7,27),(533,7,5,'{\"Runtime\": 1682.5862763333334}',6,21),(534,7,5,'{\"Runtime\": \"failure\"}',7,21),(535,6,4,'{\"Runtime\": 1548.3043044408162}',9,36),(536,5,3,'{\"Runtime\": \">9000\"}',4,87),(537,6,4,'{\"Runtime\": \"failure\"}',19,36),(538,6,4,'{\"Precision\": 1, \"Runtime\": 0.29114532470703125, \"ACC\": 1, \"Recall\": 1, \"MSE\": 0, \"MCC\": 1}',11,76),(539,6,4,'{\"Precision\": 0.9870550161812298, \"Runtime\": 0.2731734911600749, \"ACC\": 0.6363636363636364, \"Recall\": 0.6363636363636364, \"MSE\": 0.025641025641025644, \"MCC\": 0.5154283388236321}',24,76),(540,6,4,'{\"Precision\": 0.9667919799498748, \"Runtime\": 0.775601307551066, \"ACC\": 0.5350877192982456, \"Recall\": 0.5350877192982456, \"MSE\": 0.06608478802992519, \"MCC\": 0.25595832444484784}',25,76),(541,6,4,'{\"Precision\": 0.9002441491146227, \"Runtime\": 5.2777799765268965, \"ACC\": 0.8437104876465161, \"Recall\": 0.8437104876465161, \"MSE\": 0.013715172949193478, \"MCC\": 0.7418035094817276}',26,76),(542,6,4,'{\"Precision\": 0.9348540432712342, \"Runtime\": 2.0201263427734375, \"ACC\": 0.7422412841697459, \"Recall\": 0.742241284169746, \"MSE\": 0.04732589457483647, \"MCC\": 0.6491212579123568}',3,76),(543,6,4,'{\"Precision\": 0.9396093799672846, \"Runtime\": 0.4077994028727214, \"ACC\": 0.7203644950526606, \"Recall\": 0.7203644950526605, \"MSE\": 0.01555793991416309, \"MCC\": 0.622492727786932}',27,76),(544,6,4,'{\"Precision\": 0.9915553809897878, \"Runtime\": 9.200395345687866, \"ACC\": 0.609090909090909, \"Recall\": 0.6090909090909091, \"MSE\": 0.016810007818608287, \"MCC\": 0.4631380933618143}',28,76),(545,6,4,'{\"Precision\": 0.9971264367816092, \"Runtime\": 0.2732272942860921, \"ACC\": 0.5, \"Recall\": 0.5, \"MSE\": 0.005747126436781609, \"MCC\": 0}',29,76),(546,6,4,'{\"Precision\": 0.9412036561569272, \"Runtime\": 0.2739381790161133, \"ACC\": 0.9052830188679245, \"Recall\": 0.9052830188679245, \"MSE\": 0.025616698292220113, \"MCC\": 0.8457241860153822}',30,76),(547,6,4,'{\"Precision\": 0.6464835977031099, \"Runtime\": 0.37403066953023273, \"ACC\": 0.5443798193687668, \"Recall\": 0.5443798193687668, \"MSE\": 0.10632183908045977, \"MCC\": 0.16125651126762125}',31,76),(548,6,4,'{\"Precision\": 0.9167722772277228, \"Runtime\": 0.47480010986328125, \"ACC\": 0.7474929680812034, \"Recall\": 0.7474929680812034, \"MSE\": 0.05780885780885781, \"MCC\": 0.6423338941860416}',32,76),(549,6,4,'{\"Precision\": 0.8450000000000001, \"Runtime\": 0.2731531461079915, \"ACC\": 0.7845360824742268, \"Recall\": 0.7845360824742268, \"MSE\": 0.08035714285714286, \"MCC\": 0.6266257206773698}',33,76),(550,6,4,'{\"Runtime\": 0.012652873992919922}',15,62),(551,6,4,'{\"Runtime\": 0.08069292704264323}',20,62),(552,6,4,'{\"Runtime\": 69.3438491821289}',7,62),(553,6,4,'{\"Runtime\": 25.272319793701172}',9,62),(554,6,4,'{\"Runtime\": 0.08448584874471028}',43,62),(555,6,4,'{\"Runtime\": 0.5557066599527994}',21,62),(556,6,4,'{\"Runtime\": 1.0075631141662598}',13,62),(557,6,4,'{\"Runtime\": 0.2646694978078206}',18,62),(558,6,4,'{\"Runtime\": 0.0016466776529947917}',11,90),(559,6,4,'{\"Runtime\": 0.001967271169026693}',1,90),(560,6,4,'{\"Runtime\": 0.001887957255045573}',40,90),(561,6,4,'{\"Runtime\": 0.002086639404296875}',15,90),(562,6,4,'{\"Runtime\": 0.002714077631632487}',14,90),(563,6,4,'{\"Runtime\": 0.0051661332448323565}',16,90),(564,6,4,'{\"Runtime\": \"failure\"}',37,90),(565,6,4,'{\"Runtime\": 0.03139146169026693}',41,90),(566,6,4,'{\"Runtime\": 0.045116821924845375}',17,90),(567,6,4,'{\"Runtime\": 1.1643507480621338}',42,90),(568,6,4,'{\"Runtime\": 0.67329208056132}',18,90),(569,6,4,'{\"Runtime\": 3.4536897341410318}',13,90),(570,6,4,'{\"Runtime\": 0.5029387474060059}',4,90),(571,6,4,'{\"Runtime\": 17.844257752100628}',3,90),(572,6,4,'{\"Runtime\": 12.718710025151571}',5,90),(573,6,4,'{\"Runtime\": 1.2205666700998943}',6,90),(574,4,2,'{\"ComputingNeighbors\": 1245.187909, \"TreeBuilding\": 70.104243, \"Runtime\": 1315.292152}',8,27),(575,6,4,'{\"Runtime\": 261.298176129659}',7,90),(576,6,4,'{\"Runtime\": 25.901269833246868}',8,90),(577,6,4,'{\"Runtime\": 90.18859060605367}',9,90),(578,6,4,'{\"Runtime\": 29.67395536104838}',19,90),(579,6,4,'{\"Runtime\": 0.0016583601633707683}',11,92),(580,6,4,'{\"Runtime\": 0.001992305119832357}',1,92),(581,6,4,'{\"Runtime\": 0.0019048055013020833}',40,92),(582,6,4,'{\"Runtime\": 0.002084493637084961}',15,92),(583,6,4,'{\"Runtime\": 0.005808273951212565}',14,92),(584,6,4,'{\"Runtime\": 0.004799048105875651}',16,92),(585,6,4,'{\"Runtime\": \"failure\"}',37,92),(586,6,4,'{\"Runtime\": 0.07238316535949707}',41,92),(587,6,4,'{\"Runtime\": 0.17566466331481934}',17,92),(588,6,4,'{\"Runtime\": 0.43882107734680176}',42,92),(589,6,4,'{\"Runtime\": 0.3229804039001465}',18,92),(590,6,4,'{\"Runtime\": 1.024114688237508}',13,92),(591,6,4,'{\"Runtime\": 2.167049249013265}',4,92),(592,6,4,'{\"Runtime\": 5.275063594182332}',3,92),(593,6,4,'{\"Runtime\": 29.355206330617268}',5,92),(594,6,4,'{\"Runtime\": 6.04087766011556}',6,92),(595,6,4,'{\"Runtime\": 65.26228308677673}',7,92),(596,6,4,'{\"Runtime\": 42.52073486646017}',8,92),(597,6,4,'{\"Runtime\": 35.82994190851847}',9,92),(598,6,4,'{\"Runtime\": 43.550236304601036}',19,92),(599,6,4,'{\"Runtime\": 0.0016954739888509114}',11,91),(600,6,4,'{\"Runtime\": 0.0020074844360351562}',1,91),(601,6,4,'{\"Runtime\": 0.002012968063354492}',40,91),(602,6,4,'{\"Runtime\": 0.0021137396494547525}',15,91),(603,6,4,'{\"Runtime\": 0.0057888031005859375}',14,91),(604,6,4,'{\"Runtime\": 0.004865487416585286}',16,91),(605,6,4,'{\"Runtime\": \"failure\"}',37,91),(606,6,4,'{\"Runtime\": 0.07222716013590495}',41,91),(607,6,4,'{\"Runtime\": 0.17611773808797201}',17,91),(608,6,4,'{\"Runtime\": 0.4368216196695964}',42,91),(609,6,4,'{\"Runtime\": 0.3234575589497884}',18,91),(610,6,4,'{\"Runtime\": 1.0233982404073079}',13,91),(611,6,4,'{\"Runtime\": 2.1662460962931314}',4,91),(612,6,4,'{\"Runtime\": 5.274340709050496}',3,91),(613,6,4,'{\"Runtime\": 29.365867614746094}',5,91),(614,6,4,'{\"Runtime\": 6.062958796819051}',6,91),(615,6,4,'{\"Runtime\": 65.17757805188496}',7,91),(616,6,4,'{\"Runtime\": 42.53325208028158}',8,91),(617,6,4,'{\"Runtime\": 35.80354277292887}',9,91),(618,6,4,'{\"Runtime\": 43.56230386098226}',19,91),(619,6,4,'{\"Runtime\": \"failure\"}',14,93),(620,6,4,'{\"Runtime\": \"failure\"}',14,93),(621,6,4,'{\"Runtime\": \"failure\"}',11,93),(622,6,4,'{\"Runtime\": \"failure\"}',1,93),(623,6,4,'{\"Runtime\": \"failure\"}',44,93),(624,6,4,'{\"Runtime\": \"failure\"}',23,93),(625,6,4,'{\"Runtime\": \"failure\"}',39,93),(626,6,4,'{\"Runtime\": \"failure\"}',3,93),(627,6,4,'{\"Runtime\": \"failure\"}',21,93),(628,6,4,'{\"Runtime\": \"failure\"}',5,93),(629,7,5,'{\"Runtime\": \">9000\"}',8,21),(630,7,5,'{\"Runtime\": \"failure\"}',9,21),(631,7,5,'{}',6,54),(632,7,5,'{}',7,55),(633,7,5,'{}',4,55),(634,7,5,'{}',23,56),(635,7,5,'{}',22,51),(636,7,5,'{}',2,50),(637,7,5,'{}',5,52),(638,7,5,'{}',1,52),(639,7,5,'{}',3,49),(640,7,5,'{}',1,53),(641,7,5,'{}',11,53),(642,5,3,'{\"Runtime\": \">9000\"}',3,87),(643,5,3,'{\"Runtime\": \">9000\"}',5,87),(644,4,2,'{\"ComputingNeighbors\": 6226.402686999999, \"TreeBuilding\": 52.04682666666667, \"Runtime\": 6278.4495136666665}',9,27),(645,5,3,'{\"Runtime\": \">9000\"}',6,87),(646,5,3,'{\"Runtime\": \">9000\"}',7,87),(647,9,6,'{\"Runtime\": 0.05720329284667969}',15,40),(648,9,6,'{\"Runtime\": 0.00797891616821289}',20,40),(649,5,3,'{\"Runtime\": \">9000\"}',8,87),(650,9,6,'{\"Runtime\": 254.6057247320811}',21,40),(651,9,6,'{\"Runtime\": 0.019923686981201172}',11,45),(652,9,6,'{\"Runtime\": 0.020128488540649414}',24,45),(653,9,6,'{\"Runtime\": 0.8706013361612955}',25,45),(654,9,6,'{\"Runtime\": 18.9746306737264}',26,45),(655,9,6,'{\"Runtime\": 3.594217856725057}',3,45),(656,9,6,'{\"Runtime\": 0.0668042500813802}',27,45),(657,9,6,'{\"Runtime\": 163.86692762374878}',28,45),(658,9,6,'{\"Runtime\": 0.028165658315022785}',29,45),(659,9,6,'{\"Runtime\": 0.026443640391031902}',30,45),(660,9,6,'{\"Runtime\": 0.03261502583821615}',31,45),(661,9,6,'{\"Runtime\": 0.09939901034037273}',32,45),(662,9,6,'{\"Runtime\": 0.0012125968933105469}',33,45),(663,9,6,'{\"Runtime\": 0.0966934363047282}',11,90),(664,9,6,'{\"Runtime\": 0.010482072830200195}',1,90),(665,9,6,'{\"Runtime\": 0.0029848416646321616}',40,90),(666,9,6,'{\"Runtime\": 0.0008513132731119791}',15,90),(667,9,6,'{\"Runtime\": \"failure\"}',14,90),(668,9,6,'{\"Runtime\": 0.007321755091349284}',16,90),(669,9,6,'{\"Runtime\": 0.0035888353983561196}',37,90),(670,9,6,'{\"Runtime\": 0.008373181025187174}',41,90),(671,9,6,'{\"Runtime\": 0.015521049499511719}',17,90),(672,9,6,'{\"Runtime\": 0.1733251412709554}',42,90),(673,9,6,'{\"Runtime\": \"failure\"}',18,90),(674,9,6,'{\"Runtime\": 0.5675844351450602}',13,90),(675,9,6,'{\"Runtime\": 0.13193488121032715}',4,90),(676,9,6,'{\"Runtime\": 2.391620397567749}',3,90),(677,9,6,'{\"Runtime\": 29.14053972562154}',7,90),(678,9,6,'{\"Runtime\": 3.8960328896840415}',8,90),(679,9,6,'{\"Runtime\": 10.396063168843588}',9,90),(680,9,6,'{\"Runtime\": 4.269450823465983}',19,90),(681,9,6,'{\"Runtime\": 0.0007892449696858724}',11,92),(682,9,6,'{\"Runtime\": 0.0007674694061279297}',1,92),(683,9,6,'{\"Runtime\": 0.0007843971252441406}',40,92),(684,9,6,'{\"Runtime\": 0.0008561611175537109}',15,92),(685,9,6,'{\"Runtime\": 0.0016540686289469402}',14,92),(686,9,6,'{\"Runtime\": 0.0012440681457519531}',16,92),(687,9,6,'{\"Runtime\": 0.0028262933095296225}',37,92),(688,9,6,'{\"Runtime\": 0.0056963761647542315}',41,92),(689,9,6,'{\"Runtime\": 0.010967572530110678}',17,92),(690,9,6,'{\"Runtime\": 0.11198178927103679}',42,92),(691,9,6,'{\"Runtime\": 0.8005038897196451}',18,92),(692,9,6,'{\"Runtime\": 0.40584325790405273}',13,92),(693,9,6,'{\"Runtime\": 0.08980830510457356}',4,92),(694,9,6,'{\"Runtime\": 1.4613077640533447}',3,92),(695,9,6,'{\"Runtime\": 1.4821217060089111}',5,92),(696,9,6,'{\"Runtime\": 0.3037753105163574}',6,92),(697,9,6,'{\"Runtime\": 15.577231327692667}',7,92),(698,9,6,'{\"Runtime\": 2.4152897198994956}',8,92),(699,9,6,'{\"Runtime\": 5.7399264971415205}',9,92),(700,9,6,'{\"Runtime\": 2.6429529984792075}',19,92),(701,9,6,'{\"Runtime\": 0.0007210572560628256}',11,91),(702,9,6,'{\"Runtime\": 0.0007765293121337891}',1,91),(703,9,6,'{\"Runtime\": 0.0007682641347249349}',40,91),(704,9,6,'{\"Runtime\": 0.0008678436279296875}',15,91),(705,9,6,'{\"Runtime\": 0.0017045338948567708}',14,91),(706,9,6,'{\"Runtime\": 0.0012580553690592449}',16,91),(707,9,6,'{\"Runtime\": 0.002782742182413737}',37,91),(708,9,6,'{\"Runtime\": 0.005638281504313151}',41,91),(709,9,6,'{\"Runtime\": 0.010930856068929037}',17,91),(710,9,6,'{\"Runtime\": 0.11216195424397786}',42,91),(711,9,6,'{\"Runtime\": 0.7805000146230062}',18,91),(712,9,6,'{\"Runtime\": 0.4080023765563965}',13,91),(713,9,6,'{\"Runtime\": 0.08960866928100586}',4,91),(714,9,6,'{\"Runtime\": 1.4599664211273193}',3,91),(715,9,6,'{\"Runtime\": 1.4833416144053142}',5,91),(716,9,6,'{\"Runtime\": 0.3044292132059733}',6,91),(717,9,6,'{\"Runtime\": 15.548753579457602}',7,91),(718,9,6,'{\"Runtime\": 2.414018472035726}',8,91),(719,9,6,'{\"Runtime\": 5.688889741897583}',9,91),(720,9,6,'{\"Runtime\": 2.638049364089966}',19,91),(721,9,6,'{\"Runtime\": 0.04622793197631836}',11,66),(722,9,6,'{\"Runtime\": 0.0008397102355957031}',12,66),(723,9,6,'{\"Runtime\": 0.15214085578918457}',13,66),(724,9,6,'{\"Runtime\": 0.023692766825358074}',1,21),(725,9,6,'{\"Runtime\": \"failure\"}',2,21),(726,9,6,'{\"Runtime\": 7.596179564793904}',1,21),(727,9,6,'{\"Runtime\": 40.310393969217934}',3,21),(728,9,6,'{\"Runtime\": \"failure\"}',4,21),(729,5,3,'{\"Runtime\": \">9000\"}',9,87),(730,5,3,'{\"Runtime\": \">9000\"}',19,87),(731,5,3,'{\"Runtime\": 0.10721233333333335}',11,86),(732,5,3,'{\"Runtime\": 0.600458}',15,86),(733,5,3,'{\"Runtime\": 0.29315800000000003}',1,86),(734,5,3,'{\"Runtime\": 4.016809666666667}',16,86),(735,10,7,'{\"Runtime\": \"failure\"}',14,37),(736,11,6,'{\"Runtime\": 0.01808651288350423}',22,51),(737,10,7,'{\"Runtime\": \"failure\"}',11,70),(738,10,7,'{\"Runtime\": \"failure\"}',24,70),(739,10,7,'{\"Runtime\": \"failure\"}',25,70),(740,10,7,'{\"Runtime\": \"failure\"}',26,70),(741,10,7,'{\"Runtime\": \"failure\"}',3,70),(742,10,7,'{\"Runtime\": \"failure\"}',27,70),(743,10,7,'{\"Runtime\": \"failure\"}',28,70),(744,11,6,'{\"Runtime\": 15.729119221369425}',7,55),(745,10,7,'{\"Runtime\": \"failure\"}',29,70),(746,11,6,'{\"Runtime\": 1.2478099664052327}',4,55),(747,11,6,'{\"Runtime\": 0.012781778971354166}',2,50),(748,10,7,'{\"Runtime\": \"failure\"}',30,70),(749,11,6,'{\"Runtime\": 0.9416483243306478}',3,49),(750,10,7,'{\"Runtime\": \"failure\"}',31,70),(751,10,7,'{\"Runtime\": \"failure\"}',32,70),(752,10,7,'{\"Runtime\": \"failure\"}',33,70),(753,11,6,'{\"Runtime\": 17.639404773712158}',5,52),(754,11,6,'{\"Runtime\": 0.07493249575297038}',1,52),(755,11,6,'{\"Runtime\": 0.0006740887959798177}',1,53),(756,11,6,'{\"Runtime\": 0.0006223519643147787}',11,53),(757,10,7,'{\"Runtime\": 0.7291736666666666}',16,34),(758,11,6,'{\"Runtime\": \"failure\"}',6,54),(759,11,6,'{\"Runtime\": 0.010919729868570963}',23,56),(760,11,6,'{}',11,63),(761,11,6,'{}',24,63),(762,11,6,'{}',25,63),(763,10,7,'{\"Runtime\": 0.7257020000000001}',38,34),(764,10,7,'{\"Runtime\": 1.8576993333333334}',39,34),(765,5,3,'{\"Runtime\": 668.4105373333334}',17,86),(766,10,7,'{\"Runtime\": 1.0542246666666666}',22,34),(767,10,7,'{\"Runtime\": 37.070841}',21,34),(768,10,7,'{\"Runtime\": 3.3402006666666666}',10,34),(769,10,7,'{\"Runtime\": 8.074895}',3,34),(770,10,7,'{\"Runtime\": 102.77790033333333}',7,34),(771,11,6,'{}',26,63),(772,10,7,'{\"Runtime\": 29.159336}',9,34),(773,11,6,'{}',3,63),(774,11,6,'{}',27,63),(775,10,7,'{\"Runtime\": 243.37204900000003}',19,34),(776,10,7,'{\"Runtime\": 6.0333333333333334e-05}',15,62),(777,10,7,'{\"Runtime\": 6.233333333333334e-05}',20,62),(778,10,7,'{\"Runtime\": 6.466666666666666e-05}',7,62),(779,10,7,'{\"Runtime\": 6.2e-05}',9,62),(780,10,7,'{\"Runtime\": 6.133333333333334e-05}',43,62),(781,10,7,'{\"Runtime\": 6.266666666666667e-05}',21,62),(782,10,7,'{\"Runtime\": 7.033333333333333e-05}',13,62),(783,10,7,'{\"Runtime\": 8.033333333333334e-05}',18,62),(784,10,7,'{\"Runtime\": 0.332818}',1,21),(785,10,7,'{\"Runtime\": 0.33873899999999996}',2,21),(786,10,7,'{\"Runtime\": 0.37303100000000006}',1,21),(787,10,7,'{\"Runtime\": 18.799994666666667}',3,21),(788,10,7,'{\"Runtime\": 19.802706}',4,21),(789,10,7,'{\"Runtime\": 14.478462}',5,21),(790,10,7,'{\"Runtime\": 247.04695300000003}',6,21),(791,11,6,'{}',28,63),(792,11,6,'{}',29,63),(793,11,6,'{}',30,63),(794,11,6,'{}',31,63),(795,11,6,'{}',32,63),(796,11,6,'{}',33,63),(797,11,6,'{\"Runtime\": 0.043999433517456055}',11,75),(798,11,6,'{\"Runtime\": 0.01964227358500163}',24,75),(799,11,6,'{\"Runtime\": 0.1554578940073649}',25,75),(800,11,6,'{\"Runtime\": 2.06587290763855}',26,75),(801,11,6,'{\"Runtime\": 0.9639028708140055}',3,75),(802,11,6,'{\"Runtime\": 0.21744799613952637}',27,75),(803,11,6,'{\"Runtime\": 31.345789353052776}',28,75),(804,11,6,'{\"Runtime\": 0.15792028109232584}',29,75),(805,11,6,'{\"Runtime\": 0.07801246643066406}',30,75),(806,11,6,'{\"Runtime\": 0.08402323722839355}',31,75),(807,11,6,'{\"Runtime\": 0.2938060760498047}',32,75),(808,11,6,'{\"Runtime\": 0.005721330642700195}',33,75),(809,11,6,'{\"Runtime\": 0.01548329989115397}',11,45),(810,11,6,'{\"Runtime\": 0.02732404073079427}',24,45),(811,11,6,'{\"Runtime\": 1.226980447769165}',25,45),(812,11,6,'{\"Runtime\": 32.55322281519572}',26,45),(813,11,6,'{\"Runtime\": 4.476683139801025}',3,45),(814,11,6,'{\"Runtime\": 0.11136174201965332}',27,45),(815,11,6,'{\"Runtime\": 175.29825146993002}',28,45),(816,11,6,'{\"Runtime\": 0.06471443176269531}',29,45),(817,11,6,'{\"Runtime\": 0.06659022967020671}',30,45),(818,11,6,'{\"Runtime\": 0.05418252944946289}',31,45),(819,11,6,'{\"Runtime\": 0.1508310635884603}',32,45),(820,11,6,'{\"Runtime\": 0.0017689863840738933}',33,45),(821,11,6,'{\"Runtime\": 0.10982489585876465}',14,93),(822,11,6,'{\"Runtime\": 0.0035309791564941406}',14,93),(823,11,6,'{\"Runtime\": 0.0019179185231526692}',11,93),(824,11,6,'{\"Runtime\": 0.0023488998413085938}',1,93),(825,11,6,'{\"Runtime\": 0.00842428207397461}',44,93),(826,11,6,'{\"Runtime\": \"failure\"}',23,93),(827,11,6,'{\"Runtime\": \"failure\"}',39,93),(828,11,6,'{\"Runtime\": \"failure\"}',3,93),(829,11,6,'{\"Runtime\": \"failure\"}',21,93),(830,11,6,'{\"Runtime\": \"failure\"}',5,93),(831,11,6,'{\"Runtime\": 0.036401589711507164}',15,62),(832,11,6,'{\"Runtime\": \"failure\"}',20,62),(833,11,6,'{\"Runtime\": \"failure\"}',7,62),(834,11,6,'{\"Runtime\": 1.7805636723836262}',9,62),(835,11,6,'{\"Runtime\": 0.023191531499226887}',43,62),(836,11,6,'{\"Runtime\": 0.08795022964477539}',21,62),(837,11,6,'{\"Runtime\": \"failure\"}',13,62),(838,11,6,'{\"Runtime\": \"failure\"}',18,62),(839,11,6,'{\"Runtime\": \"failure\"}',11,70),(840,11,6,'{\"Runtime\": \"failure\"}',39,70),(841,11,6,'{\"Runtime\": \"failure\"}',13,70),(842,11,6,'{\"Runtime\": \"failure\"}',18,70),(843,11,6,'{\"Recall\": 0.5, \"MSE\": 0.035256410256410256, \"ACC\": 0.5, \"MCC\": 0, \"Runtime\": 0.028618733088175457, \"Precision\": 0.982371794871795}',24,1),(844,11,6,'{\"Recall\": 0.5, \"MSE\": 0.07107231920199501, \"ACC\": 0.5, \"MCC\": 0, \"Runtime\": 1.0079594453175862, \"Precision\": 0.9644638403990026}',25,1),(845,11,6,'{\"Recall\": 0.5, \"MSE\": 0.029155524885706895, \"ACC\": 0.5, \"MCC\": 0, \"Runtime\": 3.692453304926554, \"Precision\": 0.9854222375571465}',26,1),(846,11,6,'{\"Recall\": 0.5, \"MSE\": 0.0850327048864948, \"ACC\": 0.5, \"MCC\": 0, \"Runtime\": 18.869567314783733, \"Precision\": 0.9574836475567525}',3,1),(847,11,6,'{\"Recall\": 0.8322830196879482, \"MSE\": 0.17703862660944206, \"ACC\": 0.8322830196879482, \"MCC\": 0.264597646150506, \"Runtime\": 0.005441904067993164, \"Precision\": 0.5526749112956009}',27,1),(848,10,7,'{\"Runtime\": 3063.3815073333335}',7,21),(849,11,6,'{\"Runtime\": \">9000\"}',28,1),(850,11,6,'{\"Recall\": 0.7229046242774566, \"MSE\": 0.18031609195402298, \"ACC\": 0.7229046242774566, \"MCC\": 0.08738562361068085, \"Runtime\": 0.0296323299407959, \"Precision\": 0.5085644782365781}',29,1),(851,11,6,'{\"Recall\": 0.5, \"MSE\": 0.094876660341556, \"ACC\": 0.5, \"MCC\": 0, \"Runtime\": 0.008268435796101889, \"Precision\": 0.9525616698292221}',30,1),(852,11,6,'{\"Recall\": 0.7521258332519937, \"MSE\": 0.3771551724137931, \"ACC\": 0.7521258332519937, \"MCC\": 0.30334117907983543, \"Runtime\": 0.002552668253580729, \"Precision\": 0.5912400265957447}',31,1),(853,11,6,'{\"Recall\": 0.7853124617830499, \"MSE\": 0.3132867132867133, \"ACC\": 0.7853124617830499, \"MCC\": 0.35444074031145706, \"Runtime\": 0.01324931780497233, \"Precision\": 0.6100795226463511}',32,1),(854,11,6,'{\"Recall\": 0.5, \"MSE\": 0.13392857142857142, \"ACC\": 0.5, \"MCC\": 0, \"Runtime\": 0.004521846771240234, \"Precision\": 0.9330357142857143}',33,1),(855,10,7,'{\"Runtime\": 2994.077981666667}',8,21),(856,5,3,'{\"Runtime\": 8073.690862}',13,86),(857,11,6,'{\"Runtime\": \">9000\"}',31,72),(858,5,3,'{\"Runtime\": \">9000\"}',18,86),(859,11,6,'{\"Runtime\": \">9000\"}',29,72),(860,10,7,'{\"Runtime\": 3767.149858}',9,21),(861,10,7,'{\"Runtime\": 0.3520296666666667}',1,18),(862,10,7,'{\"Runtime\": 0.35276299999999994}',2,18),(863,10,7,'{\"Runtime\": 0.5625776666666668}',1,18),(864,10,7,'{\"Runtime\": 15.760047333333333}',3,18),(865,10,7,'{\"Runtime\": 61.403423}',4,18),(866,5,3,'{\"Runtime\": \">9000\"}',13,86),(867,11,6,'{\"Runtime\": \">9000\"}',30,73),(868,5,3,'{\"Runtime\": \">9000\"}',4,86),(869,11,6,'{\"Runtime\": \">9000\"}',26,73),(870,10,7,'{\"Runtime\": 8285.398932333335}',5,18),(871,5,3,'{\"Runtime\": \">9000\"}',3,86),(872,11,6,'{\"Runtime\": \">9000\"}',33,71),(873,5,3,'{\"Runtime\": \">9000\"}',5,86),(874,11,6,'{\"Runtime\": \">9000\"}',27,71),(875,10,7,'{\"Runtime\": 5392.632214666667}',6,18),(876,5,3,'{\"Runtime\": \">9000\"}',6,86),(877,11,6,'{\"Runtime\": \">9000\"}',24,74),(878,10,7,'{\"Runtime\": 1763.3597233333332}',7,18),(879,5,3,'{\"Runtime\": \">9000\"}',7,86),(880,11,6,'{\"Runtime\": \">9000\"}',25,74),(881,10,7,'{\"Runtime\": \">9000\"}',8,18),(882,10,7,'{\"Runtime\": 1693.2704606666666}',9,18),(883,10,7,'{\"Runtime\": 1.4889106666666667}',3,49),(884,10,7,'{\"Runtime\": 0.5603253333333333}',23,56),(885,5,3,'{\"Runtime\": \">9000\"}',8,86),(886,10,7,'{\"Runtime\": 31.533428}',7,55),(887,10,7,'{\"Runtime\": 2.108738333333333}',4,55),(888,10,7,'{\"Runtime\": 0.5444196666666666}',2,50),(889,10,7,'{\"Runtime\": 0.5145326666666666}',1,53),(890,10,7,'{\"Runtime\": 0.5109436666666668}',11,53),(891,11,6,'{\"Runtime\": \">9000\"}',26,74),(892,10,7,'{\"Runtime\": 642.9494326666667}',6,54),(893,10,7,'{\"Runtime\": 17.830549666666666}',5,52),(894,10,7,'{\"Runtime\": 0.6288603333333334}',1,52),(895,10,7,'{\"Runtime\": 0.5455446666666667}',22,51),(896,10,7,'{\"Runtime\": 0.513686}',11,66),(897,10,7,'{\"Runtime\": 0.46369533333333335}',12,66),(898,10,7,'{\"Runtime\": 0.5155016666666666}',13,66),(899,10,7,'{\"Runtime\": 0.3262973333333334}',1,77),(900,10,7,'{\"Runtime\": 0.3100283333333333}',16,77),(901,10,7,'{\"Runtime\": 0.325534}',2,77),(902,10,7,'{\"Runtime\": 0.3123923333333333}',44,77),(903,10,7,'{\"Runtime\": 0.430539}',13,77),(904,10,7,'{\"Runtime\": 0.371941}',18,77),(905,10,7,'{\"Runtime\": 1.175032}',4,77),(906,10,7,'{\"Runtime\": 1.4522303333333333}',3,77),(907,10,7,'{\"Runtime\": 11.149660666666668}',5,77),(908,10,7,'{\"Runtime\": 41.442238}',8,77),(909,10,7,'{\"Runtime\": 0.5500283333333332}',11,90),(910,10,7,'{\"Runtime\": 0.5144226666666666}',1,90),(911,10,7,'{\"Runtime\": 0.49640666666666666}',40,90),(912,10,7,'{\"Runtime\": 0.49673966666666663}',15,90),(913,10,7,'{\"Runtime\": 0.5274873333333333}',14,90),(914,10,7,'{\"Runtime\": 0.5122836666666667}',16,90),(915,10,7,'{\"Runtime\": 0.508432}',37,90),(916,10,7,'{\"Runtime\": 0.5133056666666667}',41,90),(917,10,7,'{\"Runtime\": 0.517778}',17,90),(918,10,7,'{\"Runtime\": 0.6290706666666667}',42,90),(919,10,7,'{\"Runtime\": 20.916657}',18,90),(920,10,7,'{\"Runtime\": 0.7699166666666667}',13,90),(921,10,7,'{\"Runtime\": 0.6937796666666666}',4,90),(922,10,7,'{\"Runtime\": 1.6215156666666666}',3,90),(923,10,7,'{\"Runtime\": 3.215098666666666}',5,90),(924,10,7,'{\"Runtime\": 1.1149516666666666}',6,90),(925,10,7,'{\"Runtime\": 13.340896666666666}',7,90),(926,10,7,'{\"Runtime\": 7.575559666666667}',8,90),(927,10,7,'{\"Runtime\": 7.528912000000001}',9,90),(928,10,7,'{\"Runtime\": 8.779532000000001}',19,90),(929,10,7,'{\"Runtime\": 0.5348226666666668}',11,91),(930,10,7,'{\"Runtime\": 0.5331283333333333}',1,91),(931,10,7,'{\"Runtime\": 0.5285329999999999}',40,91),(932,10,7,'{\"Runtime\": 0.5319586666666667}',15,91),(933,10,7,'{\"Runtime\": 0.5494823333333333}',14,91),(934,10,7,'{\"Runtime\": 0.5417936666666666}',16,91),(935,10,7,'{\"Runtime\": 0.534445}',37,91),(936,10,7,'{\"Runtime\": 0.5642683333333334}',41,91),(937,10,7,'{\"Runtime\": 0.557424}',17,91),(938,10,7,'{\"Runtime\": 0.6492043333333333}',42,91),(939,10,7,'{\"Runtime\": 21.061240333333334}',18,91),(940,10,7,'{\"Runtime\": 0.8156}',13,91),(941,10,7,'{\"Runtime\": 0.7524356666666666}',4,91),(942,10,7,'{\"Runtime\": 1.7109446666666666}',3,91),(943,10,7,'{\"Runtime\": 3.5256563333333335}',5,91),(944,10,7,'{\"Runtime\": 1.2455126666666665}',6,91),(945,10,7,'{\"Runtime\": 13.671006}',7,91),(946,10,7,'{\"Runtime\": 7.984772666666667}',8,91),(947,10,7,'{\"Runtime\": 7.8787780000000005}',9,91),(948,10,7,'{\"Runtime\": 9.270209666666666}',19,91),(949,10,7,'{\"Runtime\": 0.501374}',11,92),(950,10,7,'{\"Runtime\": 0.5024363333333334}',1,92),(951,10,7,'{\"Runtime\": 0.49855099999999997}',40,92),(952,10,7,'{\"Runtime\": 0.5035623333333333}',15,92),(953,10,7,'{\"Runtime\": 0.5311063333333333}',14,92),(954,10,7,'{\"Runtime\": 0.502647}',16,92),(955,10,7,'{\"Runtime\": 0.506765}',37,92),(956,10,7,'{\"Runtime\": 0.5148393333333333}',41,92),(957,10,7,'{\"Runtime\": 0.5248483333333334}',17,92),(958,10,7,'{\"Runtime\": 0.608603}',42,92),(959,5,3,'{\"Runtime\": \">9000\"}',9,86),(960,10,7,'{\"Runtime\": 21.06766766666667}',18,92),(961,10,7,'{\"Runtime\": 0.76788}',13,92),(962,10,7,'{\"Runtime\": 0.6948033333333333}',4,92),(963,10,7,'{\"Runtime\": 1.623717}',3,92),(964,10,7,'{\"Runtime\": 3.222796}',5,92),(965,10,7,'{\"Runtime\": 1.1294856666666666}',6,92),(966,10,7,'{\"Runtime\": 13.215638666666665}',7,92),(967,11,6,'{\"Runtime\": \">9000\"}',3,74),(968,10,7,'{\"Runtime\": 7.643206666666667}',8,92),(969,10,7,'{\"Runtime\": 7.564464}',9,92),(970,10,7,'{\"Runtime\": 8.806768}',19,92),(971,5,3,'{\"Runtime\": \">9000\"}',19,86),(972,5,3,'{\"Runtime\": 0.17245266666666667}',11,89),(973,5,3,'{\"Runtime\": 2.5335273333333332}',15,89),(974,5,3,'{\"Runtime\": 0.5433396666666667}',1,89),(975,5,3,'{\"Runtime\": 6.817484666666666}',16,89),(976,11,6,'{\"Runtime\": \">9000\"}',27,74),(977,5,3,'{\"Runtime\": \">9000\"}',17,89),(978,11,6,'{\"Runtime\": \">9000\"}',28,74),(979,5,3,'{\"Runtime\": \">9000\"}',13,89),(980,5,3,'{\"Runtime\": 0.15363133333333334}',11,85),(981,5,3,'{\"Runtime\": 6.150727666666666}',15,85),(982,5,3,'{\"Runtime\": 0.5536996666666666}',1,85),(983,5,3,'{\"Runtime\": 14.422771333333332}',16,85),(984,11,6,'{\"Runtime\": \">9000\"}',29,74),(985,5,3,'{\"Runtime\": \">9000\"}',17,85),(986,11,6,'{\"Runtime\": \">9000\"}',30,74),(987,5,3,'{\"Runtime\": \">9000\"}',13,85),(988,11,6,'{\"Runtime\": \">9000\"}',31,74),(989,5,3,'{\"Runtime\": \">9000\"}',18,85),(990,11,6,'{\"Runtime\": \">9000\"}',32,74),(991,5,3,'{\"Runtime\": \">9000\"}',13,85),(992,11,6,'{\"Runtime\": \">9000\"}',33,74),(993,5,3,'{\"Runtime\": \">9000\"}',4,85),(994,11,6,'{\"Runtime\": \">9000\"}',34,8),(995,5,3,'{\"Runtime\": \">9000\"}',3,85),(996,11,6,'{\"Runtime\": \">9000\"}',35,8),(997,5,3,'{\"Runtime\": \">9000\"}',5,85),(998,11,6,'{\"Runtime\": \">9000\"}',36,8),(999,5,3,'{\"Runtime\": \">9000\"}',6,85),(1000,11,6,'{\"Runtime\": \">9000\"}',37,8),(1001,5,3,'{\"Runtime\": \">9000\"}',7,85),(1002,11,6,'{\"Runtime\": \">9000\"}',22,8),(1003,5,3,'{\"Runtime\": \">9000\"}',8,85),(1004,11,6,'{\"Runtime\": \">9000\"}',21,8),(1005,5,3,'{\"Runtime\": \">9000\"}',9,85),(1006,11,6,'{\"Runtime\": \">9000\"}',18,8),(1007,5,3,'{\"Runtime\": \">9000\"}',19,85),(1008,5,3,'{\"Runtime\": 0.007090999999999999}',1,79),(1009,5,3,'{\"Runtime\": 0.0004786666666666669}',16,79),(1010,5,3,'{\"Runtime\": 0.0004873333333333338}',2,79),(1011,5,3,'{\"Runtime\": 0.0004900000000000005}',44,79),(1012,5,3,'{\"Runtime\": 0.0010846666666666598}',13,79),(1013,5,3,'{\"Runtime\": 0.0011043333333333367}',18,79),(1014,5,3,'{\"Runtime\": 0.0005849999999999836}',4,79),(1015,5,3,'{\"Runtime\": 0.0027890000000000783}',3,79),(1016,5,3,'{\"Runtime\": 0.013723333333333088}',5,79),(1017,5,3,'{\"Runtime\": 0.018370666666667052}',8,79),(1018,5,3,'{\"Runtime\": 0.004456}',1,77),(1019,5,3,'{\"Runtime\": 0.0013893333333333336}',16,77),(1020,5,3,'{\"Runtime\": 0.0027916666666666667}',2,77),(1021,5,3,'{\"Runtime\": 0.0017286666666666668}',44,77),(1022,5,3,'{\"Runtime\": 0.06367833333333335}',13,77),(1023,5,3,'{\"Runtime\": 0.050469333333333345}',18,77),(1024,5,3,'{\"Runtime\": 1.0353103333333333}',4,77),(1025,5,3,'{\"Runtime\": 0.4925126666666668}',3,77),(1026,5,3,'{\"Runtime\": 5.064491000000001}',5,77),(1027,5,3,'{\"Runtime\": 55.56666833333333}',8,77),(1028,5,3,'{\"Runtime\": \"failure\"}',1,78),(1029,5,3,'{\"Runtime\": \"failure\"}',16,78),(1030,5,3,'{\"Runtime\": \"failure\"}',2,78),(1031,5,3,'{\"Runtime\": \"failure\"}',44,78),(1032,5,3,'{\"Runtime\": \"failure\"}',13,78),(1033,5,3,'{\"Runtime\": \"failure\"}',18,78),(1034,5,3,'{\"Runtime\": \"failure\"}',4,78),(1035,5,3,'{\"Runtime\": \"failure\"}',3,78),(1036,5,3,'{\"Runtime\": \"failure\"}',5,78),(1037,5,3,'{\"Runtime\": \"failure\"}',8,78),(1038,5,3,'{\"Training\": 0.156571, \"Precision\": 0.9753086419753086, \"Runtime\": 0.17017266666666667, \"MSE\": 0.02666666666666667, \"MCC\": 0.9611163282360078, \"Recall\": 0.9733333333333333, \"Testing\": 1.6666666666666667e-05, \"ACC\": 0.9733333333333333}',11,70),(1039,5,3,'{\"Training\": 1.4442503333333334, \"Precision\": 0.482258064516129, \"Runtime\": 1.455023, \"MSE\": 0.041666666666666664, \"MCC\": -0.01535490895516939, \"Recall\": 0.4966777408637874, \"Testing\": 6.766666666666667e-05, \"ACC\": 0.4966777408637874}',24,70),(1040,5,3,'{\"Training\": 13.942152666666667, \"Precision\": 0.5268084066471163, \"Runtime\": 13.968303333333333, \"MSE\": 0.185785536159601, \"MCC\": 0.07443346632940229, \"Recall\": 0.5516660779465442, \"Testing\": 0.0006776666666666667, \"ACC\": 0.5516660779465442}',25,70),(1041,11,6,'{\"Runtime\": \">9000\"}',13,8),(1042,5,3,'{\"Training\": 210.47597799999997, \"Precision\": 0.9279698648581974, \"Runtime\": 210.54071033333332, \"MSE\": 0.015095316139049427, \"MCC\": 0.6957074400335973, \"Recall\": 0.7827353523351883, \"Testing\": 0.010381, \"ACC\": 0.7827353523351883}',26,70),(1043,5,3,'{\"Training\": 2.0766413333333333, \"Precision\": 0.9042342088938038, \"Runtime\": 2.111527666666667, \"MSE\": 0.03039630627164294, \"MCC\": 0.8034686605808652, \"Recall\": 0.899249911519243, \"Testing\": 0.004546333333333334, \"ACC\": 0.899249911519243}',3,70),(1044,5,3,'{\"Training\": 7.6650149999999995, \"Precision\": 0.8883001613770846, \"Runtime\": 7.687228, \"MSE\": 0.023873390557939914, \"MCC\": 0.25489291139105835, \"Recall\": 0.54183000854738, \"Testing\": 0.00035199999999999994, \"ACC\": 0.54183000854738}',27,70),(1045,5,3,'{\"Training\": 1.934978, \"Precision\": 0.9786044657097288, \"Runtime\": 2.442429666666667, \"MSE\": 0.003518373729476153, \"MCC\": 0.9135740204394748, \"Recall\": 0.9359641157883267, \"Testing\": 0.12093, \"ACC\": 0.9359641157883267}',28,70),(1046,5,3,'{\"Training\": 3.094139666666667, \"Precision\": 0.9971264367816092, \"Runtime\": 3.116324666666667, \"MSE\": 0.005747126436781609, \"MCC\": 0, \"Recall\": 0.5, \"Testing\": 0.00014533333333333333, \"ACC\": 0.5}',29,70),(1047,5,3,'{\"Training\": 4.323156333333333, \"Precision\": 0.8538579067990831, \"Runtime\": 4.340229333333334, \"MSE\": 0.04459203036053131, \"MCC\": 0.7643002446708497, \"Recall\": 0.9127044025157233, \"Testing\": 0.00020866666666666668, \"ACC\": 0.9127044025157233}',30,70),(1048,5,3,'{\"Training\": 3.2881246666666666, \"Precision\": 0.6100264570519528, \"Runtime\": 3.298135, \"MSE\": 0.21120689655172412, \"MCC\": 0.31312250691647786, \"Recall\": 0.7227775640620783, \"Testing\": 0.00014899999999999996, \"ACC\": 0.7227775640620783}',31,70),(1049,5,3,'{\"Training\": 8.409597, \"Precision\": 0.9489034064395708, \"Runtime\": 8.436818666666666, \"MSE\": 0.1020979020979021, \"MCC\": 0.09013849639772625, \"Recall\": 0.504524886877828, \"Testing\": 0.0003863333333333333, \"ACC\": 0.504524886877828}',32,70),(1050,5,3,'{\"Training\": 0.24095600000000003, \"Precision\": 0.944954128440367, \"Runtime\": 0.26043533333333335, \"MSE\": 0.10714285714285714, \"MCC\": 0.421878716429434, \"Recall\": 0.6, \"Testing\": 1.3e-05, \"ACC\": 0.6}',33,70),(1051,5,3,'{\"ComputingNeighbors\": 0.004422666666666667, \"Runtime\": 0.02333366666666667, \"TreeBuilding\": 0.0001643333333333333}',1,60),(1052,5,3,'{\"ComputingNeighbors\": 0.019277, \"Runtime\": 0.04388533333333333, \"TreeBuilding\": 0.0023786666666666665}',2,60),(1053,5,3,'{\"ComputingNeighbors\": 0.09479966666666666, \"Runtime\": 0.10932433333333332, \"TreeBuilding\": 0.00894}',1,60),(1054,5,3,'{\"ComputingNeighbors\": 0.46799900000000005, \"Runtime\": 0.9851800000000001, \"TreeBuilding\": 0.4963946666666667}',3,60),(1055,5,3,'{\"ComputingNeighbors\": 6.348837, \"Runtime\": 7.401445666666667, \"TreeBuilding\": 1.0221383333333334}',4,60),(1056,5,3,'{\"ComputingNeighbors\": 456.56644166666666, \"Runtime\": 461.78227266666664, \"TreeBuilding\": 5.0769486666666666}',5,60),(1057,5,3,'{\"ComputingNeighbors\": 1356.6503089999999, \"Runtime\": 1358.6639033333333, \"TreeBuilding\": 1.8044833333333334}',6,60),(1058,5,3,'{\"ComputingNeighbors\": 17.418416333333333, \"Runtime\": 64.391858, \"TreeBuilding\": 46.904569}',7,60),(1059,5,3,'{\"ComputingNeighbors\": 399.00751999999994, \"Runtime\": 455.28425166666665, \"TreeBuilding\": 56.116470666666665}',8,60),(1060,5,3,'{\"ComputingNeighbors\": 21.520064333333334, \"Runtime\": 60.00201966666666, \"TreeBuilding\": 38.418528333333335}',9,60),(1061,5,3,'{\"ComputingNeighbors\": 0.000951, \"Runtime\": 0.014403, \"TreeBuilding\": 0.00015800000000000002}',1,61),(1062,5,3,'{\"ComputingNeighbors\": 0.019224, \"Runtime\": 0.030151333333333332, \"TreeBuilding\": 0.0023233333333333335}',2,61),(1063,5,3,'{\"ComputingNeighbors\": 0.09486033333333332, \"Runtime\": 0.11100199999999999, \"TreeBuilding\": 0.008936}',1,61),(1064,5,3,'{\"ComputingNeighbors\": 0.46531933333333336, \"Runtime\": 0.9727456666666666, \"TreeBuilding\": 0.4929569999999999}',3,61),(1065,5,3,'{\"ComputingNeighbors\": 6.351595666666667, \"Runtime\": 7.400855666666668, \"TreeBuilding\": 1.0218180000000001}',4,61),(1066,11,6,'{\"Runtime\": \">9000\"}',10,8),(1067,5,3,'{\"ComputingNeighbors\": 456.52256166666666, \"Runtime\": 461.720717, \"TreeBuilding\": 5.063342}',5,61),(1068,5,3,'{\"ComputingNeighbors\": 1357.094821, \"Runtime\": 1359.1278243333334, \"TreeBuilding\": 1.8067736666666665}',6,61),(1069,5,3,'{\"ComputingNeighbors\": 17.427971, \"Runtime\": 64.44057566666667, \"TreeBuilding\": 46.947577}',7,61),(1070,5,3,'{\"ComputingNeighbors\": 399.58574733333336, \"Runtime\": 455.900101, \"TreeBuilding\": 56.14640433333333}',8,61),(1071,5,3,'{\"ComputingNeighbors\": 21.494066333333336, \"Runtime\": 59.98955966666667, \"TreeBuilding\": 38.439909666666665}',9,61),(1072,5,3,'{\"ComputingNeighbors\": 0.0008733333333333333, \"Runtime\": 0.014332999999999999, \"TreeBuilding\": 0.00015766666666666666}',1,59),(1073,5,3,'{\"ComputingNeighbors\": 0.014771666666666667, \"Runtime\": 0.024039666666666668, \"TreeBuilding\": 0.0023710000000000003}',2,59),(1074,5,3,'{\"ComputingNeighbors\": 0.07726, \"Runtime\": 0.09626366666666668, \"TreeBuilding\": 0.008899333333333334}',1,59),(1075,5,3,'{\"ComputingNeighbors\": 0.5632846666666667, \"Runtime\": 1.0726259999999999, \"TreeBuilding\": 0.49288366666666666}',3,59),(1076,5,3,'{\"ComputingNeighbors\": 5.855181333333334, \"Runtime\": 6.898142, \"TreeBuilding\": 1.0200606666666665}',4,59),(1077,5,3,'{\"ComputingNeighbors\": 397.2273076666667, \"Runtime\": 402.42209599999995, \"TreeBuilding\": 5.061002333333334}',5,59),(1078,11,6,'{\"Runtime\": \">9000\"}',3,8),(1079,11,6,'{\"Runtime\": \"failure\"}',5,8),(1080,5,3,'{\"ComputingNeighbors\": 1340.797441, \"Runtime\": 1342.805071, \"TreeBuilding\": 1.80616}',6,59),(1081,5,3,'{\"ComputingNeighbors\": 18.36375166666667, \"Runtime\": 65.41946800000001, \"TreeBuilding\": 46.983886000000005}',7,59),(1082,5,3,'{\"ComputingNeighbors\": 147.91936233333334, \"Runtime\": 204.285632, \"TreeBuilding\": 56.21521333333333}',8,59),(1083,5,3,'{\"ComputingNeighbors\": 11.658430000000001, \"Runtime\": 50.11818466666666, \"TreeBuilding\": 38.388857}',9,59),(1084,5,3,'{}',1,57),(1085,5,3,'{}',2,57),(1086,5,3,'{}',1,57),(1087,5,3,'{}',3,57),(1088,5,3,'{}',4,57),(1089,11,6,'{\"Runtime\": \">9000\"}',34,16),(1090,5,3,'{\"Runtime\": \">9000\"}',5,57),(1091,11,6,'{\"Runtime\": \">9000\"}',35,16),(1092,5,3,'{\"Runtime\": \">9000\"}',6,57),(1093,11,6,'{\"Runtime\": \">9000\"}',36,16),(1094,11,6,'{\"Runtime\": \">9000\"}',37,16),(1095,5,3,'{}',7,57),(1096,11,6,'{\"Runtime\": \">9000\"}',22,16),(1097,5,3,'{\"Runtime\": \">9000\"}',8,57),(1098,11,6,'{\"Runtime\": \">9000\"}',21,16),(1099,5,3,'{}',9,57),(1100,5,3,'{\"Runtime\": \"failure\"}',1,58),(1101,5,3,'{\"Runtime\": \"failure\"}',2,58),(1102,5,3,'{\"Runtime\": \"failure\"}',1,58),(1103,5,3,'{\"Runtime\": \"failure\"}',3,58),(1104,5,3,'{\"Runtime\": \"failure\"}',4,58),(1105,5,3,'{\"Runtime\": \"failure\"}',5,58),(1106,5,3,'{\"Runtime\": \"failure\"}',6,58),(1107,5,3,'{\"Runtime\": \"failure\"}',7,58),(1108,5,3,'{\"Runtime\": \"failure\"}',8,58),(1109,5,3,'{\"Runtime\": \"failure\"}',9,58),(1110,5,3,'{\"Runtime\": 4.515753}',16,34),(1111,5,3,'{\"Runtime\": 0.28476433333333334}',38,34),(1112,5,3,'{\"Runtime\": 133.51405466666665}',39,34),(1113,5,3,'{\"Runtime\": 0.5300936666666666}',22,34),(1114,5,3,'{\"Runtime\": 16.519860666666666}',21,34),(1115,5,3,'{\"Runtime\": 4.417967666666667}',10,34),(1116,11,6,'{\"Runtime\": \">9000\"}',18,16),(1117,5,3,'{\"Runtime\": 1589.5635993333333}',3,34),(1118,11,6,'{\"Runtime\": \">9000\"}',13,16),(1119,5,3,'{\"Runtime\": \">9000\"}',7,34),(1120,11,6,'{\"Runtime\": \">9000\"}',10,16),(1121,5,3,'{\"Runtime\": \">9000\"}',9,34),(1122,5,3,'{\"Runtime\": 596.8107536666665}',19,34),(1123,5,3,'{\"Runtime\": 0.10004333333333333}',16,35),(1124,5,3,'{\"Runtime\": 0.046352666666666674}',38,35),(1125,5,3,'{\"Runtime\": 1.052541}',39,35),(1126,5,3,'{\"Runtime\": 0.30795333333333336}',22,35),(1127,5,3,'{\"Runtime\": 8.048388000000001}',21,35),(1128,5,3,'{\"Runtime\": 5.877456666666667}',10,35),(1129,5,3,'{\"Runtime\": 46.373398333333334}',3,35),(1130,5,3,'{\"Runtime\": 62.842606666666676}',7,35),(1131,5,3,'{\"Runtime\": 49.33249833333333}',9,35),(1132,5,3,'{\"Runtime\": 269.5900193333334}',19,35),(1133,5,3,'{\"Runtime\": 10.998223666666666}',16,33),(1134,5,3,'{\"Runtime\": 0.8563196666666668}',38,33),(1135,5,3,'{\"Runtime\": 407.0112696666667}',39,33),(1136,5,3,'{\"Runtime\": 291.1150753333333}',22,33),(1137,5,3,'{\"Runtime\": 86.26217799999999}',21,33),(1138,5,3,'{\"Runtime\": 35.73188633333333}',10,33),(1139,11,6,'{\"Runtime\": \">9000\"}',3,16),(1140,11,6,'{\"Runtime\": \"failure\"}',5,16),(1141,5,3,'{\"Runtime\": \">9000\"}',3,33),(1142,11,6,'{\"Runtime\": \">9000\"}',34,11),(1143,5,3,'{\"Runtime\": \">9000\"}',7,33),(1144,11,6,'{\"Runtime\": \">9000\"}',35,11),(1145,12,8,'{\"Runtime\": 0.0030530293782552085}',11,31),(1146,12,8,'{\"Runtime\": \"failure\"}',24,31),(1147,12,8,'{\"Runtime\": 0.022791306177775066}',25,31),(1148,12,8,'{\"Runtime\": 1.3896752993265789}',26,31),(1149,12,8,'{\"Runtime\": 0.12707599004109701}',3,31),(1150,12,8,'{\"Runtime\": 0.2584398587544759}',27,31),(1151,12,8,'{\"Runtime\": \"failure\"}',28,31),(1152,12,8,'{\"Runtime\": 0.06890471776326497}',29,31),(1153,12,8,'{\"Runtime\": 0.02953179677327474}',30,31),(1154,12,8,'{\"Runtime\": 0.05565460522969564}',31,31),(1155,12,8,'{\"Runtime\": \"failure\"}',32,31),(1156,12,8,'{\"Runtime\": \"failure\"}',33,31),(1157,5,3,'{\"Runtime\": \">9000\"}',9,33),(1158,11,6,'{\"Runtime\": \">9000\"}',36,11),(1159,5,3,'{\"Runtime\": \">9000\"}',19,33),(1160,5,3,'{\"Training\": 0.006604666666666666, \"Runtime\": 0.006809999999999999}',11,41),(1161,5,3,'{\"Training\": 0.13724499999999998, \"Runtime\": 0.13748866666666668}',2,41),(1162,5,3,'{\"Training\": 5.891550666666666, \"Runtime\": 5.895401, \"Testing\": 0.0017279999999999997}',13,41),(1163,5,3,'{\"Training\": 0.0028216666666666668, \"Runtime\": 0.0030483333333333335}',11,42),(1164,5,3,'{\"Training\": 0.19255199999999997, \"Runtime\": 0.19283033333333333}',2,42),(1165,5,3,'{\"Training\": 8.417844333333333, \"Runtime\": 8.421846, \"Testing\": 0.0017346666666666667}',13,42),(1166,5,3,'{\"Runtime\": \"failure\"}',14,69),(1167,5,3,'{\"Runtime\": 29.628349333333333}',14,68),(1168,5,3,'{\"Runtime\": 0.0004136666666666666}',15,107),(1169,5,3,'{\"Runtime\": 0.48276600000000003}',20,107),(1170,5,3,'{\"Runtime\": 9.108174666666667}',18,107),(1171,5,3,'{\"Runtime\": 2.2305146666666666}',13,107),(1172,5,3,'{\"Runtime\": 0.0004753333333333333}',15,108),(1173,5,3,'{\"Runtime\": 0.5578026666666667}',20,108),(1174,5,3,'{\"Runtime\": 9.257705999999999}',18,108),(1175,5,3,'{\"Runtime\": 14.398611666666667}',13,108),(1176,5,3,'{\"Runtime\": \"failure\"}',1,48),(1177,5,3,'{\"Runtime\": \"failure\"}',2,48),(1178,5,3,'{\"Runtime\": \"failure\"}',1,48),(1179,5,3,'{\"Runtime\": \"failure\"}',3,48),(1180,5,3,'{\"Runtime\": \"failure\"}',4,48),(1181,5,3,'{\"Runtime\": \"failure\"}',5,48),(1182,5,3,'{\"Runtime\": \"failure\"}',6,48),(1183,5,3,'{\"Runtime\": \"failure\"}',7,48),(1184,5,3,'{\"Runtime\": \"failure\"}',8,48),(1185,5,3,'{\"Runtime\": \"failure\"}',9,48),(1186,5,3,'{}',1,47),(1187,5,3,'{}',2,47),(1188,5,3,'{}',1,47),(1189,5,3,'{}',3,47),(1190,5,3,'{}',4,47),(1191,11,6,'{\"Runtime\": \">9000\"}',37,11),(1192,5,3,'{\"Runtime\": \">9000\"}',5,47),(1193,11,6,'{\"Runtime\": \">9000\"}',22,11),(1194,5,3,'{\"Runtime\": \">9000\"}',6,47),(1195,11,6,'{\"Runtime\": \">9000\"}',21,11),(1196,11,6,'{\"Runtime\": \">9000\"}',18,11),(1197,5,3,'{}',7,47),(1198,11,6,'{\"Runtime\": \">9000\"}',13,11),(1199,5,3,'{\"Runtime\": \">9000\"}',8,47),(1200,11,6,'{\"Runtime\": \">9000\"}',10,11),(1201,5,3,'{}',9,47),(1202,5,3,'{\"ComputingNeighbors\": 0.00019400000000000003, \"Runtime\": 0.020020666666666666, \"TreeBuilding\": 0.00016333333333333334, \"BaseCases\": 1445}',1,46),(1203,5,3,'{\"ComputingNeighbors\": 0.00197, \"Runtime\": 0.009052, \"TreeBuilding\": 0.0023606666666666667, \"BaseCases\": 10313}',2,46),(1204,5,3,'{\"ComputingNeighbors\": 0.008625666666666665, \"Runtime\": 0.022749333333333333, \"TreeBuilding\": 0.008985666666666668, \"BaseCases\": 48544}',1,46),(1205,5,3,'{\"ComputingNeighbors\": 24.10266233333333, \"Runtime\": 24.623770333333336, \"TreeBuilding\": 0.49445133333333335, \"BaseCases\": 20706012}',3,46),(1206,5,3,'{\"ComputingNeighbors\": 8.523524333333333, \"Runtime\": 9.580098999999999, \"TreeBuilding\": 1.0299036666666668, \"BaseCases\": 45979591}',4,46),(1207,5,3,'{\"ComputingNeighbors\": 6.851639333333334, \"Runtime\": 12.012558666666665, \"TreeBuilding\": 5.039137, \"BaseCases\": 33779775}',5,46),(1208,11,6,'{\"Runtime\": \">9000\"}',3,11),(1209,11,6,'{\"Runtime\": \">9000\"}',5,11),(1210,5,3,'{\"ComputingNeighbors\": 5731.9199960000005, \"Runtime\": 5733.921720666666, \"TreeBuilding\": 1.8153143333333333, \"BaseCases\": 20996172448}',6,46),(1211,11,6,'{\"Runtime\": \">9000\"}',34,10),(1212,11,6,'{\"Runtime\": \">9000\"}',35,10),(1213,5,3,'{\"ComputingNeighbors\": 3789.9967540000002, \"Runtime\": 3836.759453666667, \"TreeBuilding\": 46.694783333333326, \"BaseCases\": 3649909617}',7,46),(1214,5,3,'{\"ComputingNeighbors\": 1.2080110000000002, \"Runtime\": 57.139883999999995, \"TreeBuilding\": 55.774082, \"BaseCases\": 2339839}',8,46),(1215,11,6,'{\"Runtime\": \">9000\"}',36,10),(1216,5,3,'{\"ComputingNeighbors\": 3471.3429730000003, \"Runtime\": 3510.0752023333334, \"TreeBuilding\": 38.65383333333333, \"BaseCases\": 5126604881}',9,46),(1217,5,3,'{\"Runtime\": \"failure\"}',14,37),(1218,5,3,'{\"Runtime\": \"failure\"}',42,64),(1219,5,3,'{\"Runtime\": \"failure\"}',11,64),(1220,5,3,'{\"Runtime\": \"failure\"}',39,64),(1221,5,3,'{\"Runtime\": \"failure\"}',17,64),(1222,5,3,'{\"Runtime\": \"failure\"}',12,64),(1223,5,3,'{\"Runtime\": \"failure\"}',18,64),(1224,5,3,'{\"Runtime\": 0.030477333333333332}',34,11),(1225,5,3,'{\"Runtime\": 3.674628333333333}',35,11),(1226,5,3,'{\"Runtime\": 259.13486566666666}',36,11),(1227,5,3,'{\"Runtime\": 87.82831233333333}',37,11),(1228,5,3,'{\"Runtime\": 465.978997}',22,11),(1229,5,3,'{\"Runtime\": \"failure\"}',21,11),(1230,5,3,'{\"Runtime\": 0.09969866666666666}',18,11),(1231,11,6,'{\"Runtime\": \">9000\"}',37,10),(1232,5,3,'{\"Runtime\": 29.58771966666667}',13,11),(1233,5,3,'{\"Runtime\": 0.2409983333333334}',10,11),(1234,5,3,'{\"Runtime\": 1704.8888613333331}',3,11),(1235,5,3,'{\"Runtime\": \"failure\"}',5,11),(1236,5,3,'{\"Runtime\": 0.05622099999999999}',34,10),(1237,5,3,'{\"Runtime\": 2.632702333333333}',35,10),(1238,5,3,'{\"Runtime\": 255.17321433333333}',36,10),(1239,5,3,'{\"Runtime\": 68.28856666666667}',37,10),(1240,5,3,'{\"Runtime\": 436.64150866666665}',22,10),(1241,11,6,'{\"Runtime\": \">9000\"}',22,10),(1242,5,3,'{\"Runtime\": \"failure\"}',21,10),(1243,5,3,'{\"Runtime\": 0.11639466666666669}',18,10),(1244,5,3,'{\"Runtime\": 26.641883000000004}',13,10),(1245,5,3,'{\"Runtime\": 0.28083433333333335}',10,10),(1246,5,3,'{\"Runtime\": 2556.929495}',3,10),(1247,5,3,'{\"Runtime\": \"failure\"}',5,10),(1248,5,3,'{\"Runtime\": 0.03924866666666667}',34,7),(1249,5,3,'{\"Runtime\": 3.623154}',35,7),(1250,11,6,'{\"Runtime\": \">9000\"}',21,10),(1251,5,3,'{\"Runtime\": 293.72406666666666}',36,7),(1252,5,3,'{\"Runtime\": 89.26020999999999}',37,7),(1253,5,3,'{\"Runtime\": 505.26948999999996}',22,7),(1254,5,3,'{\"Runtime\": \"failure\"}',21,7),(1255,5,3,'{\"Runtime\": 0.10052499999999999}',18,7),(1256,5,3,'{\"Runtime\": 29.745748333333335}',13,7),(1257,5,3,'{\"Runtime\": 0.25115599999999993}',10,7),(1258,11,6,'{\"Runtime\": \">9000\"}',18,10),(1259,5,3,'{\"Runtime\": 1899.2283773333336}',3,7),(1260,5,3,'{\"Runtime\": \"failure\"}',5,7),(1261,5,3,'{\"Runtime\": 0.03994866666666667}',34,8),(1262,5,3,'{\"Runtime\": 2.9978496666666667}',35,8),(1263,5,3,'{\"Runtime\": 255.19372966666666}',36,8),(1264,5,3,'{\"Runtime\": 76.48125699999999}',37,8),(1265,5,3,'{\"Runtime\": 436.1665766666667}',22,8),(1266,5,3,'{\"Runtime\": \"failure\"}',21,8),(1267,5,3,'{\"Runtime\": 0.119681}',18,8),(1268,5,3,'{\"Runtime\": 29.82694433333333}',13,8),(1269,5,3,'{\"Runtime\": 0.29534466666666664}',10,8),(1270,5,3,'{\"Runtime\": 1660.4958233333334}',3,8),(1271,5,3,'{\"Runtime\": \"failure\"}',5,8),(1272,5,3,'{\"Runtime\": 0.047116333333333336}',34,5),(1273,5,3,'{\"Runtime\": 2.990466}',35,5),(1274,5,3,'{\"Runtime\": 255.3148163333333}',36,5),(1275,5,3,'{\"Runtime\": 76.68229766666667}',37,5),(1276,5,3,'{\"Runtime\": 445.9632333333334}',22,5),(1277,5,3,'{\"Runtime\": \"failure\"}',21,5),(1278,5,3,'{\"Runtime\": 0.8275513333333334}',18,5),(1279,5,3,'{\"Runtime\": 49.19137166666667}',13,5),(1280,5,3,'{\"Runtime\": 3.2171726666666665}',10,5),(1281,5,3,'{\"Runtime\": 1802.4754833333334}',3,5),(1282,5,3,'{\"Runtime\": \"failure\"}',5,5),(1283,5,3,'{\"Runtime\": 0.043567999999999996}',34,14),(1284,5,3,'{\"Runtime\": 0.011000333333333332}',35,14),(1285,5,3,'{\"Runtime\": 0.009158000000000001}',36,14),(1286,5,3,'{\"Runtime\": 0.013930333333333336}',37,14),(1287,5,3,'{\"Runtime\": 0.061216666666666676}',22,14),(1288,5,3,'{\"Runtime\": 2.120716}',21,14),(1289,13,6,'{\"Runtime\": 0.023612578709920246}',11,66),(1290,13,6,'{\"Runtime\": 0.0007950464884440104}',12,66),(1291,13,6,'{\"Runtime\": 0.17284321784973145}',13,66),(1292,13,6,'{\"Runtime\": 0.011102040608723959}',1,21),(1293,13,6,'{\"Runtime\": \"failure\"}',2,21),(1294,13,6,'{\"Runtime\": 5.273265520731608}',1,21),(1295,13,6,'{\"Runtime\": 24.723869641621906}',3,21),(1296,13,6,'{\"Runtime\": \"failure\"}',4,21),(1297,5,3,'{\"Runtime\": \">9000\"}',18,14),(1298,5,3,'{\"Runtime\": 4.772985}',13,14),(1299,5,3,'{\"Runtime\": 918.01887}',10,14),(1300,5,3,'{\"Runtime\": 36.388869}',3,14),(1301,5,3,'{\"Runtime\": 13.844534333333334}',5,14),(1302,5,3,'{\"Runtime\": 0.030483333333333335}',34,13),(1303,5,3,'{\"Runtime\": 0.006774000000000002}',35,13),(1304,5,3,'{\"Runtime\": 0.00430433333333333}',36,13),(1305,5,3,'{\"Runtime\": 0.010649666666666668}',37,13),(1306,5,3,'{\"Runtime\": 0.22931233333333334}',22,13),(1307,5,3,'{\"Runtime\": 4.292774666666666}',21,13),(1308,13,6,'{\"Runtime\": \">9000\"}',5,21),(1309,13,6,'{\"Runtime\": \"failure\"}',6,21),(1310,13,6,'{\"Runtime\": 2477.2329931259155}',7,21),(1311,13,6,'{\"Runtime\": \"failure\"}',8,21),(1312,13,6,'{\"Runtime\": \"failure\"}',9,21),(1313,13,6,'{\"Runtime\": 17.400897185007732}',5,52),(1314,13,6,'{\"Runtime\": 0.07454252243041992}',1,52),(1315,13,6,'{\"Runtime\": 0.012268384297688803}',2,50),(1316,13,6,'{\"Runtime\": 0.01094349225362142}',23,56),(1317,13,6,'{\"Runtime\": 0.018361330032348633}',22,51),(1318,13,6,'{\"Runtime\": \"failure\"}',6,54),(1319,13,6,'{\"Runtime\": 15.483943700790405}',7,55),(1320,13,6,'{\"Runtime\": 1.2319941520690918}',4,55),(1321,13,6,'{\"Runtime\": 0.9324228763580322}',3,49),(1322,13,6,'{\"Runtime\": 0.0006636778513590494}',1,53),(1323,13,6,'{\"Runtime\": 0.0005898475646972656}',11,53),(1324,13,6,'{\"Runtime\": 0.028888146082560223}',14,93),(1325,13,6,'{\"Runtime\": 0.0023544629414876304}',14,93),(1326,13,6,'{\"Runtime\": 0.0011588732401529949}',11,93),(1327,13,6,'{\"Runtime\": 0.0015299320220947266}',1,93),(1328,13,6,'{\"Runtime\": 0.006267070770263672}',44,93),(1329,13,6,'{\"Runtime\": \"failure\"}',23,93),(1330,13,6,'{\"Runtime\": \"failure\"}',39,93),(1331,13,6,'{\"Runtime\": \"failure\"}',3,93),(1332,13,6,'{\"Runtime\": \"failure\"}',21,93),(1333,13,6,'{\"Runtime\": \"failure\"}',5,93),(1334,13,6,'{\"Recall\": 0.5, \"ACC\": 0.5, \"Precision\": 0.982371794871795, \"MSE\": 0.035256410256410256, \"Runtime\": 0.019379297892252605, \"MCC\": 0}',24,1),(1335,13,6,'{\"Recall\": 0.5, \"ACC\": 0.5, \"Precision\": 0.9644638403990026, \"MSE\": 0.07107231920199501, \"Runtime\": 1.0103971163431804, \"MCC\": 0}',25,1),(1336,13,6,'{\"Recall\": 0.5, \"ACC\": 0.5, \"Precision\": 0.9854222375571465, \"MSE\": 0.029155524885706895, \"Runtime\": 3.7370476722717285, \"MCC\": 0}',26,1),(1337,5,3,'{\"Runtime\": \">9000\"}',18,13),(1338,5,3,'{\"Runtime\": 17.096282}',13,13),(1339,13,6,'{\"Recall\": 0.5, \"ACC\": 0.5, \"Precision\": 0.9574836475567525, \"MSE\": 0.0850327048864948, \"Runtime\": 18.808923959732056, \"MCC\": 0}',3,1),(1340,13,6,'{\"Recall\": 0.8322830196879482, \"ACC\": 0.8322830196879482, \"Precision\": 0.5526749112956009, \"MSE\": 0.17703862660944206, \"Runtime\": 0.005588372548421224, \"MCC\": 0.264597646150506}',27,1),(1341,5,3,'{\"Runtime\": 2908.894796333334}',10,13),(1342,5,3,'{\"Runtime\": 60.346794666666675}',3,13),(1343,13,6,'{\"Runtime\": \">9000\"}',28,1),(1344,13,6,'{\"Recall\": 0.7229046242774566, \"ACC\": 0.7229046242774566, \"Precision\": 0.5085644782365781, \"MSE\": 0.18031609195402298, \"Runtime\": 0.0026061534881591797, \"MCC\": 0.08738562361068085}',29,1),(1345,13,6,'{\"Recall\": 0.5, \"ACC\": 0.5, \"Precision\": 0.9525616698292221, \"MSE\": 0.094876660341556, \"Runtime\": 0.008319695790608725, \"MCC\": 0}',30,1),(1346,13,6,'{\"Recall\": 0.7521258332519937, \"ACC\": 0.7521258332519937, \"Precision\": 0.5912400265957447, \"MSE\": 0.3771551724137931, \"Runtime\": 0.002578179041544596, \"MCC\": 0.30334117907983543}',31,1),(1347,13,6,'{\"Recall\": 0.7853124617830499, \"ACC\": 0.7853124617830499, \"Precision\": 0.6100795226463511, \"MSE\": 0.3132867132867133, \"Runtime\": 0.013720273971557617, \"MCC\": 0.35444074031145706}',32,1),(1348,13,6,'{\"Recall\": 0.5, \"ACC\": 0.5, \"Precision\": 0.9330357142857143, \"MSE\": 0.13392857142857142, \"Runtime\": 0.00042740503946940106, \"MCC\": 0}',33,1),(1349,5,3,'{\"Runtime\": 24.254837666666663}',5,13),(1350,5,3,'{\"Runtime\": 0.013897}',34,12),(1351,5,3,'{\"Runtime\": 0.010363666666666667}',35,12),(1352,5,3,'{\"Runtime\": 0.014118333333333335}',36,12),(1353,5,3,'{\"Runtime\": 0.013097999999999999}',37,12),(1354,5,3,'{\"Runtime\": 0.13807766666666665}',22,12),(1355,5,3,'{\"Runtime\": 4.019644}',21,12),(1356,13,6,'{\"Runtime\": \">9000\"}',11,63),(1357,5,3,'{\"Runtime\": \">9000\"}',18,12),(1358,5,3,'{\"Runtime\": 9.098335666666665}',13,12),(1359,13,6,'{\"Runtime\": \">9000\"}',24,63),(1360,13,6,'{\"Runtime\": \">9000\"}',25,63),(1361,5,3,'{\"Runtime\": 7253.715756333332}',10,12),(1362,5,3,'{\"Runtime\": 68.310514}',3,12),(1363,5,3,'{\"Runtime\": 226.41439100000002}',5,12),(1364,5,3,'{\"Runtime\": 0.014126666666666668}',34,6),(1365,5,3,'{\"Runtime\": 0.0074533333333333335}',35,6),(1366,5,3,'{\"Runtime\": 0.014623333333333334}',36,6),(1367,5,3,'{\"Runtime\": 0.013372333333333332}',37,6),(1368,5,3,'{\"Runtime\": 0.30426233333333325}',22,6),(1369,5,3,'{\"Runtime\": 7.142696999999999}',21,6),(1370,13,6,'{\"Runtime\": \">9000\"}',26,63),(1371,5,3,'{\"Runtime\": \">9000\"}',18,6),(1372,5,3,'{\"Runtime\": 21.50107}',13,6),(1373,13,6,'{\"Runtime\": \">9000\"}',3,63),(1374,5,3,'{\"Runtime\": 1460.8909533333335}',10,6),(1375,5,3,'{\"Runtime\": 102.51766266666668}',3,6),(1376,5,3,'{\"Runtime\": 43.66262066666667}',5,6),(1377,5,3,'{\"Runtime\": 0.014034}',34,15),(1378,5,3,'{\"Runtime\": 0.0027990000000000007}',35,15),(1379,5,3,'{\"Runtime\": 0.013768333333333334}',36,15),(1380,5,3,'{\"Runtime\": 0.01279366666666667}',37,15),(1381,5,3,'{\"Runtime\": 0.13120033333333334}',22,15),(1382,5,3,'{\"Runtime\": 3.2904200000000006}',21,15),(1383,13,6,'{\"Runtime\": \">9000\"}',27,63),(1384,5,3,'{\"Runtime\": \">9000\"}',18,15),(1385,5,3,'{\"Runtime\": 9.386166333333334}',13,15),(1386,5,3,'{\"Runtime\": 935.81709}',10,15),(1387,5,3,'{\"Runtime\": 69.27624866666666}',3,15),(1388,5,3,'{\"Runtime\": 24.145910333333337}',5,15),(1389,5,3,'{\"Runtime\": 0.013958666666666666}',34,9),(1390,5,3,'{\"Runtime\": 0.007146333333333334}',35,9),(1391,5,3,'{\"Runtime\": 0.008860999999999999}',36,9),(1392,5,3,'{\"Runtime\": 0.013484333333333333}',37,9),(1393,5,3,'{\"Runtime\": 0.05490666666666669}',22,9),(1394,5,3,'{\"Runtime\": 1.4832836666666671}',21,9),(1395,13,6,'{\"Runtime\": \">9000\"}',28,63),(1396,5,3,'{\"Runtime\": \">9000\"}',18,9),(1397,5,3,'{\"Runtime\": 5.065189}',13,9),(1398,5,3,'{\"Runtime\": 889.9102563333335}',10,9),(1399,5,3,'{\"Runtime\": 37.45514633333334}',3,9),(1400,5,3,'{\"Runtime\": 10.614374666666665}',5,9),(1401,5,3,'{\"Runtime\": 0.027850666666666662}',34,16),(1402,5,3,'{\"Runtime\": 2.992438}',35,16),(1403,5,3,'{\"Runtime\": 255.07972266666664}',36,16),(1404,5,3,'{\"Runtime\": 76.60983733333335}',37,16),(1405,13,6,'{\"Runtime\": \">9000\"}',29,63),(1406,5,3,'{\"Runtime\": 435.86077133333333}',22,16),(1407,5,3,'{\"Runtime\": \"failure\"}',21,16),(1408,5,3,'{\"Runtime\": 0.1180343333333333}',18,16),(1409,5,3,'{\"Runtime\": 29.847419000000002}',13,16),(1410,5,3,'{\"Runtime\": 0.289901}',10,16),(1411,13,6,'{\"Runtime\": \">9000\"}',30,63),(1412,5,3,'{\"Runtime\": 1669.1689806666666}',3,16),(1413,5,3,'{\"Runtime\": \"failure\"}',5,16),(1414,5,3,'{\"Runtime\": 5.125465, \"TreeBuilding\": 0.7943076666666666}',39,100),(1415,5,3,'{\"Runtime\": 28.557663, \"TreeBuilding\": 15.234689666666666}',39,98),(1416,5,3,'{\"Runtime\": 4.714094666666667, \"TreeBuilding\": 0.7948209999999999}',39,104),(1417,5,3,'{\"Runtime\": \"failure\"}',39,105),(1418,5,3,'{\"Runtime\": 15.519801333333334, \"TreeBuilding\": 3.4743023333333327}',39,99),(1419,5,3,'{\"Runtime\": 4.117265333333333, \"TreeBuilding\": 0.7943666666666668}',39,103),(1420,5,3,'{\"Runtime\": 23.378679666666667, \"TreeBuilding\": 5.2283919999999995}',39,101),(1421,5,3,'{\"Runtime\": 16.742451, \"TreeBuilding\": 3.0898566666666665}',39,102),(1422,5,3,'{\"Runtime\": 16.807857000000002, \"TreeBuilding\": 5.131073333333333}',39,106),(1423,5,3,'{\"Runtime\": 0.033515333333333334}',11,90),(1424,5,3,'{\"Runtime\": 0.010800333333333334}',1,90),(1425,5,3,'{\"Runtime\": 0.012626333333333337}',40,90),(1426,5,3,'{\"Runtime\": 0.014579333333333335}',15,90),(1427,5,3,'{\"Runtime\": \"failure\"}',14,32),(1428,5,3,'{\"HashBuilding\": 0.004113333333333333, \"Runtime\": 0.005466333333333333}',1,38),(1429,5,3,'{\"HashBuilding\": 0.029172666666666666, \"Runtime\": 0.037847}',2,38),(1430,5,3,'{\"HashBuilding\": 0.08136766666666667, \"Runtime\": 0.11270433333333334}',1,38),(1431,5,3,'{\"HashBuilding\": 1.9324763333333335, \"Runtime\": 2.484366}',3,38),(1432,5,3,'{\"HashBuilding\": 1.2484853333333332, \"Runtime\": 13.549644666666666}',4,38),(1433,5,3,'{\"HashBuilding\": 18.811736, \"Runtime\": 200.88544133333335}',5,38),(1434,5,3,'{\"HashBuilding\": 15.656649666666667, \"Runtime\": 309.96256999999997}',6,38),(1435,5,3,'{\"HashBuilding\": 21.990352333333334, \"Runtime\": 38.049074}',7,38),(1436,5,3,'{\"HashBuilding\": 25.326377666666662, \"Runtime\": 298.99260599999997}',8,38),(1437,5,3,'{\"HashBuilding\": 15.937241, \"Runtime\": 55.86734666666667}',9,38),(1438,5,3,'{\"ComputingNeighbors\": 0.000624, \"Runtime\": 0.010306, \"TreeBuilding\": 0.000312, \"BaseCases\": 3087}',1,27),(1439,5,3,'{\"ComputingNeighbors\": 0.006805666666666666, \"Runtime\": 0.025630333333333335, \"TreeBuilding\": 0.002335, \"BaseCases\": 82022}',2,27),(1440,5,3,'{\"ComputingNeighbors\": 0.029113666666666666, \"Runtime\": 0.045664333333333335, \"TreeBuilding\": 0.008977333333333332, \"BaseCases\": 281697}',1,27),(1441,5,3,'{\"ComputingNeighbors\": 15.231391666666667, \"Runtime\": 15.744250999999998, \"TreeBuilding\": 0.48860666666666663, \"BaseCases\": 11316977}',3,27),(1442,5,3,'{\"ComputingNeighbors\": 10.255347333333333, \"Runtime\": 11.315643, \"TreeBuilding\": 1.029148, \"BaseCases\": 26935901}',4,27),(1443,5,3,'{\"ComputingNeighbors\": 9.030868333333334, \"Runtime\": 14.141510333333331, \"TreeBuilding\": 4.984001, \"BaseCases\": 29090095}',5,27),(1444,5,3,'{\"ComputingNeighbors\": 952.663799, \"Runtime\": 954.6860216666668, \"TreeBuilding\": 1.8062666666666667, \"BaseCases\": 2731283746}',6,27),(1445,13,6,'{\"Runtime\": \">9000\"}',31,63),(1446,5,3,'{\"ComputingNeighbors\": 3256.1807483333337, \"Runtime\": 3301.7924719999996, \"TreeBuilding\": 45.54730466666667, \"BaseCases\": 2453954001}',7,27),(1447,13,6,'{\"Runtime\": \">9000\"}',32,63),(1448,5,3,'{\"ComputingNeighbors\": 1087.5465203333333, \"Runtime\": 1142.2821776666667, \"TreeBuilding\": 54.57274833333333, \"BaseCases\": 1846538229}',8,27),(1449,13,6,'{\"Runtime\": \">9000\"}',33,63),(1450,13,6,'{\"Runtime\": \"failure\"}',11,70),(1451,13,6,'{\"Runtime\": \"failure\"}',39,70),(1452,13,6,'{\"Runtime\": \"failure\"}',13,70),(1453,13,6,'{\"Runtime\": \"failure\"}',18,70),(1454,13,6,'{\"Runtime\": 0.01785588264465332}',11,45),(1455,13,6,'{\"Runtime\": 0.02743188540140788}',24,45),(1456,13,6,'{\"Runtime\": 1.1358323097229004}',25,45),(1457,13,6,'{\"Runtime\": 25.347962458928425}',26,45),(1458,13,6,'{\"Runtime\": 4.466108481089274}',3,45),(1459,13,6,'{\"Runtime\": 0.11121654510498047}',27,45),(1460,13,6,'{\"Runtime\": 183.31933315594992}',28,45),(1461,13,6,'{\"Runtime\": 0.04683828353881836}',29,45),(1462,13,6,'{\"Runtime\": 0.061398585637410484}',30,45),(1463,13,6,'{\"Runtime\": 0.04914387067159017}',31,45),(1464,13,6,'{\"Runtime\": 0.15038180351257324}',32,45),(1465,13,6,'{\"Runtime\": 0.0018566449483235676}',33,45),(1466,5,3,'{\"ComputingNeighbors\": 5027.515376333334, \"Runtime\": 5064.8351059999995, \"TreeBuilding\": 37.255443666666665, \"BaseCases\": 7754919823}',9,27),(1467,5,3,'{\"ComputingNeighbors\": 0.0003, \"Runtime\": 0.03408733333333334, \"TreeBuilding\": 0.00016366666666666667, \"BaseCases\": 3138}',1,22),(1468,5,3,'{\"ComputingNeighbors\": 0.007153333333333334, \"Runtime\": 0.016035666666666667, \"TreeBuilding\": 0.00231, \"BaseCases\": 91368}',2,22),(1469,5,3,'{\"ComputingNeighbors\": 0.029990666666666665, \"Runtime\": 0.04031933333333333, \"TreeBuilding\": 0.008971999999999999, \"BaseCases\": 319392}',1,22),(1470,5,3,'{\"ComputingNeighbors\": 17.916535666666665, \"Runtime\": 18.42874733333333, \"TreeBuilding\": 0.48878266666666664, \"BaseCases\": 14197711}',3,22),(1471,5,3,'{\"ComputingNeighbors\": 12.065448333333334, \"Runtime\": 13.125766333333333, \"TreeBuilding\": 1.0291759999999999, \"BaseCases\": 36510060}',4,22),(1472,5,3,'{\"ComputingNeighbors\": 9.741029, \"Runtime\": 14.86290666666667, \"TreeBuilding\": 4.992075333333333, \"BaseCases\": 32750435}',5,22),(1473,5,3,'{\"ComputingNeighbors\": 1228.9747653333334, \"Runtime\": 1230.9966150000002, \"TreeBuilding\": 1.807319, \"BaseCases\": 3932684997}',6,22),(1474,5,3,'{\"ComputingNeighbors\": 3578.8829659999997, \"Runtime\": 3624.8277326666666, \"TreeBuilding\": 45.879056, \"BaseCases\": 2869704944}',7,22),(1475,5,3,'{\"ComputingNeighbors\": 1286.5427636666666, \"Runtime\": 1341.6833576666666, \"TreeBuilding\": 54.967000000000006, \"BaseCases\": 2541594775}',8,22),(1476,13,6,'{\"Runtime\": -2}',11,75),(1477,5,3,'{\"ComputingNeighbors\": 5213.810644333334, \"Runtime\": 5251.317342666666, \"TreeBuilding\": 37.44221966666667, \"BaseCases\": 8241031045}',9,22),(1478,5,3,'{\"ComputingNeighbors\": 0.0003, \"Runtime\": 0.027422666666666668, \"TreeBuilding\": 0.00015966666666666665, \"BaseCases\": 3138}',1,17),(1479,5,3,'{\"ComputingNeighbors\": 0.007203, \"Runtime\": 0.015451, \"TreeBuilding\": 0.0023213333333333337, \"BaseCases\": 91368}',2,17),(1480,5,3,'{\"ComputingNeighbors\": 0.02976366666666667, \"Runtime\": 0.04479833333333333, \"TreeBuilding\": 0.008987333333333333, \"BaseCases\": 319392}',1,17),(1481,5,3,'{\"ComputingNeighbors\": 17.912888, \"Runtime\": 18.431867666666665, \"TreeBuilding\": 0.4894343333333333, \"BaseCases\": 14197711}',3,17),(1482,5,3,'{\"ComputingNeighbors\": 12.089398666666668, \"Runtime\": 13.146563, \"TreeBuilding\": 1.0287903333333333, \"BaseCases\": 36510060}',4,17),(1483,5,3,'{\"ComputingNeighbors\": 9.774981666666667, \"Runtime\": 14.910175, \"TreeBuilding\": 5.002305666666666, \"BaseCases\": 32750435}',5,17),(1484,5,3,'{\"ComputingNeighbors\": 1231.3443346666666, \"Runtime\": 1233.3712836666666, \"TreeBuilding\": 1.8088406666666665, \"BaseCases\": 3932684997}',6,17),(1485,13,6,'{\"Runtime\": -2}',24,75),(1486,5,3,'{\"ComputingNeighbors\": 3575.5381686666665, \"Runtime\": 3621.5306450000003, \"TreeBuilding\": 45.93521766666667, \"BaseCases\": 2869704944}',7,17),(1487,5,3,'{\"ComputingNeighbors\": 1271.139989666667, \"Runtime\": 1326.279593, \"TreeBuilding\": 54.99194166666667, \"BaseCases\": 2541594775}',8,17),(1488,5,3,'{\"ComputingNeighbors\": 5214.805240666667, \"Runtime\": 5252.446343, \"TreeBuilding\": 37.58299566666667, \"BaseCases\": 8241031045}',9,17),(1489,5,3,'{}',1,18),(1490,5,3,'{}',2,18),(1491,5,3,'{}',1,18),(1492,5,3,'{}',3,18),(1493,5,3,'{}',4,18),(1494,13,6,'{\"Runtime\": -2}',25,75),(1495,5,3,'{\"Runtime\": \">9000\"}',5,18),(1496,5,3,'{\"Runtime\": \">9000\"}',6,18),(1497,13,6,'{\"Runtime\": -2}',26,75),(1498,5,3,'{}',7,18),(1499,5,3,'{\"Runtime\": \">9000\"}',8,18),(1500,5,3,'{}',9,18),(1501,5,3,'{\"ComputingNeighbors\": 0.0011623333333333331, \"Runtime\": 0.017147333333333334, \"TreeBuilding\": 0.0002776666666666667, \"BaseCases\": 2725}',1,19),(1502,5,3,'{\"ComputingNeighbors\": 0.02907433333333333, \"Runtime\": 0.04354066666666667, \"TreeBuilding\": 0.005371666666666667, \"BaseCases\": 128056}',2,19),(1503,5,3,'{\"ComputingNeighbors\": 0.13676, \"Runtime\": 0.17664500000000002, \"TreeBuilding\": 0.02875866666666667, \"BaseCases\": 593447}',1,19),(1504,5,3,'{\"ComputingNeighbors\": 27.87214133333333, \"Runtime\": 40.514510666666666, \"TreeBuilding\": 12.618848999999999, \"BaseCases\": 27937223}',3,19),(1505,5,3,'{\"ComputingNeighbors\": 96.222193, \"Runtime\": 118.67040666666666, \"TreeBuilding\": 22.408519, \"BaseCases\": 281323326}',4,19),(1506,5,3,'{\"ComputingNeighbors\": 56.36114666666666, \"Runtime\": 181.081765, \"TreeBuilding\": 124.55894333333333, \"BaseCases\": 161975455}',5,19),(1507,13,6,'{\"Runtime\": -2}',3,75),(1508,5,3,'{\"Runtime\": \">9000\"}',6,19),(1509,5,3,'{\"ComputingNeighbors\": 3314.2471176666663, \"Runtime\": 4774.056708, \"TreeBuilding\": 1459.7464746666665, \"BaseCases\": 2737482185}',7,19),(1510,13,6,'{\"Runtime\": -2}',27,75),(1511,5,3,'{\"ComputingNeighbors\": 4723.957283333333, \"Runtime\": 5935.174709333333, \"TreeBuilding\": 1211.0654673333336, \"BaseCases\": 7930285337}',8,19),(1512,13,6,'{\"Runtime\": -2}',28,75),(1513,5,3,'{\"ComputingNeighbors\": 5924.855426333333, \"Runtime\": 8277.986005, \"TreeBuilding\": 2353.080898333333, \"BaseCases\": 8320081059}',9,19),(1514,5,3,'{\"ComputingNeighbors\": 0.0003313333333333334, \"Runtime\": 0.014662333333333333, \"TreeBuilding\": 0.00016266666666666667, \"BaseCases\": 3194}',1,20),(1515,5,3,'{\"ComputingNeighbors\": 0.007982999999999999, \"Runtime\": 0.015802, \"TreeBuilding\": 0.0023396666666666665, \"BaseCases\": 101428}',2,20),(1516,5,3,'{\"ComputingNeighbors\": 0.032986999999999995, \"Runtime\": 0.050566, \"TreeBuilding\": 0.008960333333333334, \"BaseCases\": 359800}',1,20),(1517,5,3,'{\"ComputingNeighbors\": 18.49124433333333, \"Runtime\": 18.997701333333335, \"TreeBuilding\": 0.4890296666666667, \"BaseCases\": 14841413}',3,20),(1518,5,3,'{\"ComputingNeighbors\": 12.938759, \"Runtime\": 14.009334333333333, \"TreeBuilding\": 1.0286193333333333, \"BaseCases\": 41002019}',4,20),(1519,5,3,'{\"ComputingNeighbors\": 10.898873333333334, \"Runtime\": 16.039210333333333, \"TreeBuilding\": 4.994361333333334, \"BaseCases\": 37624410}',5,20),(1520,5,3,'{\"ComputingNeighbors\": 1320.6309793333332, \"Runtime\": 1322.6415303333333, \"TreeBuilding\": 1.810713, \"BaseCases\": 4504040034}',6,20),(1521,5,3,'{\"ComputingNeighbors\": 3659.7388140000003, \"Runtime\": 3705.515489666667, \"TreeBuilding\": 45.71343233333334, \"BaseCases\": 2975132390}',7,20),(1522,13,6,'{\"Runtime\": -2}',29,75),(1523,5,3,'{\"ComputingNeighbors\": 1348.0521420000002, \"Runtime\": 1403.1031176666665, \"TreeBuilding\": 54.88344866666667, \"BaseCases\": 2789550688}',8,20),(1524,5,3,'{\"ComputingNeighbors\": 5234.919498, \"Runtime\": 5272.449084666667, \"TreeBuilding\": 37.465577, \"BaseCases\": 8291912889}',9,20),(1525,5,3,'{\"ComputingNeighbors\": 0.0003, \"Runtime\": 0.033281, \"TreeBuilding\": 0.00016233333333333334, \"BaseCases\": 3087}',1,25),(1526,5,3,'{\"ComputingNeighbors\": 0.007079666666666667, \"Runtime\": 0.010316, \"TreeBuilding\": 0.0023076666666666666, \"BaseCases\": 86586}',2,25),(1527,5,3,'{\"ComputingNeighbors\": 0.029437333333333333, \"Runtime\": 0.04511600000000001, \"TreeBuilding\": 0.009001666666666666, \"BaseCases\": 299100}',1,25),(1528,5,3,'{\"ComputingNeighbors\": 16.487289999999998, \"Runtime\": 17.006632666666665, \"TreeBuilding\": 0.489674, \"BaseCases\": 12648722}',3,25),(1529,5,3,'{\"ComputingNeighbors\": 11.094302333333331, \"Runtime\": 12.160586, \"TreeBuilding\": 1.029067, \"BaseCases\": 31194903}',4,25),(1530,5,3,'{\"ComputingNeighbors\": 9.415940666666666, \"Runtime\": 14.544687999999999, \"TreeBuilding\": 5.005696333333334, \"BaseCases\": 30789926}',5,25),(1531,5,3,'{\"ComputingNeighbors\": 1118.0469373333333, \"Runtime\": 1120.0482273333334, \"TreeBuilding\": 1.8108373333333334, \"BaseCases\": 3259849974}',6,25),(1532,13,6,'{\"Runtime\": -2}',30,75),(1533,5,3,'{\"ComputingNeighbors\": 3415.8262646666667, \"Runtime\": 3461.856264, \"TreeBuilding\": 45.96897066666667, \"BaseCases\": 2658802890}',7,25),(1534,5,3,'{\"ComputingNeighbors\": 1170.7566086666666, \"Runtime\": 1226.0431993333334, \"TreeBuilding\": 55.147245, \"BaseCases\": 2155931120}',8,25),(1535,5,3,'{\"ComputingNeighbors\": 5151.721157333333, \"Runtime\": 5189.567893333334, \"TreeBuilding\": 37.790079999999996, \"BaseCases\": 8003118069}',9,25),(1536,5,3,'{\"ComputingNeighbors\": 0.000373, \"Runtime\": 0.026726333333333335, \"TreeBuilding\": 0.00016233333333333334, \"BaseCases\": 3138}',1,23),(1537,5,3,'{\"ComputingNeighbors\": 0.008513666666666668, \"Runtime\": 0.01776366666666666, \"TreeBuilding\": 0.0023530000000000005, \"BaseCases\": 90177}',2,23),(1538,5,3,'{\"ComputingNeighbors\": 0.033634000000000004, \"Runtime\": 0.044065, \"TreeBuilding\": 0.008986666666666665, \"BaseCases\": 315619}',1,23),(1539,5,3,'{\"ComputingNeighbors\": 23.489308000000005, \"Runtime\": 24.001252333333337, \"TreeBuilding\": 0.49061400000000005, \"BaseCases\": 14189461}',3,23),(1540,5,3,'{\"ComputingNeighbors\": 7.977449, \"Runtime\": 9.035563999999999, \"TreeBuilding\": 1.0282506666666666, \"BaseCases\": 35963565}',4,23),(1541,5,3,'{\"ComputingNeighbors\": 10.762019333333333, \"Runtime\": 15.907010333333332, \"TreeBuilding\": 5.016584333333333, \"BaseCases\": 32146240}',5,23),(1542,5,3,'{\"ComputingNeighbors\": 282.42863566666665, \"Runtime\": 284.43503833333335, \"TreeBuilding\": 1.8150646666666665, \"BaseCases\": 3837480952}',6,23),(1543,13,6,'{\"Runtime\": -2}',31,75),(1544,5,3,'{\"ComputingNeighbors\": 4693.854387333334, \"Runtime\": 4740.061126333333, \"TreeBuilding\": 46.13957133333333, \"BaseCases\": 2851648317}',7,23),(1545,5,3,'{\"ComputingNeighbors\": 690.782674, \"Runtime\": 746.2952543333332, \"TreeBuilding\": 55.34962366666667, \"BaseCases\": 2493998410}',8,23),(1546,13,6,'{\"Runtime\": -2}',32,75),(1547,5,3,'{\"ComputingNeighbors\": 7014.587161, \"Runtime\": 7052.518480333333, \"TreeBuilding\": 37.86230666666667, \"BaseCases\": 8217322482}',9,23),(1548,5,3,'{\"ComputingNeighbors\": 0.0002933333333333333, \"Runtime\": 0.01258, \"TreeBuilding\": 0.00017866666666666668, \"BaseCases\": 2963}',1,28),(1549,5,3,'{\"ComputingNeighbors\": 0.006338333333333334, \"Runtime\": 0.020134, \"TreeBuilding\": 0.0023813333333333334, \"BaseCases\": 72434}',2,28),(1550,5,3,'{\"ComputingNeighbors\": 0.026296999999999997, \"Runtime\": 0.041055, \"TreeBuilding\": 0.009018333333333331, \"BaseCases\": 239981}',1,28),(1551,5,3,'{\"ComputingNeighbors\": 12.221708, \"Runtime\": 12.733825666666666, \"TreeBuilding\": 0.48999299999999996, \"BaseCases\": 8326193}',3,28),(1552,5,3,'{\"ComputingNeighbors\": 7.849158333333333, \"Runtime\": 8.901194666666667, \"TreeBuilding\": 1.0281813333333334, \"BaseCases\": 18355995}',4,28),(1553,5,3,'{\"ComputingNeighbors\": 8.147018000000001, \"Runtime\": 13.309779999999998, \"TreeBuilding\": 5.020413666666667, \"BaseCases\": 25219845}',5,28),(1554,5,3,'{\"ComputingNeighbors\": 567.4267356666668, \"Runtime\": 569.4749346666666, \"TreeBuilding\": 1.8158046666666667, \"BaseCases\": 1700887285}',6,28),(1555,5,3,'{\"ComputingNeighbors\": 2819.2826383333336, \"Runtime\": 2865.5265883333336, \"TreeBuilding\": 46.17771333333334, \"BaseCases\": 1894948927}',7,28),(1556,5,3,'{\"ComputingNeighbors\": 879.1959436666666, \"Runtime\": 934.7358623333334, \"TreeBuilding\": 55.376201, \"BaseCases\": 1210647432}',8,28),(1557,13,6,'{\"Runtime\": -2}',33,75),(1558,13,6,'{\"Runtime\": \">9000\"}',33,71),(1559,5,3,'{\"ComputingNeighbors\": 4715.703582333333, \"Runtime\": 4753.740232333334, \"TreeBuilding\": 37.97260933333333, \"BaseCases\": 6965803590}',9,28),(1560,5,3,'{\"ComputingNeighbors\": 0.000296, \"Runtime\": 0.019310666666666667, \"TreeBuilding\": 0.000166, \"BaseCases\": 3016}',1,29),(1561,5,3,'{\"ComputingNeighbors\": 0.006488000000000001, \"Runtime\": 0.013756666666666667, \"TreeBuilding\": 0.0023076666666666666, \"BaseCases\": 75278}',2,29),(1562,5,3,'{\"ComputingNeighbors\": 0.02699966666666667, \"Runtime\": 0.043450666666666665, \"TreeBuilding\": 0.008994333333333333, \"BaseCases\": 252384}',1,29),(1563,5,3,'{\"ComputingNeighbors\": 13.125661666666666, \"Runtime\": 13.638524333333335, \"TreeBuilding\": 0.48986366666666664, \"BaseCases\": 9183443}',3,29),(1564,5,3,'{\"ComputingNeighbors\": 8.480101, \"Runtime\": 9.534239999999999, \"TreeBuilding\": 1.0288226666666667, \"BaseCases\": 20690308}',4,29),(1565,5,3,'{\"ComputingNeighbors\": 8.417014, \"Runtime\": 13.577487, \"TreeBuilding\": 5.022265666666667, \"BaseCases\": 26335183}',5,29),(1566,5,3,'{\"ComputingNeighbors\": 664.6820106666667, \"Runtime\": 666.7080966666667, \"TreeBuilding\": 1.814362, \"BaseCases\": 1973922611}',6,29),(1567,13,6,'{\"Runtime\": \">9000\"}',27,71),(1568,5,3,'{\"ComputingNeighbors\": 2957.4506913333335, \"Runtime\": 3003.710690666667, \"TreeBuilding\": 46.19070966666666, \"BaseCases\": 2070459873}',7,29),(1569,5,3,'{\"ComputingNeighbors\": 935.9242093333334, \"Runtime\": 991.489911, \"TreeBuilding\": 55.41953666666666, \"BaseCases\": 1384412300}',8,29),(1570,13,6,'{\"Runtime\": \">9000\"}',30,73),(1571,13,6,'{\"Runtime\": \">9000\"}',26,73),(1572,5,3,'{\"ComputingNeighbors\": 4822.895855333333, \"Runtime\": 4860.950362333333, \"TreeBuilding\": 37.98981566666667, \"BaseCases\": 7234653139}',9,29),(1573,5,3,'{\"ComputingNeighbors\": 0.001706, \"Runtime\": 0.017753333333333333, \"TreeBuilding\": 0.00026033333333333334, \"BaseCases\": 4545}',1,26),(1574,5,3,'{\"ComputingNeighbors\": 0.05191300000000001, \"Runtime\": 0.07033066666666667, \"TreeBuilding\": 0.005386666666666667, \"BaseCases\": 221952}',2,26),(1575,5,3,'{\"ComputingNeighbors\": 0.294093, \"Runtime\": 0.3360526666666666, \"TreeBuilding\": 0.028810666666666665, \"BaseCases\": 1270285}',1,26),(1576,5,3,'{\"ComputingNeighbors\": 33.38364933333333, \"Runtime\": 46.07113533333333, \"TreeBuilding\": 12.658582666666666, \"BaseCases\": 35377796}',3,26),(1577,5,3,'{\"ComputingNeighbors\": 164.75112900000002, \"Runtime\": 185.88233300000002, \"TreeBuilding\": 21.113136, \"BaseCases\": 542875068}',4,26),(1578,5,3,'{\"ComputingNeighbors\": 201.90075233333334, \"Runtime\": 326.87743066666667, \"TreeBuilding\": 124.83091833333333, \"BaseCases\": 589569405}',5,26),(1579,13,6,'{\"Runtime\": \">9000\"}',31,72),(1580,5,3,'{\"Runtime\": \">9000\"}',6,26),(1581,13,6,'{\"Runtime\": \">9000\"}',29,72),(1582,13,6,'{\"Runtime\": \">9000\"}',34,16),(1583,5,3,'{\"ComputingNeighbors\": 4338.062061, \"Runtime\": 5797.540066333332, \"TreeBuilding\": 1459.416278, \"BaseCases\": 3659647125}',7,26),(1584,13,6,'{\"Runtime\": \">9000\"}',35,16),(1585,5,3,'{\"Runtime\": \">9000\"}',8,26),(1586,13,6,'{\"Runtime\": \">9000\"}',36,16),(1587,13,6,'{\"Runtime\": \">9000\"}',37,16),(1588,13,6,'{\"Runtime\": \">9000\"}',22,16),(1589,5,3,'{\"ComputingNeighbors\": 6403.413718, \"Runtime\": 8756.971425666665, \"TreeBuilding\": 2353.5013596666668, \"BaseCases\": 9362812333}',9,26),(1590,5,3,'{\"ComputingNeighbors\": 0.00029633333333333334, \"Runtime\": 0.014547666666666667, \"TreeBuilding\": 0.00016366666666666667, \"BaseCases\": 3047}',1,24),(1591,5,3,'{\"ComputingNeighbors\": 0.006643666666666667, \"Runtime\": 0.015216333333333332, \"TreeBuilding\": 0.0023203333333333335, \"BaseCases\": 78384}',2,24),(1592,5,3,'{\"ComputingNeighbors\": 0.027671, \"Runtime\": 0.04332133333333332, \"TreeBuilding\": 0.008960666666666667, \"BaseCases\": 266021}',1,24),(1593,5,3,'{\"ComputingNeighbors\": 14.117296000000001, \"Runtime\": 14.628791666666665, \"TreeBuilding\": 0.49038133333333334, \"BaseCases\": 10176459}',3,24),(1594,5,3,'{\"ComputingNeighbors\": 9.204171666666666, \"Runtime\": 10.249144333333332, \"TreeBuilding\": 1.0279853333333333, \"BaseCases\": 23498350}',4,24),(1595,5,3,'{\"ComputingNeighbors\": 8.708444333333333, \"Runtime\": 13.869186999999998, \"TreeBuilding\": 5.025575, \"BaseCases\": 27614934}',5,24),(1596,5,3,'{\"ComputingNeighbors\": 806.2939543333333, \"Runtime\": 808.3049629999999, \"TreeBuilding\": 1.814313, \"BaseCases\": 2311149889}',6,24),(1597,13,6,'{\"Runtime\": \">9000\"}',21,16),(1598,5,3,'{\"ComputingNeighbors\": 3103.160244, \"Runtime\": 3149.4160663333337, \"TreeBuilding\": 46.20003566666666, \"BaseCases\": 2257227119}',7,24),(1599,13,6,'{\"Runtime\": \">9000\"}',18,16),(1600,5,3,'{\"ComputingNeighbors\": 1002.3774726666667, \"Runtime\": 1057.880082, \"TreeBuilding\": 55.35193866666666, \"BaseCases\": 1593351863}',8,24),(1601,13,6,'{\"Runtime\": \">9000\"}',13,16),(1602,5,3,'{\"ComputingNeighbors\": 4926.979274, \"Runtime\": 4965.015393999999, \"TreeBuilding\": 37.967949, \"BaseCases\": 7498187466}',9,24),(1603,5,3,'{\"Runtime\": 25.571956999999998}',10,96),(1604,5,3,'{\"Runtime\": 19.965492666666666}',10,95),(1605,5,3,'{\"Runtime\": 0.0053286666666666664}',23,56),(1606,13,6,'{\"Runtime\": \">9000\"}',10,16),(1607,5,3,'{\"Runtime\": 1476.5954003333334}',6,54),(1608,5,3,'{\"Runtime\": 0.011810666666666665}',22,51),(1609,5,3,'{\"Runtime\": 31.073694666666665}',5,52),(1610,5,3,'{\"Runtime\": 0.052941999999999996}',1,52),(1611,5,3,'{\"Runtime\": 0.000107}',1,53),(1612,5,3,'{\"Runtime\": 8.366666666666668e-05}',11,53),(1613,5,3,'{\"Runtime\": 4.410556}',3,49),(1614,5,3,'{\"Runtime\": 61.218435666666664}',7,55),(1615,5,3,'{\"Runtime\": 1.9407156666666667}',4,55),(1616,5,3,'{\"Runtime\": 0.00622}',2,50),(1617,5,3,'{\"Runtime\": 0.006390666666666667}',15,62),(1618,5,3,'{\"Runtime\": 0.13016833333333333}',20,62),(1619,5,3,'{\"Runtime\": \"failure\"}',7,62),(1620,5,3,'{\"Runtime\": \"failure\"}',9,62),(1621,5,3,'{\"Runtime\": 8.804967}',43,62),(1622,5,3,'{\"Runtime\": 239.37975633333335}',21,62),(1623,5,3,'{\"Runtime\": 5.906421666666667}',13,62),(1624,5,3,'{\"Runtime\": 0.39212766666666665}',18,62),(1625,5,3,'{\"Runtime\": 0.0021503333333333335, \"TreeBuilding\": 0.0006039999999999999}',11,3),(1626,5,3,'{\"Runtime\": 0.025636999999999997, \"TreeBuilding\": 0.0035429999999999997}',44,3),(1627,5,3,'{\"Runtime\": 19.336085666666666, \"TreeBuilding\": 0.037158}',22,3),(1628,5,3,'{\"Runtime\": 42.123149000000005, \"TreeBuilding\": 1.24134}',4,3),(1629,5,3,'{\"Runtime\": 142.21500433333333, \"TreeBuilding\": 0.7641186666666666}',3,3),(1630,13,6,'{\"Runtime\": \">9000\"}',3,16),(1631,13,6,'{\"Runtime\": \"failure\"}',5,16),(1632,5,3,'{\"Runtime\": \">9000\"}',9,3),(1633,5,3,'{}',11,4),(1634,5,3,'{}',44,4),(1635,5,3,'{}',22,4),(1636,13,6,'{\"Runtime\": \">9000\"}',34,10),(1637,13,6,'{\"Runtime\": \">9000\"}',35,10),(1638,13,6,'{\"Runtime\": \">9000\"}',36,10),(1639,13,6,'{\"Runtime\": \">9000\"}',37,10),(1640,13,6,'{\"Runtime\": \">9000\"}',22,10),(1641,13,6,'{\"Runtime\": \">9000\"}',21,10),(1642,13,6,'{\"Runtime\": \">9000\"}',18,10),(1643,13,6,'{\"Runtime\": \">9000\"}',13,10),(1644,13,6,'{\"Runtime\": \">9000\"}',10,10),(1645,13,6,'{\"Runtime\": \">9000\"}',3,10),(1646,13,6,'{\"Runtime\": \"failure\"}',5,10),(1647,13,6,'{\"Runtime\": \">9000\"}',34,8),(1648,13,6,'{\"Runtime\": \">9000\"}',35,8),(1649,13,6,'{\"Runtime\": \">9000\"}',36,8),(1650,13,6,'{\"Runtime\": \">9000\"}',37,8),(1651,13,6,'{\"Runtime\": \">9000\"}',22,8),(1652,13,6,'{\"Runtime\": \">9000\"}',21,8),(1653,13,6,'{\"Runtime\": \">9000\"}',18,8),(1654,13,6,'{\"Runtime\": \">9000\"}',13,8),(1655,13,6,'{\"Runtime\": \">9000\"}',10,8),(1656,13,6,'{\"Runtime\": \">9000\"}',3,8),(1657,13,6,'{\"Runtime\": \"failure\"}',5,8),(1658,13,6,'{\"Runtime\": \">9000\"}',34,11),(1659,13,6,'{\"Runtime\": \">9000\"}',35,11),(1660,13,6,'{\"Runtime\": \">9000\"}',36,11),(1661,13,6,'{\"Runtime\": \">9000\"}',37,11),(1662,13,6,'{\"Runtime\": \">9000\"}',22,11),(1663,13,6,'{\"Runtime\": \">9000\"}',21,11),(1664,13,6,'{\"Runtime\": \">9000\"}',18,11),(1665,13,6,'{\"Runtime\": \">9000\"}',13,11),(1666,13,6,'{\"Runtime\": \">9000\"}',10,11),(1667,13,6,'{\"Runtime\": \">9000\"}',3,11),(1668,13,6,'{\"Runtime\": \">9000\"}',5,11),(1669,13,6,'{\"Runtime\": \">9000\"}',15,40),(1670,13,6,'{\"Runtime\": \">9000\"}',20,40),(1671,13,6,'{\"Runtime\": \">9000\"}',21,40),(1672,13,6,'{\"Runtime\": \">9000\"}',15,62),(1673,13,6,'{\"Runtime\": \">9000\"}',20,62),(1674,13,6,'{\"Runtime\": \">9000\"}',7,62),(1675,13,6,'{\"Runtime\": \">9000\"}',9,62),(1676,13,6,'{\"Runtime\": \">9000\"}',43,62),(1677,13,6,'{\"Runtime\": \">9000\"}',21,62),(1678,13,6,'{\"Runtime\": \">9000\"}',13,62),(1679,13,6,'{\"Runtime\": \">9000\"}',18,62),(1680,13,6,'{\"Runtime\": \">9000\"}',15,65),(1681,13,6,'{\"Runtime\": \">9000\"}',20,65),(1682,13,6,'{\"Runtime\": \">9000\"}',18,65),(1683,13,6,'{\"Runtime\": \">9000\"}',13,65),(1684,13,6,'{\"Runtime\": \">9000\"}',24,74),(1685,13,6,'{\"Runtime\": \">9000\"}',25,74),(1686,13,6,'{\"Runtime\": \">9000\"}',26,74),(1687,13,6,'{\"Runtime\": \">9000\"}',3,74),(1688,13,6,'{\"Runtime\": \">9000\"}',27,74),(1689,13,6,'{\"Runtime\": \">9000\"}',28,74),(1690,13,6,'{\"Runtime\": \">9000\"}',29,74),(1691,13,6,'{\"Runtime\": \">9000\"}',30,74),(1692,13,6,'{\"Runtime\": \">9000\"}',31,74),(1693,13,6,'{\"Runtime\": \">9000\"}',32,74),(1694,13,6,'{\"Runtime\": \">9000\"}',33,74),(1695,13,6,'{\"Runtime\": 0.014387289683024088}',11,90),(1696,13,6,'{\"Runtime\": 0.0010356903076171875}',1,90),(1697,13,6,'{\"Runtime\": 0.0010620752970377605}',40,90),(1698,13,6,'{\"Runtime\": 0.001095136006673177}',15,90),(1699,13,6,'{\"Runtime\": \"failure\"}',14,90),(1700,13,6,'{\"Runtime\": 0.0018267631530761719}',16,90),(1701,13,6,'{\"Runtime\": 0.00418861707051595}',37,90),(1702,13,6,'{\"Runtime\": 0.009633938471476236}',41,90),(1703,13,6,'{\"Runtime\": 0.017947991689046223}',17,90),(1704,13,6,'{\"Runtime\": \">9000\"}',42,90),(1705,13,6,'{\"Runtime\": \"failure\"}',18,90),(1706,13,6,'{\"Runtime\": \">9000\"}',13,90),(1707,13,6,'{\"Runtime\": 0.1368549664815267}',4,90),(1708,13,6,'{\"Runtime\": \">9000\"}',3,90),(1709,13,6,'{\"Runtime\": \">9000\"}',7,90),(1710,13,6,'{\"Runtime\": \">9000\"}',8,90),(1711,13,6,'{\"Runtime\": \">9000\"}',9,90),(1712,13,6,'{\"Runtime\": \">9000\"}',19,90),(1713,13,6,'{\"Runtime\": 0.0009065469106038412}',11,92),(1714,13,6,'{\"Runtime\": 0.0009902318318684895}',1,92),(1715,13,6,'{\"Runtime\": 0.0009719530741373698}',40,92),(1716,13,6,'{\"Runtime\": 0.0009965896606445312}',15,92),(1717,13,6,'{\"Runtime\": 0.005140384038289388}',14,92),(1718,13,6,'{\"Runtime\": 0.0015052954355875652}',16,92),(1719,13,6,'{\"Runtime\": 0.0029946168263753257}',37,92),(1720,13,6,'{\"Runtime\": 0.005845864613850911}',41,92),(1721,13,6,'{\"Runtime\": 0.011246681213378906}',17,92),(1722,13,6,'{\"Runtime\": \">9000\"}',42,92),(1723,13,6,'{\"Runtime\": 0.7946784496307373}',18,92),(1724,13,6,'{\"Runtime\": \">9000\"}',13,92),(1725,13,6,'{\"Runtime\": 0.0902558167775472}',4,92),(1726,13,6,'{\"Runtime\": \">9000\"}',3,92),(1727,13,6,'{\"Runtime\": 1.5018169085184734}',5,92),(1728,13,6,'{\"Runtime\": 0.3084767659505208}',6,92),(1729,13,6,'{\"Runtime\": \">9000\"}',7,92),(1730,13,6,'{\"Runtime\": \">9000\"}',8,92),(1731,13,6,'{\"Runtime\": \">9000\"}',9,92),(1732,13,6,'{\"Runtime\": \">9000\"}',19,92),(1733,13,6,'{\"Runtime\": 0.0008952617645263672}',11,91),(1734,13,6,'{\"Runtime\": 0.0009734630584716797}',1,91),(1735,13,6,'{\"Runtime\": 0.0009722709655761719}',40,91),(1736,13,6,'{\"Runtime\": 0.0009802182515462239}',15,91),(1737,13,6,'{\"Runtime\": 0.0018922487894694011}',14,91),(1738,13,6,'{\"Runtime\": 0.001503308614095052}',16,91),(1739,13,6,'{\"Runtime\": 0.0029587745666503906}',37,91),(1740,13,6,'{\"Runtime\": 0.005893548329671224}',41,91),(1741,13,6,'{\"Runtime\": 0.011252005894978842}',17,91),(1742,13,6,'{\"Runtime\": \">9000\"}',42,91),(1743,13,6,'{\"Runtime\": 0.7844207286834717}',18,91),(1744,13,6,'{\"Runtime\": \">9000\"}',13,91),(1745,13,6,'{\"Runtime\": 0.0896608829498291}',4,91),(1746,13,6,'{\"Runtime\": \">9000\"}',3,91),(1747,13,6,'{\"Runtime\": 1.4947216510772705}',5,91),(1748,13,6,'{\"Runtime\": 0.3090541362762451}',6,91),(1749,13,6,'{\"Runtime\": \">9000\"}',7,91),(1750,13,6,'{\"Runtime\": \">9000\"}',8,91),(1751,13,6,'{\"Runtime\": \">9000\"}',9,91),(1752,13,6,'{\"Runtime\": \">9000\"}',19,91);
/*!40000 ALTER TABLE `metrics` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `results`
--

DROP TABLE IF EXISTS `results`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `results` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `build_id` int(11) NOT NULL,
  `libary_id` int(11) NOT NULL,
  `time` double NOT NULL,
  `var` double NOT NULL,
  `dataset_id` int(11) NOT NULL,
  `method_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `build_id` (`build_id`),
  KEY `libary_id` (`libary_id`),
  KEY `dataset_id` (`dataset_id`),
  KEY `method_id` (`method_id`),
  CONSTRAINT `results_ibfk_1` FOREIGN KEY (`build_id`) REFERENCES `builds` (`id`) ON DELETE CASCADE,
  CONSTRAINT `results_ibfk_2` FOREIGN KEY (`libary_id`) REFERENCES `libraries` (`id`) ON DELETE CASCADE,
  CONSTRAINT `results_ibfk_3` FOREIGN KEY (`dataset_id`) REFERENCES `datasets` (`id`) ON DELETE CASCADE,
  CONSTRAINT `results_ibfk_4` FOREIGN KEY (`method_id`) REFERENCES `methods` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=1752 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `results`
--

LOCK TABLES `results` WRITE;
/*!40000 ALTER TABLE `results` DISABLE KEYS */;
INSERT INTO `results` VALUES (1,1,1,0.000254,0,1,17),(2,1,1,0.006852,0,2,17),(3,1,1,0.025937,0,1,17),(4,1,1,19.294976,0,3,17),(5,1,1,8.065343,0,4,17),(6,1,1,4.895281,0,5,17),(7,1,1,622.202203,0,6,17),(8,1,1,4405.939593,0,7,17),(9,1,1,1373.238238,0,8,17),(10,1,1,6291.235372,0,9,17),(11,1,1,0.000252,0,1,21),(12,1,1,0.006823,0,2,21),(13,1,1,0.025879,0,1,21),(14,1,1,19.282697,0,3,21),(15,1,1,7.936245,0,4,21),(16,1,1,4.898988,0,5,21),(17,1,1,618.521608,0,6,21),(18,1,1,4404.87748,0,7,21),(19,1,1,1378.742129,0,8,21),(20,1,1,6290.256301,0,9,21),(21,1,1,0.000246,0,1,24),(22,1,1,0.006321,0,2,24),(23,1,1,0.023778,0,1,24),(24,1,1,16.888558,0,3,24),(25,1,1,5.684023,0,4,24),(26,1,1,4.449521,0,5,24),(27,1,1,399.050242,0,6,24),(28,1,1,4376.726525,0,7,24),(29,1,1,996.238517,0,8,24),(30,1,1,6254.337385,0,9,24),(31,1,1,0.000254,0,1,25),(32,1,1,0.006637,0,2,25),(33,1,1,0.025151,0,1,25),(34,1,1,18.413305,0,3,25),(35,1,1,7.11487,0,4,25),(36,1,1,4.733183,0,5,25),(37,1,1,532.370904,0,6,25),(38,1,1,4393.698491,0,7,25),(39,1,1,1228.546812,0,8,25),(40,1,1,6281.773638,0,9,25),(41,1,1,0.000249,0,1,27),(42,1,1,0.006423,0,2,27),(43,1,1,0.024425,0,1,27),(44,1,1,17.602404,0,3,27),(45,1,1,6.333901,0,4,27),(46,1,1,4.576261,0,5,27),(47,1,1,458.818582,0,6,27),(48,1,1,4379.171437,0,7,27),(49,1,1,1103.997308,0,8,27),(50,1,1,6266.53402,0,9,27),(51,1,1,0.000246,0,1,28),(52,1,1,0.006046,0,2,28),(53,1,1,0.022592,0,1,28),(54,1,1,15.519008,0,3,28),(55,1,1,4.660115,0,4,28),(56,1,1,4.222796,0,5,28),(57,1,1,308.39439,0,6,28),(58,1,1,4335.686479,0,7,28),(59,1,1,819.769605,0,8,28),(60,1,1,6214.555894,0,9,28),(61,1,1,0.000247,0,1,29),(62,1,1,0.006142,0,2,29),(63,1,1,0.023185,0,1,29),(64,1,1,16.1557,0,3,29),(65,1,1,5.126539,0,4,29),(66,1,1,4.328684,0,5,29),(67,1,1,349.813576,0,6,29),(68,1,1,4363.331404,0,7,29),(69,1,1,901.761466,0,8,29),(70,1,1,6236.924739,0,9,29),(71,2,2,0.000827,0,1,17),(72,2,2,0.015365,0,2,17),(73,2,2,0.051001,0,1,17),(74,3,1,0.000255,0,1,17),(75,3,1,0.006832,0,2,17),(76,3,1,0.026011,0,1,17),(77,4,2,0.000826,0,1,21),(78,4,2,0.015352,0,2,21),(79,4,2,0.05092,0,1,21),(80,3,1,19.281087,0,3,17),(81,5,3,20.772048,0,10,82),(82,3,1,8.049246,0,4,17),(83,4,2,29.728305,0,3,21),(84,5,3,20.787809,0,10,80),(85,3,1,4.89748,0,5,17),(86,4,2,8.759758,0,4,21),(87,5,3,15.81968,0,10,81),(88,5,3,0.008256,0,11,66),(89,5,3,0.017463,0,12,66),(90,5,3,1.195171,0,13,66),(91,5,3,0.010601,0,11,67),(92,5,3,0.006315,0,12,67),(93,5,3,1.205615,0,13,67),(94,5,3,-2,0,14,39),(95,5,3,0.154517,0,11,84),(96,5,3,6.321282,0,15,84),(97,5,3,0.552283,0,1,84),(98,4,2,10.360115,0,5,21),(99,5,3,14.457007,0,16,84),(100,4,2,375.134366,0,6,21),(101,3,1,622.328987,0,6,17),(102,5,3,-1,0,17,84),(103,3,1,4411.470773,0,7,17),(104,5,3,-1,0,13,84),(105,4,2,5817.011302,0,7,21),(106,3,1,1372.672554,0,8,17),(107,4,2,1483.832045,0,8,21),(108,5,3,-1,0,18,84),(109,5,3,-1,0,13,84),(110,3,1,6295.00932,0,9,17),(111,3,1,0.000249,0,1,21),(112,3,1,0.006738,0,2,21),(113,3,1,0.025987,0,1,21),(114,3,1,19.244196,0,3,21),(115,3,1,7.950619,0,4,21),(116,3,1,4.894472,0,5,21),(117,3,1,618.736717,0,6,21),(118,4,2,6282.111938,0,9,21),(119,4,2,0.000849,0,1,24),(120,4,2,0.014836,0,2,24),(121,4,2,0.049065,0,1,24),(122,4,2,28.245807,0,3,24),(123,4,2,7.382456,0,4,24),(124,4,2,10.044463,0,5,24),(125,4,2,299.146405,0,6,24),(126,5,3,-1,0,4,84),(127,3,1,4406.014277,0,7,21),(128,5,3,-1,0,3,84),(129,3,1,1378.46517,0,8,21),(130,4,2,5803.077505,0,7,24),(131,5,3,-1,0,5,84),(132,4,2,1244.627958,0,8,24),(133,5,3,-1,0,6,84),(134,3,1,6292.106761,0,9,21),(135,3,1,0.000252,0,1,25),(136,3,1,0.006618,0,2,25),(137,3,1,0.025194,0,1,25),(138,3,1,18.404397,0,3,25),(139,3,1,7.118003,0,4,25),(140,3,1,4.7269,0,5,25),(141,3,1,531.911672,0,6,25),(142,5,3,-1,0,7,84),(143,4,2,6270.013014,0,9,24),(144,4,2,0.000822,0,1,25),(145,4,2,0.01523,0,2,25),(146,4,2,0.050368,0,1,25),(147,4,2,29.238339,0,3,25),(148,4,2,8.237163,0,4,25),(149,4,2,10.228907,0,5,25),(150,4,2,346.580785,0,6,25),(151,5,3,-1,0,8,84),(152,3,1,4395.282607,0,7,25),(153,3,1,1227.857006,0,8,25),(154,5,3,-1,0,9,84),(155,4,2,5822.002389,0,7,25),(156,4,2,1392.852289,0,8,25),(157,5,3,-1,0,19,84),(158,5,3,0.171083,0,11,88),(159,5,3,2.527809,0,15,88),(160,5,3,0.534998,0,1,88),(161,5,3,6.792966,0,16,88),(162,3,1,6280.597931,0,9,25),(163,3,1,0.000247,0,1,24),(164,3,1,0.006301,0,2,24),(165,3,1,0.023732,0,1,24),(166,3,1,16.85906,0,3,24),(167,3,1,5.668164,0,4,24),(168,3,1,4.446436,0,5,24),(169,3,1,398.641983,0,6,24),(170,5,3,-1,0,17,88),(171,4,2,6278.787042,0,9,25),(172,4,2,0.000821,0,1,29),(173,4,2,0.014716,0,2,29),(174,4,2,0.048526,0,1,29),(175,4,2,27.82444,0,3,29),(176,4,2,7.013517,0,4,29),(177,4,2,9.966459,0,5,29),(178,5,3,-1,0,13,88),(179,4,2,279.505046,0,6,29),(180,3,1,4371.607627,0,7,24),(181,3,1,995.424238,0,8,24),(182,5,3,-1,0,18,88),(183,4,2,5802.000744,0,7,29),(184,5,3,-1,0,13,88),(185,4,2,1179.40281,0,8,29),(186,3,1,6249.412846,0,9,24),(187,3,1,0.000247,0,1,28),(188,3,1,0.006052,0,2,28),(189,3,1,0.022567,0,1,28),(190,3,1,15.513659,0,3,28),(191,3,1,4.661145,0,4,28),(192,3,1,4.225239,0,5,28),(193,3,1,308.358032,0,6,28),(194,5,3,-1,0,4,88),(195,5,3,-1,0,3,88),(196,3,1,4343.629012,0,7,28),(197,4,2,6267.271647,0,9,29),(198,4,2,0.000844,0,1,28),(199,4,2,0.014589,0,2,28),(200,4,2,0.04805,0,1,28),(201,4,2,27.37339,0,3,28),(202,4,2,6.689612,0,4,28),(203,4,2,9.883095,0,5,28),(204,4,2,261.981993,0,6,28),(205,3,1,820.112518,0,8,28),(206,5,3,-1,0,5,88),(207,6,4,0.001272,0,15,65),(208,6,4,0.004235,0,20,65),(209,6,4,1.509752,0,18,65),(210,6,4,0.111905,0,13,65),(211,6,4,0.018039,0,15,40),(212,6,4,0.013963,0,20,40),(213,6,4,216.920645,0,21,40),(214,6,4,-2,0,6,54),(215,6,4,0.025563,0,2,50),(216,6,4,7.542338,0,5,52),(217,6,4,0.034262,0,1,52),(218,6,4,0.657508,0,3,49),(219,6,4,0.010685,0,22,51),(220,6,4,0.001543,0,1,53),(221,6,4,0.00145,0,11,53),(222,6,4,11.933173,0,7,55),(223,6,4,0.455502,0,4,55),(224,6,4,0.011543,0,23,56),(225,6,4,-2,0,10,81),(226,6,4,-2,0,10,80),(227,6,4,-2,0,10,82),(228,6,4,0.001238,0,11,45),(229,6,4,0.016291,0,24,45),(230,6,4,0.741728,0,25,45),(231,6,4,5.429908,0,26,45),(232,6,4,3.403366,0,3,45),(233,6,4,0.040918,0,27,45),(234,6,4,35.854121,0,28,45),(235,6,4,0.009739,0,29,45),(236,6,4,0.010917,0,30,45),(237,6,4,0.018097,0,31,45),(238,6,4,0.063348,0,32,45),(239,6,4,0.00144,0,33,45),(240,6,4,0.007479,0,11,66),(241,6,4,0.002252,0,12,66),(242,6,4,0.015759,0,13,66),(243,6,4,0.00961,0,1,21),(244,6,4,0.018291,0,2,21),(245,6,4,0.08057,0,1,21),(246,6,4,50.636067,0,3,21),(247,6,4,60.908667,0,4,21),(248,6,4,52.491902,0,5,21),(249,6,4,1142.602727,0,6,21),(250,5,3,-1,0,6,88),(251,4,2,5798.187795,0,7,28),(252,3,1,6215.040678,0,9,28),(253,3,1,0.000248,0,1,27),(254,3,1,0.006476,0,2,27),(255,3,1,0.02443,0,1,27),(256,3,1,17.60797,0,3,27),(257,3,1,6.333585,0,4,27),(258,3,1,4.578507,0,5,27),(259,6,4,-1,0,7,21),(260,3,1,458.616205,0,6,27),(261,4,2,1121.336978,0,8,28),(262,5,3,-1,0,7,88),(263,5,3,-1,0,8,88),(264,3,1,4389.596649,0,7,27),(265,6,4,4578.86588,0,8,21),(266,3,1,1104.237773,0,8,27),(267,4,2,6263.218532,0,9,28),(268,4,2,0.000837,0,1,17),(269,4,2,0.015392,0,2,17),(270,4,2,0.050981,0,1,17),(271,4,2,29.787057,0,3,17),(272,5,3,-1,0,9,88),(273,4,2,8.744621,0,4,17),(274,4,2,10.339711,0,5,17),(275,4,2,375.631887,0,6,17),(276,6,4,-1,0,9,21),(277,6,4,0.21966,0,11,70),(278,6,4,0.814149,0,24,70),(279,6,4,8.213579,0,25,70),(280,6,4,177.61372,0,26,70),(281,6,4,54.65005,0,3,70),(282,6,4,4.148548,0,27,70),(283,5,3,-1,0,19,88),(284,5,3,0.106043,0,11,87),(285,5,3,0.601486,0,15,87),(286,5,3,0.292885,0,1,87),(287,5,3,4.00286,0,16,87),(288,6,4,1459.105953,0,28,70),(289,6,4,1.597769,0,29,70),(290,6,4,2.569662,0,30,70),(291,6,4,1.833954,0,31,70),(292,6,4,5.356623,0,32,70),(293,6,4,0.114198,0,33,70),(294,6,4,0.037095,0,34,8),(295,6,4,0.719345,0,35,8),(296,6,4,1.212067,0,36,8),(297,6,4,-2,0,37,8),(298,6,4,96.868802,0,22,8),(299,6,4,-2,0,21,8),(300,6,4,0.182967,0,18,8),(301,6,4,13.169976,0,13,8),(302,6,4,0.288872,0,10,8),(303,6,4,456.632228,0,3,8),(304,5,3,668.823285,0,17,87),(305,6,4,-2,0,5,8),(306,6,4,0.00691,0,34,16),(307,6,4,0.72068,0,35,16),(308,6,4,1.202898,0,36,16),(309,6,4,-2,0,37,16),(310,6,4,96.851238,0,22,16),(311,6,4,-2,0,21,16),(312,6,4,0.167002,0,18,16),(313,6,4,13.747834,0,13,16),(314,6,4,0.256348,0,10,16),(315,6,4,459.745839,0,3,16),(316,6,4,-2,0,5,16),(317,6,4,0.011952,0,34,10),(318,6,4,0.429041,0,35,10),(319,6,4,3.843311,0,36,10),(320,6,4,-2,0,37,10),(321,6,4,98.036375,0,22,10),(322,6,4,-2,0,21,10),(323,6,4,0.153711,0,18,10),(324,6,4,8.744084,0,13,10),(325,6,4,0.285853,0,10,10),(326,6,4,462.630732,0,3,10),(327,6,4,-2,0,5,10),(328,6,4,0.02388,0,11,1),(329,6,4,0.022516,0,24,1),(330,6,4,0.850415,0,25,1),(331,6,4,12.625956,0,26,1),(332,6,4,11.73205,0,3,1),(333,6,4,0.006668,0,27,1),(334,3,1,6267.579766,0,9,27),(335,3,1,0.000247,0,1,29),(336,3,1,0.006235,0,2,29),(337,3,1,0.023306,0,1,29),(338,3,1,16.189801,0,3,29),(339,3,1,5.125351,0,4,29),(340,3,1,4.332744,0,5,29),(341,3,1,349.881355,0,6,29),(342,4,2,5830.501428,0,7,17),(343,4,2,1484.661185,0,8,17),(344,6,4,2785.47675,0,28,1),(345,6,4,0.005207,0,29,1),(346,6,4,0.025807,0,30,1),(347,6,4,0.005321,0,31,1),(348,6,4,0.040352,0,32,1),(349,6,4,0.001798,0,33,1),(350,6,4,0.047995,0,11,63),(351,6,4,0.008222,0,24,63),(352,6,4,0.868246,0,25,63),(353,6,4,182.051233,0,26,63),(354,6,4,21.715216,0,3,63),(355,6,4,0.122067,0,27,63),(356,3,1,4359.476775,0,7,29),(357,6,4,458.318606,0,28,63),(358,6,4,0.030349,0,29,63),(359,6,4,0.057439,0,30,63),(360,6,4,0.028919,0,31,63),(361,6,4,0.364174,0,32,63),(362,6,4,0.002873,0,33,63),(363,6,4,0.026412,0,33,71),(364,6,4,0.003058,0,27,71),(365,6,4,0.006006,0,30,73),(366,6,4,1.995687,0,26,73),(367,6,4,0.002474,0,31,72),(368,6,4,0.002462,0,29,72),(369,6,4,0.155928,0,11,43),(370,6,4,0.237531,0,24,43),(371,6,4,3.743567,0,25,43),(372,6,4,7.619949,0,26,43),(373,6,4,21.534659,0,3,43),(374,6,4,0.423894,0,27,43),(375,3,1,902.474946,0,8,29),(376,6,4,138.802785,0,28,43),(377,6,4,0.255003,0,29,43),(378,6,4,0.230574,0,30,43),(379,6,4,0.255681,0,31,43),(380,6,4,0.636101,0,32,43),(381,6,4,0.105507,0,33,43),(382,6,4,0.032509,0,15,108),(383,6,4,0.529339,0,20,108),(384,6,4,0.752961,0,18,108),(385,6,4,0.927631,0,13,108),(386,6,4,0.145494,0,11,30),(387,6,4,0.444187,0,24,30),(388,6,4,3.504028,0,25,30),(389,6,4,306.618171,0,26,30),(390,6,4,33.428392,0,3,30),(391,6,4,9.625615,0,27,30),(392,5,3,8293.487323,0,13,87),(393,6,4,608.625907,0,28,30),(394,6,4,1.428026,0,29,30),(395,6,4,1.76446,0,30,30),(396,6,4,1.414324,0,31,30),(397,6,4,8.808982,0,32,30),(398,6,4,0.153308,0,33,30),(399,6,4,0.013941,0,11,2),(400,6,4,0.011636,0,24,2),(401,6,4,0.738219,0,25,2),(402,6,4,9.926352,0,26,2),(403,6,4,9.72162,0,3,2),(404,6,4,0.007848,0,27,2),(405,4,2,6286.591062,0,9,17),(406,4,2,0.000826,0,1,27),(407,4,2,0.015113,0,2,27),(408,4,2,0.049649,0,1,27),(409,4,2,28.739208,0,3,27),(410,4,2,7.777131,0,4,27),(411,4,2,10.128804,0,5,27),(412,4,2,321.350359,0,6,27),(413,5,3,-1,0,18,87),(414,6,4,2601.328736,0,28,2),(415,6,4,0.005685,0,29,2),(416,6,4,0.022327,0,30,2),(417,6,4,0.005923,0,31,2),(418,6,4,0.034662,0,32,2),(419,6,4,0.002539,0,33,2),(420,6,4,0.025717,0,11,83),(421,6,4,0.117831,0,24,83),(422,6,4,0.463049,0,25,83),(423,6,4,24.597234,0,26,83),(424,6,4,5.316252,0,3,83),(425,6,4,0.325361,0,27,83),(426,6,4,61.554318,0,28,83),(427,6,4,0.03882,0,29,83),(428,6,4,0.982433,0,30,83),(429,6,4,0.160468,0,31,83),(430,6,4,5.160095,0,32,83),(431,6,4,0.002582,0,33,83),(432,6,4,0,0,11,44),(433,6,4,0,0,24,44),(434,6,4,0,0,25,44),(435,6,4,0,0,26,44),(436,6,4,0,0,3,44),(437,6,4,0,0,27,44),(438,6,4,0,0,28,44),(439,6,4,0,0,29,44),(440,6,4,0,0,30,44),(441,6,4,0,0,31,44),(442,6,4,0,0,32,44),(443,6,4,0,0,33,44),(444,6,4,-2,0,16,36),(445,6,4,0.43733,0,38,36),(446,6,4,2.816514,0,39,36),(447,6,4,-2,0,22,36),(448,3,1,6235.214792,0,9,29),(449,6,4,685.840288,0,21,36),(450,6,4,8.636096,0,10,36),(451,6,4,-2,0,3,36),(452,5,3,-1,0,13,87),(453,7,5,-2,0,11,90),(454,7,5,-2,0,1,90),(455,7,5,-2,0,40,90),(456,7,5,-2,0,15,90),(457,7,5,-2,0,14,90),(458,7,5,-2,0,16,90),(459,7,5,0,0,37,90),(460,7,5,-2,0,41,90),(461,7,5,-2,0,17,90),(462,7,5,-2,0,42,90),(463,7,5,-2,0,18,90),(464,7,5,-2,0,13,90),(465,7,5,-2,0,4,90),(466,7,5,-2,0,3,90),(467,7,5,-2,0,5,90),(468,7,5,-2,0,6,90),(469,7,5,-2,0,7,90),(470,7,5,-2,0,8,90),(471,7,5,-2,0,9,90),(472,7,5,-2,0,19,90),(473,7,5,-2,0,11,91),(474,7,5,-2,0,1,91),(475,7,5,-2,0,40,91),(476,7,5,-2,0,15,91),(477,7,5,-2,0,14,91),(478,7,5,-2,0,16,91),(479,7,5,0,0,37,91),(480,7,5,-2,0,41,91),(481,7,5,-2,0,17,91),(482,7,5,-2,0,42,91),(483,7,5,-2,0,18,91),(484,7,5,-2,0,13,91),(485,7,5,-2,0,4,91),(486,7,5,-2,0,3,91),(487,7,5,-2,0,5,91),(488,7,5,-2,0,6,91),(489,7,5,-2,0,7,91),(490,7,5,-2,0,8,91),(491,7,5,-2,0,9,91),(492,7,5,-2,0,19,91),(493,7,5,-2,0,11,92),(494,7,5,-2,0,1,92),(495,7,5,-2,0,40,92),(496,7,5,-2,0,15,92),(497,7,5,-2,0,14,92),(498,7,5,-2,0,16,92),(499,7,5,0,0,37,92),(500,7,5,-2,0,41,92),(501,7,5,-2,0,17,92),(502,7,5,-2,0,42,92),(503,7,5,-2,0,18,92),(504,7,5,-2,0,13,92),(505,7,5,-2,0,4,92),(506,7,5,-2,0,3,92),(507,7,5,-2,0,5,92),(508,7,5,-2,0,6,92),(509,7,5,-2,0,7,92),(510,7,5,-2,0,8,92),(511,7,5,-2,0,9,92),(512,7,5,-2,0,19,92),(513,7,5,-2,0,15,62),(514,7,5,-2,0,20,62),(515,7,5,-2,0,7,62),(516,7,5,-2,0,9,62),(517,7,5,-2,0,43,62),(518,7,5,-2,0,21,62),(519,7,5,10.344604,0,13,62),(520,7,5,3.15512,0,18,62),(521,7,5,0.069958,0,11,66),(522,7,5,0.067106,0,12,66),(523,7,5,1.382342,0,13,66),(524,7,5,0.044644,0,1,21),(525,7,5,0.112254,0,2,21),(526,7,5,0.909472,0,1,21),(527,7,5,258.043351,0,3,21),(528,7,5,29.382852,0,4,21),(529,7,5,42.505025,0,5,21),(530,6,4,1656.070872,0,7,36),(531,4,2,5807.656135,0,7,27),(532,7,5,1682.586276,0,6,21),(533,7,5,-2,0,7,21),(534,6,4,1548.304304,0,9,36),(535,5,3,-1,0,4,87),(536,6,4,-2,0,19,36),(537,6,4,0.291145,0,11,76),(538,6,4,0.273173,0,24,76),(539,6,4,0.775601,0,25,76),(540,6,4,5.27778,0,26,76),(541,6,4,2.020126,0,3,76),(542,6,4,0.407799,0,27,76),(543,6,4,9.200395,0,28,76),(544,6,4,0.273227,0,29,76),(545,6,4,0.273938,0,30,76),(546,6,4,0.374031,0,31,76),(547,6,4,0.4748,0,32,76),(548,6,4,0.273153,0,33,76),(549,6,4,0.012653,0,15,62),(550,6,4,0.080693,0,20,62),(551,6,4,69.343849,0,7,62),(552,6,4,25.27232,0,9,62),(553,6,4,0.084486,0,43,62),(554,6,4,0.555707,0,21,62),(555,6,4,1.007563,0,13,62),(556,6,4,0.264669,0,18,62),(557,6,4,0.001647,0,11,90),(558,6,4,0.001967,0,1,90),(559,6,4,0.001888,0,40,90),(560,6,4,0.002087,0,15,90),(561,6,4,0.002714,0,14,90),(562,6,4,0.005166,0,16,90),(563,6,4,-2,0,37,90),(564,6,4,0.031391,0,41,90),(565,6,4,0.045117,0,17,90),(566,6,4,1.164351,0,42,90),(567,6,4,0.673292,0,18,90),(568,6,4,3.45369,0,13,90),(569,6,4,0.502939,0,4,90),(570,6,4,17.844258,0,3,90),(571,6,4,12.71871,0,5,90),(572,6,4,1.220567,0,6,90),(573,4,2,1315.292152,0,8,27),(574,6,4,261.298176,0,7,90),(575,6,4,25.90127,0,8,90),(576,6,4,90.188591,0,9,90),(577,6,4,29.673955,0,19,90),(578,6,4,0.001658,0,11,92),(579,6,4,0.001992,0,1,92),(580,6,4,0.001905,0,40,92),(581,6,4,0.002084,0,15,92),(582,6,4,0.005808,0,14,92),(583,6,4,0.004799,0,16,92),(584,6,4,-2,0,37,92),(585,6,4,0.072383,0,41,92),(586,6,4,0.175665,0,17,92),(587,6,4,0.438821,0,42,92),(588,6,4,0.32298,0,18,92),(589,6,4,1.024115,0,13,92),(590,6,4,2.167049,0,4,92),(591,6,4,5.275064,0,3,92),(592,6,4,29.355206,0,5,92),(593,6,4,6.040878,0,6,92),(594,6,4,65.262283,0,7,92),(595,6,4,42.520735,0,8,92),(596,6,4,35.829942,0,9,92),(597,6,4,43.550236,0,19,92),(598,6,4,0.001695,0,11,91),(599,6,4,0.002007,0,1,91),(600,6,4,0.002013,0,40,91),(601,6,4,0.002114,0,15,91),(602,6,4,0.005789,0,14,91),(603,6,4,0.004865,0,16,91),(604,6,4,-2,0,37,91),(605,6,4,0.072227,0,41,91),(606,6,4,0.176118,0,17,91),(607,6,4,0.436822,0,42,91),(608,6,4,0.323458,0,18,91),(609,6,4,1.023398,0,13,91),(610,6,4,2.166246,0,4,91),(611,6,4,5.274341,0,3,91),(612,6,4,29.365868,0,5,91),(613,6,4,6.062959,0,6,91),(614,6,4,65.177578,0,7,91),(615,6,4,42.533252,0,8,91),(616,6,4,35.803543,0,9,91),(617,6,4,43.562304,0,19,91),(618,6,4,-2,0,14,93),(619,6,4,-2,0,14,93),(620,6,4,-2,0,11,93),(621,6,4,-2,0,1,93),(622,6,4,-2,0,44,93),(623,6,4,-2,0,23,93),(624,6,4,-2,0,39,93),(625,6,4,-2,0,3,93),(626,6,4,-2,0,21,93),(627,6,4,-2,0,5,93),(628,7,5,-1,0,8,21),(629,7,5,-2,0,9,21),(630,7,5,0,0,6,54),(631,7,5,0,0,7,55),(632,7,5,0,0,4,55),(633,7,5,0,0,23,56),(634,7,5,0,0,22,51),(635,7,5,0,0,2,50),(636,7,5,0,0,5,52),(637,7,5,0,0,1,52),(638,7,5,0,0,3,49),(639,7,5,0,0,1,53),(640,7,5,0,0,11,53),(641,5,3,-1,0,3,87),(642,5,3,-1,0,5,87),(643,4,2,6278.449514,0,9,27),(644,5,3,-1,0,6,87),(645,5,3,-1,0,7,87),(646,9,6,0.057203,0,15,40),(647,9,6,0.007979,0,20,40),(648,5,3,-1,0,8,87),(649,9,6,254.605725,0,21,40),(650,9,6,0.019924,0,11,45),(651,9,6,0.020128,0,24,45),(652,9,6,0.870601,0,25,45),(653,9,6,18.974631,0,26,45),(654,9,6,3.594218,0,3,45),(655,9,6,0.066804,0,27,45),(656,9,6,163.866928,0,28,45),(657,9,6,0.028166,0,29,45),(658,9,6,0.026444,0,30,45),(659,9,6,0.032615,0,31,45),(660,9,6,0.099399,0,32,45),(661,9,6,0.001213,0,33,45),(662,9,6,0.096693,0,11,90),(663,9,6,0.010482,0,1,90),(664,9,6,0.002985,0,40,90),(665,9,6,0.000851,0,15,90),(666,9,6,-2,0,14,90),(667,9,6,0.007322,0,16,90),(668,9,6,0.003589,0,37,90),(669,9,6,0.008373,0,41,90),(670,9,6,0.015521,0,17,90),(671,9,6,0.173325,0,42,90),(672,9,6,-2,0,18,90),(673,9,6,0.567584,0,13,90),(674,9,6,0.131935,0,4,90),(675,9,6,2.39162,0,3,90),(676,9,6,29.14054,0,7,90),(677,9,6,3.896033,0,8,90),(678,9,6,10.396063,0,9,90),(679,9,6,4.269451,0,19,90),(680,9,6,0.000789,0,11,92),(681,9,6,0.000767,0,1,92),(682,9,6,0.000784,0,40,92),(683,9,6,0.000856,0,15,92),(684,9,6,0.001654,0,14,92),(685,9,6,0.001244,0,16,92),(686,9,6,0.002826,0,37,92),(687,9,6,0.005696,0,41,92),(688,9,6,0.010968,0,17,92),(689,9,6,0.111982,0,42,92),(690,9,6,0.800504,0,18,92),(691,9,6,0.405843,0,13,92),(692,9,6,0.089808,0,4,92),(693,9,6,1.461308,0,3,92),(694,9,6,1.482122,0,5,92),(695,9,6,0.303775,0,6,92),(696,9,6,15.577231,0,7,92),(697,9,6,2.41529,0,8,92),(698,9,6,5.739926,0,9,92),(699,9,6,2.642953,0,19,92),(700,9,6,0.000721,0,11,91),(701,9,6,0.000777,0,1,91),(702,9,6,0.000768,0,40,91),(703,9,6,0.000868,0,15,91),(704,9,6,0.001705,0,14,91),(705,9,6,0.001258,0,16,91),(706,9,6,0.002783,0,37,91),(707,9,6,0.005638,0,41,91),(708,9,6,0.010931,0,17,91),(709,9,6,0.112162,0,42,91),(710,9,6,0.7805,0,18,91),(711,9,6,0.408002,0,13,91),(712,9,6,0.089609,0,4,91),(713,9,6,1.459966,0,3,91),(714,9,6,1.483342,0,5,91),(715,9,6,0.304429,0,6,91),(716,9,6,15.548754,0,7,91),(717,9,6,2.414018,0,8,91),(718,9,6,5.68889,0,9,91),(719,9,6,2.638049,0,19,91),(720,9,6,0.046228,0,11,66),(721,9,6,0.00084,0,12,66),(722,9,6,0.152141,0,13,66),(723,9,6,0.023693,0,1,21),(724,9,6,-2,0,2,21),(725,9,6,7.59618,0,1,21),(726,9,6,40.310394,0,3,21),(727,9,6,-2,0,4,21),(728,5,3,-1,0,9,87),(729,5,3,-1,0,19,87),(730,5,3,0.107212,0,11,86),(731,5,3,0.600458,0,15,86),(732,5,3,0.293158,0,1,86),(733,5,3,4.01681,0,16,86),(734,10,7,-2,0,14,37),(735,11,6,0.018087,0,22,51),(736,10,7,-2,0,11,70),(737,10,7,-2,0,24,70),(738,10,7,-2,0,25,70),(739,10,7,-2,0,26,70),(740,10,7,-2,0,3,70),(741,10,7,-2,0,27,70),(742,10,7,-2,0,28,70),(743,11,6,15.729119,0,7,55),(744,10,7,-2,0,29,70),(745,11,6,1.24781,0,4,55),(746,11,6,0.012782,0,2,50),(747,10,7,-2,0,30,70),(748,11,6,0.941648,0,3,49),(749,10,7,-2,0,31,70),(750,10,7,-2,0,32,70),(751,10,7,-2,0,33,70),(752,11,6,17.639405,0,5,52),(753,11,6,0.074932,0,1,52),(754,11,6,0.000674,0,1,53),(755,11,6,0.000622,0,11,53),(756,10,7,0.729174,0,16,34),(757,11,6,-2,0,6,54),(758,11,6,0.01092,0,23,56),(759,11,6,0,0,11,63),(760,11,6,0,0,24,63),(761,11,6,0,0,25,63),(762,10,7,0.725702,0,38,34),(763,10,7,1.857699,0,39,34),(764,5,3,668.410537,0,17,86),(765,10,7,1.054225,0,22,34),(766,10,7,37.070841,0,21,34),(767,10,7,3.340201,0,10,34),(768,10,7,8.074895,0,3,34),(769,10,7,102.7779,0,7,34),(770,11,6,0,0,26,63),(771,10,7,29.159336,0,9,34),(772,11,6,0,0,3,63),(773,11,6,0,0,27,63),(774,10,7,243.372049,0,19,34),(775,10,7,0.00006,0,15,62),(776,10,7,0.000062,0,20,62),(777,10,7,0.000065,0,7,62),(778,10,7,0.000062,0,9,62),(779,10,7,0.000061,0,43,62),(780,10,7,0.000063,0,21,62),(781,10,7,0.00007,0,13,62),(782,10,7,0.00008,0,18,62),(783,10,7,0.332818,0,1,21),(784,10,7,0.338739,0,2,21),(785,10,7,0.373031,0,1,21),(786,10,7,18.799995,0,3,21),(787,10,7,19.802706,0,4,21),(788,10,7,14.478462,0,5,21),(789,10,7,247.046953,0,6,21),(790,11,6,0,0,28,63),(791,11,6,0,0,29,63),(792,11,6,0,0,30,63),(793,11,6,0,0,31,63),(794,11,6,0,0,32,63),(795,11,6,0,0,33,63),(796,11,6,0.043999,0,11,75),(797,11,6,0.019642,0,24,75),(798,11,6,0.155458,0,25,75),(799,11,6,2.065873,0,26,75),(800,11,6,0.963903,0,3,75),(801,11,6,0.217448,0,27,75),(802,11,6,31.345789,0,28,75),(803,11,6,0.15792,0,29,75),(804,11,6,0.078012,0,30,75),(805,11,6,0.084023,0,31,75),(806,11,6,0.293806,0,32,75),(807,11,6,0.005721,0,33,75),(808,11,6,0.015483,0,11,45),(809,11,6,0.027324,0,24,45),(810,11,6,1.22698,0,25,45),(811,11,6,32.553223,0,26,45),(812,11,6,4.476683,0,3,45),(813,11,6,0.111362,0,27,45),(814,11,6,175.298251,0,28,45),(815,11,6,0.064714,0,29,45),(816,11,6,0.06659,0,30,45),(817,11,6,0.054183,0,31,45),(818,11,6,0.150831,0,32,45),(819,11,6,0.001769,0,33,45),(820,11,6,0.109825,0,14,93),(821,11,6,0.003531,0,14,93),(822,11,6,0.001918,0,11,93),(823,11,6,0.002349,0,1,93),(824,11,6,0.008424,0,44,93),(825,11,6,-2,0,23,93),(826,11,6,-2,0,39,93),(827,11,6,-2,0,3,93),(828,11,6,-2,0,21,93),(829,11,6,-2,0,5,93),(830,11,6,0.036402,0,15,62),(831,11,6,-2,0,20,62),(832,11,6,-2,0,7,62),(833,11,6,1.780564,0,9,62),(834,11,6,0.023192,0,43,62),(835,11,6,0.08795,0,21,62),(836,11,6,-2,0,13,62),(837,11,6,-2,0,18,62),(838,11,6,-2,0,11,70),(839,11,6,-2,0,39,70),(840,11,6,-2,0,13,70),(841,11,6,-2,0,18,70),(842,11,6,0.028619,0,24,1),(843,11,6,1.007959,0,25,1),(844,11,6,3.692453,0,26,1),(845,11,6,18.869567,0,3,1),(846,11,6,0.005442,0,27,1),(847,10,7,3063.381507,0,7,21),(848,11,6,-1,0,28,1),(849,11,6,0.029632,0,29,1),(850,11,6,0.008268,0,30,1),(851,11,6,0.002553,0,31,1),(852,11,6,0.013249,0,32,1),(853,11,6,0.004522,0,33,1),(854,10,7,2994.077982,0,8,21),(855,5,3,8073.690862,0,13,86),(856,11,6,-1,0,31,72),(857,5,3,-1,0,18,86),(858,11,6,-1,0,29,72),(859,10,7,3767.149858,0,9,21),(860,10,7,0.35203,0,1,18),(861,10,7,0.352763,0,2,18),(862,10,7,0.562578,0,1,18),(863,10,7,15.760047,0,3,18),(864,10,7,61.403423,0,4,18),(865,5,3,-1,0,13,86),(866,11,6,-1,0,30,73),(867,5,3,-1,0,4,86),(868,11,6,-1,0,26,73),(869,10,7,8285.398932,0,5,18),(870,5,3,-1,0,3,86),(871,11,6,-1,0,33,71),(872,5,3,-1,0,5,86),(873,11,6,-1,0,27,71),(874,10,7,5392.632215,0,6,18),(875,5,3,-1,0,6,86),(876,11,6,-1,0,24,74),(877,10,7,1763.359723,0,7,18),(878,5,3,-1,0,7,86),(879,11,6,-1,0,25,74),(880,10,7,-1,0,8,18),(881,10,7,1693.270461,0,9,18),(882,10,7,1.488911,0,3,49),(883,10,7,0.560325,0,23,56),(884,5,3,-1,0,8,86),(885,10,7,31.533428,0,7,55),(886,10,7,2.108738,0,4,55),(887,10,7,0.54442,0,2,50),(888,10,7,0.514533,0,1,53),(889,10,7,0.510944,0,11,53),(890,11,6,-1,0,26,74),(891,10,7,642.949433,0,6,54),(892,10,7,17.83055,0,5,52),(893,10,7,0.62886,0,1,52),(894,10,7,0.545545,0,22,51),(895,10,7,0.513686,0,11,66),(896,10,7,0.463695,0,12,66),(897,10,7,0.515502,0,13,66),(898,10,7,0.326297,0,1,77),(899,10,7,0.310028,0,16,77),(900,10,7,0.325534,0,2,77),(901,10,7,0.312392,0,44,77),(902,10,7,0.430539,0,13,77),(903,10,7,0.371941,0,18,77),(904,10,7,1.175032,0,4,77),(905,10,7,1.45223,0,3,77),(906,10,7,11.149661,0,5,77),(907,10,7,41.442238,0,8,77),(908,10,7,0.550028,0,11,90),(909,10,7,0.514423,0,1,90),(910,10,7,0.496407,0,40,90),(911,10,7,0.49674,0,15,90),(912,10,7,0.527487,0,14,90),(913,10,7,0.512284,0,16,90),(914,10,7,0.508432,0,37,90),(915,10,7,0.513306,0,41,90),(916,10,7,0.517778,0,17,90),(917,10,7,0.629071,0,42,90),(918,10,7,20.916657,0,18,90),(919,10,7,0.769917,0,13,90),(920,10,7,0.69378,0,4,90),(921,10,7,1.621516,0,3,90),(922,10,7,3.215099,0,5,90),(923,10,7,1.114952,0,6,90),(924,10,7,13.340897,0,7,90),(925,10,7,7.57556,0,8,90),(926,10,7,7.528912,0,9,90),(927,10,7,8.779532,0,19,90),(928,10,7,0.534823,0,11,91),(929,10,7,0.533128,0,1,91),(930,10,7,0.528533,0,40,91),(931,10,7,0.531959,0,15,91),(932,10,7,0.549482,0,14,91),(933,10,7,0.541794,0,16,91),(934,10,7,0.534445,0,37,91),(935,10,7,0.564268,0,41,91),(936,10,7,0.557424,0,17,91),(937,10,7,0.649204,0,42,91),(938,10,7,21.06124,0,18,91),(939,10,7,0.8156,0,13,91),(940,10,7,0.752436,0,4,91),(941,10,7,1.710945,0,3,91),(942,10,7,3.525656,0,5,91),(943,10,7,1.245513,0,6,91),(944,10,7,13.671006,0,7,91),(945,10,7,7.984773,0,8,91),(946,10,7,7.878778,0,9,91),(947,10,7,9.27021,0,19,91),(948,10,7,0.501374,0,11,92),(949,10,7,0.502436,0,1,92),(950,10,7,0.498551,0,40,92),(951,10,7,0.503562,0,15,92),(952,10,7,0.531106,0,14,92),(953,10,7,0.502647,0,16,92),(954,10,7,0.506765,0,37,92),(955,10,7,0.514839,0,41,92),(956,10,7,0.524848,0,17,92),(957,10,7,0.608603,0,42,92),(958,5,3,-1,0,9,86),(959,10,7,21.067668,0,18,92),(960,10,7,0.76788,0,13,92),(961,10,7,0.694803,0,4,92),(962,10,7,1.623717,0,3,92),(963,10,7,3.222796,0,5,92),(964,10,7,1.129486,0,6,92),(965,10,7,13.215639,0,7,92),(966,11,6,-1,0,3,74),(967,10,7,7.643207,0,8,92),(968,10,7,7.564464,0,9,92),(969,10,7,8.806768,0,19,92),(970,5,3,-1,0,19,86),(971,5,3,0.172453,0,11,89),(972,5,3,2.533527,0,15,89),(973,5,3,0.54334,0,1,89),(974,5,3,6.817485,0,16,89),(975,11,6,-1,0,27,74),(976,5,3,-1,0,17,89),(977,11,6,-1,0,28,74),(978,5,3,-1,0,13,89),(979,5,3,0.153631,0,11,85),(980,5,3,6.150728,0,15,85),(981,5,3,0.5537,0,1,85),(982,5,3,14.422771,0,16,85),(983,11,6,-1,0,29,74),(984,5,3,-1,0,17,85),(985,11,6,-1,0,30,74),(986,5,3,-1,0,13,85),(987,11,6,-1,0,31,74),(988,5,3,-1,0,18,85),(989,11,6,-1,0,32,74),(990,5,3,-1,0,13,85),(991,11,6,-1,0,33,74),(992,5,3,-1,0,4,85),(993,11,6,-1,0,34,8),(994,5,3,-1,0,3,85),(995,11,6,-1,0,35,8),(996,5,3,-1,0,5,85),(997,11,6,-1,0,36,8),(998,5,3,-1,0,6,85),(999,11,6,-1,0,37,8),(1000,5,3,-1,0,7,85),(1001,11,6,-1,0,22,8),(1002,5,3,-1,0,8,85),(1003,11,6,-1,0,21,8),(1004,5,3,-1,0,9,85),(1005,11,6,-1,0,18,8),(1006,5,3,-1,0,19,85),(1007,5,3,0.007091,0,1,79),(1008,5,3,0.000479,0,16,79),(1009,5,3,0.000487,0,2,79),(1010,5,3,0.00049,0,44,79),(1011,5,3,0.001085,0,13,79),(1012,5,3,0.001104,0,18,79),(1013,5,3,0.000585,0,4,79),(1014,5,3,0.002789,0,3,79),(1015,5,3,0.013723,0,5,79),(1016,5,3,0.018371,0,8,79),(1017,5,3,0.004456,0,1,77),(1018,5,3,0.001389,0,16,77),(1019,5,3,0.002792,0,2,77),(1020,5,3,0.001729,0,44,77),(1021,5,3,0.063678,0,13,77),(1022,5,3,0.050469,0,18,77),(1023,5,3,1.03531,0,4,77),(1024,5,3,0.492513,0,3,77),(1025,5,3,5.064491,0,5,77),(1026,5,3,55.566668,0,8,77),(1027,5,3,-2,0,1,78),(1028,5,3,-2,0,16,78),(1029,5,3,-2,0,2,78),(1030,5,3,-2,0,44,78),(1031,5,3,-2,0,13,78),(1032,5,3,-2,0,18,78),(1033,5,3,-2,0,4,78),(1034,5,3,-2,0,3,78),(1035,5,3,-2,0,5,78),(1036,5,3,-2,0,8,78),(1037,5,3,0.170173,0,11,70),(1038,5,3,1.455023,0,24,70),(1039,5,3,13.968303,0,25,70),(1040,11,6,-1,0,13,8),(1041,5,3,210.54071,0,26,70),(1042,5,3,2.111528,0,3,70),(1043,5,3,7.687228,0,27,70),(1044,5,3,2.44243,0,28,70),(1045,5,3,3.116325,0,29,70),(1046,5,3,4.340229,0,30,70),(1047,5,3,3.298135,0,31,70),(1048,5,3,8.436819,0,32,70),(1049,5,3,0.260435,0,33,70),(1050,5,3,0.023334,0,1,60),(1051,5,3,0.043885,0,2,60),(1052,5,3,0.109324,0,1,60),(1053,5,3,0.98518,0,3,60),(1054,5,3,7.401446,0,4,60),(1055,5,3,461.782273,0,5,60),(1056,5,3,1358.663903,0,6,60),(1057,5,3,64.391858,0,7,60),(1058,5,3,455.284252,0,8,60),(1059,5,3,60.00202,0,9,60),(1060,5,3,0.014403,0,1,61),(1061,5,3,0.030151,0,2,61),(1062,5,3,0.111002,0,1,61),(1063,5,3,0.972746,0,3,61),(1064,5,3,7.400856,0,4,61),(1065,11,6,-1,0,10,8),(1066,5,3,461.720717,0,5,61),(1067,5,3,1359.127824,0,6,61),(1068,5,3,64.440576,0,7,61),(1069,5,3,455.900101,0,8,61),(1070,5,3,59.98956,0,9,61),(1071,5,3,0.014333,0,1,59),(1072,5,3,0.02404,0,2,59),(1073,5,3,0.096264,0,1,59),(1074,5,3,1.072626,0,3,59),(1075,5,3,6.898142,0,4,59),(1076,5,3,402.422096,0,5,59),(1077,11,6,-1,0,3,8),(1078,11,6,-2,0,5,8),(1079,5,3,1342.805071,0,6,59),(1080,5,3,65.419468,0,7,59),(1081,5,3,204.285632,0,8,59),(1082,5,3,50.118185,0,9,59),(1083,5,3,0,0,1,57),(1084,5,3,0,0,2,57),(1085,5,3,0,0,1,57),(1086,5,3,0,0,3,57),(1087,5,3,0,0,4,57),(1088,11,6,-1,0,34,16),(1089,5,3,-1,0,5,57),(1090,11,6,-1,0,35,16),(1091,5,3,-1,0,6,57),(1092,11,6,-1,0,36,16),(1093,11,6,-1,0,37,16),(1094,5,3,0,0,7,57),(1095,11,6,-1,0,22,16),(1096,5,3,-1,0,8,57),(1097,11,6,-1,0,21,16),(1098,5,3,0,0,9,57),(1099,5,3,-2,0,1,58),(1100,5,3,-2,0,2,58),(1101,5,3,-2,0,1,58),(1102,5,3,-2,0,3,58),(1103,5,3,-2,0,4,58),(1104,5,3,-2,0,5,58),(1105,5,3,-2,0,6,58),(1106,5,3,-2,0,7,58),(1107,5,3,-2,0,8,58),(1108,5,3,-2,0,9,58),(1109,5,3,4.515753,0,16,34),(1110,5,3,0.284764,0,38,34),(1111,5,3,133.514055,0,39,34),(1112,5,3,0.530094,0,22,34),(1113,5,3,16.519861,0,21,34),(1114,5,3,4.417968,0,10,34),(1115,11,6,-1,0,18,16),(1116,5,3,1589.563599,0,3,34),(1117,11,6,-1,0,13,16),(1118,5,3,-1,0,7,34),(1119,11,6,-1,0,10,16),(1120,5,3,-1,0,9,34),(1121,5,3,596.810754,0,19,34),(1122,5,3,0.100043,0,16,35),(1123,5,3,0.046353,0,38,35),(1124,5,3,1.052541,0,39,35),(1125,5,3,0.307953,0,22,35),(1126,5,3,8.048388,0,21,35),(1127,5,3,5.877457,0,10,35),(1128,5,3,46.373398,0,3,35),(1129,5,3,62.842607,0,7,35),(1130,5,3,49.332498,0,9,35),(1131,5,3,269.590019,0,19,35),(1132,5,3,10.998224,0,16,33),(1133,5,3,0.85632,0,38,33),(1134,5,3,407.01127,0,39,33),(1135,5,3,291.115075,0,22,33),(1136,5,3,86.262178,0,21,33),(1137,5,3,35.731886,0,10,33),(1138,11,6,-1,0,3,16),(1139,11,6,-2,0,5,16),(1140,5,3,-1,0,3,33),(1141,11,6,-1,0,34,11),(1142,5,3,-1,0,7,33),(1143,11,6,-1,0,35,11),(1144,12,8,0.003053,0,11,31),(1145,12,8,-2,0,24,31),(1146,12,8,0.022791,0,25,31),(1147,12,8,1.389675,0,26,31),(1148,12,8,0.127076,0,3,31),(1149,12,8,0.25844,0,27,31),(1150,12,8,-2,0,28,31),(1151,12,8,0.068905,0,29,31),(1152,12,8,0.029532,0,30,31),(1153,12,8,0.055655,0,31,31),(1154,12,8,-2,0,32,31),(1155,12,8,-2,0,33,31),(1156,5,3,-1,0,9,33),(1157,11,6,-1,0,36,11),(1158,5,3,-1,0,19,33),(1159,5,3,0.00681,0,11,41),(1160,5,3,0.137489,0,2,41),(1161,5,3,5.895401,0,13,41),(1162,5,3,0.003048,0,11,42),(1163,5,3,0.19283,0,2,42),(1164,5,3,8.421846,0,13,42),(1165,5,3,-2,0,14,69),(1166,5,3,29.628349,0,14,68),(1167,5,3,0.000414,0,15,107),(1168,5,3,0.482766,0,20,107),(1169,5,3,9.108175,0,18,107),(1170,5,3,2.230515,0,13,107),(1171,5,3,0.000475,0,15,108),(1172,5,3,0.557803,0,20,108),(1173,5,3,9.257706,0,18,108),(1174,5,3,14.398612,0,13,108),(1175,5,3,-2,0,1,48),(1176,5,3,-2,0,2,48),(1177,5,3,-2,0,1,48),(1178,5,3,-2,0,3,48),(1179,5,3,-2,0,4,48),(1180,5,3,-2,0,5,48),(1181,5,3,-2,0,6,48),(1182,5,3,-2,0,7,48),(1183,5,3,-2,0,8,48),(1184,5,3,-2,0,9,48),(1185,5,3,0,0,1,47),(1186,5,3,0,0,2,47),(1187,5,3,0,0,1,47),(1188,5,3,0,0,3,47),(1189,5,3,0,0,4,47),(1190,11,6,-1,0,37,11),(1191,5,3,-1,0,5,47),(1192,11,6,-1,0,22,11),(1193,5,3,-1,0,6,47),(1194,11,6,-1,0,21,11),(1195,11,6,-1,0,18,11),(1196,5,3,0,0,7,47),(1197,11,6,-1,0,13,11),(1198,5,3,-1,0,8,47),(1199,11,6,-1,0,10,11),(1200,5,3,0,0,9,47),(1201,5,3,0.020021,0,1,46),(1202,5,3,0.009052,0,2,46),(1203,5,3,0.022749,0,1,46),(1204,5,3,24.62377,0,3,46),(1205,5,3,9.580099,0,4,46),(1206,5,3,12.012559,0,5,46),(1207,11,6,-1,0,3,11),(1208,11,6,-1,0,5,11),(1209,5,3,5733.921721,0,6,46),(1210,11,6,-1,0,34,10),(1211,11,6,-1,0,35,10),(1212,5,3,3836.759454,0,7,46),(1213,5,3,57.139884,0,8,46),(1214,11,6,-1,0,36,10),(1215,5,3,3510.075202,0,9,46),(1216,5,3,-2,0,14,37),(1217,5,3,-2,0,42,64),(1218,5,3,-2,0,11,64),(1219,5,3,-2,0,39,64),(1220,5,3,-2,0,17,64),(1221,5,3,-2,0,12,64),(1222,5,3,-2,0,18,64),(1223,5,3,0.030477,0,34,11),(1224,5,3,3.674628,0,35,11),(1225,5,3,259.134866,0,36,11),(1226,5,3,87.828312,0,37,11),(1227,5,3,465.978997,0,22,11),(1228,5,3,-2,0,21,11),(1229,5,3,0.099699,0,18,11),(1230,11,6,-1,0,37,10),(1231,5,3,29.58772,0,13,11),(1232,5,3,0.240998,0,10,11),(1233,5,3,1704.888861,0,3,11),(1234,5,3,-2,0,5,11),(1235,5,3,0.056221,0,34,10),(1236,5,3,2.632702,0,35,10),(1237,5,3,255.173214,0,36,10),(1238,5,3,68.288567,0,37,10),(1239,5,3,436.641509,0,22,10),(1240,11,6,-1,0,22,10),(1241,5,3,-2,0,21,10),(1242,5,3,0.116395,0,18,10),(1243,5,3,26.641883,0,13,10),(1244,5,3,0.280834,0,10,10),(1245,5,3,2556.929495,0,3,10),(1246,5,3,-2,0,5,10),(1247,5,3,0.039249,0,34,7),(1248,5,3,3.623154,0,35,7),(1249,11,6,-1,0,21,10),(1250,5,3,293.724067,0,36,7),(1251,5,3,89.26021,0,37,7),(1252,5,3,505.26949,0,22,7),(1253,5,3,-2,0,21,7),(1254,5,3,0.100525,0,18,7),(1255,5,3,29.745748,0,13,7),(1256,5,3,0.251156,0,10,7),(1257,11,6,-1,0,18,10),(1258,5,3,1899.228377,0,3,7),(1259,5,3,-2,0,5,7),(1260,5,3,0.039949,0,34,8),(1261,5,3,2.99785,0,35,8),(1262,5,3,255.19373,0,36,8),(1263,5,3,76.481257,0,37,8),(1264,5,3,436.166577,0,22,8),(1265,5,3,-2,0,21,8),(1266,5,3,0.119681,0,18,8),(1267,5,3,29.826944,0,13,8),(1268,5,3,0.295345,0,10,8),(1269,5,3,1660.495823,0,3,8),(1270,5,3,-2,0,5,8),(1271,5,3,0.047116,0,34,5),(1272,5,3,2.990466,0,35,5),(1273,5,3,255.314816,0,36,5),(1274,5,3,76.682298,0,37,5),(1275,5,3,445.963233,0,22,5),(1276,5,3,-2,0,21,5),(1277,5,3,0.827551,0,18,5),(1278,5,3,49.191372,0,13,5),(1279,5,3,3.217173,0,10,5),(1280,5,3,1802.475483,0,3,5),(1281,5,3,-2,0,5,5),(1282,5,3,0.043568,0,34,14),(1283,5,3,0.011,0,35,14),(1284,5,3,0.009158,0,36,14),(1285,5,3,0.01393,0,37,14),(1286,5,3,0.061217,0,22,14),(1287,5,3,2.120716,0,21,14),(1288,13,6,0.023613,0,11,66),(1289,13,6,0.000795,0,12,66),(1290,13,6,0.172843,0,13,66),(1291,13,6,0.011102,0,1,21),(1292,13,6,-2,0,2,21),(1293,13,6,5.273266,0,1,21),(1294,13,6,24.72387,0,3,21),(1295,13,6,-2,0,4,21),(1296,5,3,-1,0,18,14),(1297,5,3,4.772985,0,13,14),(1298,5,3,918.01887,0,10,14),(1299,5,3,36.388869,0,3,14),(1300,5,3,13.844534,0,5,14),(1301,5,3,0.030483,0,34,13),(1302,5,3,0.006774,0,35,13),(1303,5,3,0.004304,0,36,13),(1304,5,3,0.01065,0,37,13),(1305,5,3,0.229312,0,22,13),(1306,5,3,4.292775,0,21,13),(1307,13,6,-1,0,5,21),(1308,13,6,-2,0,6,21),(1309,13,6,2477.232993,0,7,21),(1310,13,6,-2,0,8,21),(1311,13,6,-2,0,9,21),(1312,13,6,17.400897,0,5,52),(1313,13,6,0.074543,0,1,52),(1314,13,6,0.012268,0,2,50),(1315,13,6,0.010943,0,23,56),(1316,13,6,0.018361,0,22,51),(1317,13,6,-2,0,6,54),(1318,13,6,15.483944,0,7,55),(1319,13,6,1.231994,0,4,55),(1320,13,6,0.932423,0,3,49),(1321,13,6,0.000664,0,1,53),(1322,13,6,0.00059,0,11,53),(1323,13,6,0.028888,0,14,93),(1324,13,6,0.002354,0,14,93),(1325,13,6,0.001159,0,11,93),(1326,13,6,0.00153,0,1,93),(1327,13,6,0.006267,0,44,93),(1328,13,6,-2,0,23,93),(1329,13,6,-2,0,39,93),(1330,13,6,-2,0,3,93),(1331,13,6,-2,0,21,93),(1332,13,6,-2,0,5,93),(1333,13,6,0.019379,0,24,1),(1334,13,6,1.010397,0,25,1),(1335,13,6,3.737048,0,26,1),(1336,5,3,-1,0,18,13),(1337,5,3,17.096282,0,13,13),(1338,13,6,18.808924,0,3,1),(1339,13,6,0.005588,0,27,1),(1340,5,3,2908.894796,0,10,13),(1341,5,3,60.346795,0,3,13),(1342,13,6,-1,0,28,1),(1343,13,6,0.002606,0,29,1),(1344,13,6,0.00832,0,30,1),(1345,13,6,0.002578,0,31,1),(1346,13,6,0.01372,0,32,1),(1347,13,6,0.000427,0,33,1),(1348,5,3,24.254838,0,5,13),(1349,5,3,0.013897,0,34,12),(1350,5,3,0.010364,0,35,12),(1351,5,3,0.014118,0,36,12),(1352,5,3,0.013098,0,37,12),(1353,5,3,0.138078,0,22,12),(1354,5,3,4.019644,0,21,12),(1355,13,6,-1,0,11,63),(1356,5,3,-1,0,18,12),(1357,5,3,9.098336,0,13,12),(1358,13,6,-1,0,24,63),(1359,13,6,-1,0,25,63),(1360,5,3,7253.715756,0,10,12),(1361,5,3,68.310514,0,3,12),(1362,5,3,226.414391,0,5,12),(1363,5,3,0.014127,0,34,6),(1364,5,3,0.007453,0,35,6),(1365,5,3,0.014623,0,36,6),(1366,5,3,0.013372,0,37,6),(1367,5,3,0.304262,0,22,6),(1368,5,3,7.142697,0,21,6),(1369,13,6,-1,0,26,63),(1370,5,3,-1,0,18,6),(1371,5,3,21.50107,0,13,6),(1372,13,6,-1,0,3,63),(1373,5,3,1460.890953,0,10,6),(1374,5,3,102.517663,0,3,6),(1375,5,3,43.662621,0,5,6),(1376,5,3,0.014034,0,34,15),(1377,5,3,0.002799,0,35,15),(1378,5,3,0.013768,0,36,15),(1379,5,3,0.012794,0,37,15),(1380,5,3,0.1312,0,22,15),(1381,5,3,3.29042,0,21,15),(1382,13,6,-1,0,27,63),(1383,5,3,-1,0,18,15),(1384,5,3,9.386166,0,13,15),(1385,5,3,935.81709,0,10,15),(1386,5,3,69.276249,0,3,15),(1387,5,3,24.14591,0,5,15),(1388,5,3,0.013959,0,34,9),(1389,5,3,0.007146,0,35,9),(1390,5,3,0.008861,0,36,9),(1391,5,3,0.013484,0,37,9),(1392,5,3,0.054907,0,22,9),(1393,5,3,1.483284,0,21,9),(1394,13,6,-1,0,28,63),(1395,5,3,-1,0,18,9),(1396,5,3,5.065189,0,13,9),(1397,5,3,889.910256,0,10,9),(1398,5,3,37.455146,0,3,9),(1399,5,3,10.614375,0,5,9),(1400,5,3,0.027851,0,34,16),(1401,5,3,2.992438,0,35,16),(1402,5,3,255.079723,0,36,16),(1403,5,3,76.609837,0,37,16),(1404,13,6,-1,0,29,63),(1405,5,3,435.860771,0,22,16),(1406,5,3,-2,0,21,16),(1407,5,3,0.118034,0,18,16),(1408,5,3,29.847419,0,13,16),(1409,5,3,0.289901,0,10,16),(1410,13,6,-1,0,30,63),(1411,5,3,1669.168981,0,3,16),(1412,5,3,-2,0,5,16),(1413,5,3,5.125465,0,39,100),(1414,5,3,28.557663,0,39,98),(1415,5,3,4.714095,0,39,104),(1416,5,3,-2,0,39,105),(1417,5,3,15.519801,0,39,99),(1418,5,3,4.117265,0,39,103),(1419,5,3,23.37868,0,39,101),(1420,5,3,16.742451,0,39,102),(1421,5,3,16.807857,0,39,106),(1422,5,3,0.033515,0,11,90),(1423,5,3,0.0108,0,1,90),(1424,5,3,0.012626,0,40,90),(1425,5,3,0.014579,0,15,90),(1426,5,3,-2,0,14,32),(1427,5,3,0.005466,0,1,38),(1428,5,3,0.037847,0,2,38),(1429,5,3,0.112704,0,1,38),(1430,5,3,2.484366,0,3,38),(1431,5,3,13.549645,0,4,38),(1432,5,3,200.885441,0,5,38),(1433,5,3,309.96257,0,6,38),(1434,5,3,38.049074,0,7,38),(1435,5,3,298.992606,0,8,38),(1436,5,3,55.867347,0,9,38),(1437,5,3,0.010306,0,1,27),(1438,5,3,0.02563,0,2,27),(1439,5,3,0.045664,0,1,27),(1440,5,3,15.744251,0,3,27),(1441,5,3,11.315643,0,4,27),(1442,5,3,14.14151,0,5,27),(1443,5,3,954.686022,0,6,27),(1444,13,6,-1,0,31,63),(1445,5,3,3301.792472,0,7,27),(1446,13,6,-1,0,32,63),(1447,5,3,1142.282178,0,8,27),(1448,13,6,-1,0,33,63),(1449,13,6,-2,0,11,70),(1450,13,6,-2,0,39,70),(1451,13,6,-2,0,13,70),(1452,13,6,-2,0,18,70),(1453,13,6,0.017856,0,11,45),(1454,13,6,0.027432,0,24,45),(1455,13,6,1.135832,0,25,45),(1456,13,6,25.347962,0,26,45),(1457,13,6,4.466108,0,3,45),(1458,13,6,0.111217,0,27,45),(1459,13,6,183.319333,0,28,45),(1460,13,6,0.046838,0,29,45),(1461,13,6,0.061399,0,30,45),(1462,13,6,0.049144,0,31,45),(1463,13,6,0.150382,0,32,45),(1464,13,6,0.001857,0,33,45),(1465,5,3,5064.835106,0,9,27),(1466,5,3,0.034087,0,1,22),(1467,5,3,0.016036,0,2,22),(1468,5,3,0.040319,0,1,22),(1469,5,3,18.428747,0,3,22),(1470,5,3,13.125766,0,4,22),(1471,5,3,14.862907,0,5,22),(1472,5,3,1230.996615,0,6,22),(1473,5,3,3624.827733,0,7,22),(1474,5,3,1341.683358,0,8,22),(1475,13,6,-2,0,11,75),(1476,5,3,5251.317343,0,9,22),(1477,5,3,0.027423,0,1,17),(1478,5,3,0.015451,0,2,17),(1479,5,3,0.044798,0,1,17),(1480,5,3,18.431868,0,3,17),(1481,5,3,13.146563,0,4,17),(1482,5,3,14.910175,0,5,17),(1483,5,3,1233.371284,0,6,17),(1484,13,6,-2,0,24,75),(1485,5,3,3621.530645,0,7,17),(1486,5,3,1326.279593,0,8,17),(1487,5,3,5252.446343,0,9,17),(1488,5,3,0,0,1,18),(1489,5,3,0,0,2,18),(1490,5,3,0,0,1,18),(1491,5,3,0,0,3,18),(1492,5,3,0,0,4,18),(1493,13,6,-2,0,25,75),(1494,5,3,-1,0,5,18),(1495,5,3,-1,0,6,18),(1496,13,6,-2,0,26,75),(1497,5,3,0,0,7,18),(1498,5,3,-1,0,8,18),(1499,5,3,0,0,9,18),(1500,5,3,0.017147,0,1,19),(1501,5,3,0.043541,0,2,19),(1502,5,3,0.176645,0,1,19),(1503,5,3,40.514511,0,3,19),(1504,5,3,118.670407,0,4,19),(1505,5,3,181.081765,0,5,19),(1506,13,6,-2,0,3,75),(1507,5,3,-1,0,6,19),(1508,5,3,4774.056708,0,7,19),(1509,13,6,-2,0,27,75),(1510,5,3,5935.174709,0,8,19),(1511,13,6,-2,0,28,75),(1512,5,3,8277.986005,0,9,19),(1513,5,3,0.014662,0,1,20),(1514,5,3,0.015802,0,2,20),(1515,5,3,0.050566,0,1,20),(1516,5,3,18.997701,0,3,20),(1517,5,3,14.009334,0,4,20),(1518,5,3,16.03921,0,5,20),(1519,5,3,1322.64153,0,6,20),(1520,5,3,3705.51549,0,7,20),(1521,13,6,-2,0,29,75),(1522,5,3,1403.103118,0,8,20),(1523,5,3,5272.449085,0,9,20),(1524,5,3,0.033281,0,1,25),(1525,5,3,0.010316,0,2,25),(1526,5,3,0.045116,0,1,25),(1527,5,3,17.006633,0,3,25),(1528,5,3,12.160586,0,4,25),(1529,5,3,14.544688,0,5,25),(1530,5,3,1120.048227,0,6,25),(1531,13,6,-2,0,30,75),(1532,5,3,3461.856264,0,7,25),(1533,5,3,1226.043199,0,8,25),(1534,5,3,5189.567893,0,9,25),(1535,5,3,0.026726,0,1,23),(1536,5,3,0.017764,0,2,23),(1537,5,3,0.044065,0,1,23),(1538,5,3,24.001252,0,3,23),(1539,5,3,9.035564,0,4,23),(1540,5,3,15.90701,0,5,23),(1541,5,3,284.435038,0,6,23),(1542,13,6,-2,0,31,75),(1543,5,3,4740.061126,0,7,23),(1544,5,3,746.295254,0,8,23),(1545,13,6,-2,0,32,75),(1546,5,3,7052.51848,0,9,23),(1547,5,3,0.01258,0,1,28),(1548,5,3,0.020134,0,2,28),(1549,5,3,0.041055,0,1,28),(1550,5,3,12.733826,0,3,28),(1551,5,3,8.901195,0,4,28),(1552,5,3,13.30978,0,5,28),(1553,5,3,569.474935,0,6,28),(1554,5,3,2865.526588,0,7,28),(1555,5,3,934.735862,0,8,28),(1556,13,6,-2,0,33,75),(1557,13,6,-1,0,33,71),(1558,5,3,4753.740232,0,9,28),(1559,5,3,0.019311,0,1,29),(1560,5,3,0.013757,0,2,29),(1561,5,3,0.043451,0,1,29),(1562,5,3,13.638524,0,3,29),(1563,5,3,9.53424,0,4,29),(1564,5,3,13.577487,0,5,29),(1565,5,3,666.708097,0,6,29),(1566,13,6,-1,0,27,71),(1567,5,3,3003.710691,0,7,29),(1568,5,3,991.489911,0,8,29),(1569,13,6,-1,0,30,73),(1570,13,6,-1,0,26,73),(1571,5,3,4860.950362,0,9,29),(1572,5,3,0.017753,0,1,26),(1573,5,3,0.070331,0,2,26),(1574,5,3,0.336053,0,1,26),(1575,5,3,46.071135,0,3,26),(1576,5,3,185.882333,0,4,26),(1577,5,3,326.877431,0,5,26),(1578,13,6,-1,0,31,72),(1579,5,3,-1,0,6,26),(1580,13,6,-1,0,29,72),(1581,13,6,-1,0,34,16),(1582,5,3,5797.540066,0,7,26),(1583,13,6,-1,0,35,16),(1584,5,3,-1,0,8,26),(1585,13,6,-1,0,36,16),(1586,13,6,-1,0,37,16),(1587,13,6,-1,0,22,16),(1588,5,3,8756.971426,0,9,26),(1589,5,3,0.014548,0,1,24),(1590,5,3,0.015216,0,2,24),(1591,5,3,0.043321,0,1,24),(1592,5,3,14.628792,0,3,24),(1593,5,3,10.249144,0,4,24),(1594,5,3,13.869187,0,5,24),(1595,5,3,808.304963,0,6,24),(1596,13,6,-1,0,21,16),(1597,5,3,3149.416066,0,7,24),(1598,13,6,-1,0,18,16),(1599,5,3,1057.880082,0,8,24),(1600,13,6,-1,0,13,16),(1601,5,3,4965.015394,0,9,24),(1602,5,3,25.571957,0,10,96),(1603,5,3,19.965493,0,10,95),(1604,5,3,0.005329,0,23,56),(1605,13,6,-1,0,10,16),(1606,5,3,1476.5954,0,6,54),(1607,5,3,0.011811,0,22,51),(1608,5,3,31.073695,0,5,52),(1609,5,3,0.052942,0,1,52),(1610,5,3,0.000107,0,1,53),(1611,5,3,0.000084,0,11,53),(1612,5,3,4.410556,0,3,49),(1613,5,3,61.218436,0,7,55),(1614,5,3,1.940716,0,4,55),(1615,5,3,0.00622,0,2,50),(1616,5,3,0.006391,0,15,62),(1617,5,3,0.130168,0,20,62),(1618,5,3,-2,0,7,62),(1619,5,3,-2,0,9,62),(1620,5,3,8.804967,0,43,62),(1621,5,3,239.379756,0,21,62),(1622,5,3,5.906422,0,13,62),(1623,5,3,0.392128,0,18,62),(1624,5,3,0.00215,0,11,3),(1625,5,3,0.025637,0,44,3),(1626,5,3,19.336086,0,22,3),(1627,5,3,42.123149,0,4,3),(1628,5,3,142.215004,0,3,3),(1629,13,6,-1,0,3,16),(1630,13,6,-2,0,5,16),(1631,5,3,-1,0,9,3),(1632,5,3,0,0,11,4),(1633,5,3,0,0,44,4),(1634,5,3,0,0,22,4),(1635,13,6,-1,0,34,10),(1636,13,6,-1,0,35,10),(1637,13,6,-1,0,36,10),(1638,13,6,-1,0,37,10),(1639,13,6,-1,0,22,10),(1640,13,6,-1,0,21,10),(1641,13,6,-1,0,18,10),(1642,13,6,-1,0,13,10),(1643,13,6,-1,0,10,10),(1644,13,6,-1,0,3,10),(1645,13,6,-2,0,5,10),(1646,13,6,-1,0,34,8),(1647,13,6,-1,0,35,8),(1648,13,6,-1,0,36,8),(1649,13,6,-1,0,37,8),(1650,13,6,-1,0,22,8),(1651,13,6,-1,0,21,8),(1652,13,6,-1,0,18,8),(1653,13,6,-1,0,13,8),(1654,13,6,-1,0,10,8),(1655,13,6,-1,0,3,8),(1656,13,6,-2,0,5,8),(1657,13,6,-1,0,34,11),(1658,13,6,-1,0,35,11),(1659,13,6,-1,0,36,11),(1660,13,6,-1,0,37,11),(1661,13,6,-1,0,22,11),(1662,13,6,-1,0,21,11),(1663,13,6,-1,0,18,11),(1664,13,6,-1,0,13,11),(1665,13,6,-1,0,10,11),(1666,13,6,-1,0,3,11),(1667,13,6,-1,0,5,11),(1668,13,6,-1,0,15,40),(1669,13,6,-1,0,20,40),(1670,13,6,-1,0,21,40),(1671,13,6,-1,0,15,62),(1672,13,6,-1,0,20,62),(1673,13,6,-1,0,7,62),(1674,13,6,-1,0,9,62),(1675,13,6,-1,0,43,62),(1676,13,6,-1,0,21,62),(1677,13,6,-1,0,13,62),(1678,13,6,-1,0,18,62),(1679,13,6,-1,0,15,65),(1680,13,6,-1,0,20,65),(1681,13,6,-1,0,18,65),(1682,13,6,-1,0,13,65),(1683,13,6,-1,0,24,74),(1684,13,6,-1,0,25,74),(1685,13,6,-1,0,26,74),(1686,13,6,-1,0,3,74),(1687,13,6,-1,0,27,74),(1688,13,6,-1,0,28,74),(1689,13,6,-1,0,29,74),(1690,13,6,-1,0,30,74),(1691,13,6,-1,0,31,74),(1692,13,6,-1,0,32,74),(1693,13,6,-1,0,33,74),(1694,13,6,0.014387,0,11,90),(1695,13,6,0.001036,0,1,90),(1696,13,6,0.001062,0,40,90),(1697,13,6,0.001095,0,15,90),(1698,13,6,-2,0,14,90),(1699,13,6,0.001827,0,16,90),(1700,13,6,0.004189,0,37,90),(1701,13,6,0.009634,0,41,90),(1702,13,6,0.017948,0,17,90),(1703,13,6,-1,0,42,90),(1704,13,6,-2,0,18,90),(1705,13,6,-1,0,13,90),(1706,13,6,0.136855,0,4,90),(1707,13,6,-1,0,3,90),(1708,13,6,-1,0,7,90),(1709,13,6,-1,0,8,90),(1710,13,6,-1,0,9,90),(1711,13,6,-1,0,19,90),(1712,13,6,0.000907,0,11,92),(1713,13,6,0.00099,0,1,92),(1714,13,6,0.000972,0,40,92),(1715,13,6,0.000997,0,15,92),(1716,13,6,0.00514,0,14,92),(1717,13,6,0.001505,0,16,92),(1718,13,6,0.002995,0,37,92),(1719,13,6,0.005846,0,41,92),(1720,13,6,0.011247,0,17,92),(1721,13,6,-1,0,42,92),(1722,13,6,0.794678,0,18,92),(1723,13,6,-1,0,13,92),(1724,13,6,0.090256,0,4,92),(1725,13,6,-1,0,3,92),(1726,13,6,1.501817,0,5,92),(1727,13,6,0.308477,0,6,92),(1728,13,6,-1,0,7,92),(1729,13,6,-1,0,8,92),(1730,13,6,-1,0,9,92),(1731,13,6,-1,0,19,92),(1732,13,6,0.000895,0,11,91),(1733,13,6,0.000973,0,1,91),(1734,13,6,0.000972,0,40,91),(1735,13,6,0.00098,0,15,91),(1736,13,6,0.001892,0,14,91),(1737,13,6,0.001503,0,16,91),(1738,13,6,0.002959,0,37,91),(1739,13,6,0.005894,0,41,91),(1740,13,6,0.011252,0,17,91),(1741,13,6,-1,0,42,91),(1742,13,6,0.784421,0,18,91),(1743,13,6,-1,0,13,91),(1744,13,6,0.089661,0,4,91),(1745,13,6,-1,0,3,91),(1746,13,6,1.494722,0,5,91),(1747,13,6,0.309054,0,6,91),(1748,13,6,-1,0,7,91),(1749,13,6,-1,0,8,91),(1750,13,6,-1,0,9,91),(1751,13,6,-1,0,19,91);
/*!40000 ALTER TABLE `results` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `sweeps`
--

DROP TABLE IF EXISTS `sweeps`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `sweeps` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `type` text NOT NULL,
  `begin` text NOT NULL,
  `step` text NOT NULL,
  `end` text NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `sweeps`
--

LOCK TABLES `sweeps` WRITE;
/*!40000 ALTER TABLE `sweeps` DISABLE KEYS */;
/*!40000 ALTER TABLE `sweeps` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2018-07-03 22:16:44
